{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from math import sqrt\n",
    "from math import exp\n",
    "from math import pi\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocessing and Spliting data\n",
    "def split_data_train_test(data, long_input=True, normalized_data=True, test_size=0.2):\n",
    "    datapoints = [point.split(':') for point in data]\n",
    "    labels_data = [int(point[0]) for point in datapoints]\n",
    "    if long_input:\n",
    "        input_data = [json.loads(point[3]) for point in datapoints]\n",
    "    else:\n",
    "        input_data = [json.loads(point[1]) for point in datapoints]\n",
    "    input_data_pd = pd.DataFrame(input_data)\n",
    "    if not long_input:\n",
    "        input_data_pd[6] = input_data_pd[6].fillna(0)\n",
    "\n",
    "    if normalized_data:\n",
    "        input_data_pd = preprocessing.normalize(input_data_pd)\n",
    "        input_data_pd = pd.DataFrame(input_data_pd, columns=input_data_pd.columns)\n",
    "    \n",
    "    X = input_data_pd.to_numpy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels_data, test_size=0.2)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Given a point $X_i = [x_1 \\cdot x_n]$ in dataset $X$ where each datapoint $X_i$ belongs to class $c \\in C$. . Determine the label of $X_i$\n",
    "- $P(C_j|X_i) \\propto P(x_1|C_j)\\cdot P(x_i|C_j)\\cdots P(x_n|C_j) \\cdot P(C_j)$\n",
    "- $argmax([P(C_1|X_i), \\cdots P(C_m|X_i)])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Algorithm\n",
    "def naive_bayes(train, test):\n",
    "    summarize = summarize_by_class(train)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict(summarize, row)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "\n",
    "# for each class, get mean, stdev of each feature, size of that class\n",
    "def summarize_by_class(dataset):\n",
    "    separated = separate_by_class(dataset)\n",
    "    summaries = dict()\n",
    "    for class_value, rows in separated.items():\n",
    "        summaries[class_value] = summarize_dataset(rows)\n",
    "    return summaries\n",
    "\n",
    "def separate_by_class(dataset):\n",
    "    separated = dict()\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        class_value = vector[-1]\n",
    "        if (class_value not in separated):\n",
    "            separated[class_value] = list()\n",
    "        separated[class_value].append(vector[:-1])\n",
    "    return separated\n",
    "def summarize_dataset(dataset):\n",
    "    summaries = [(statistics.mean(column), statistics.stdev(column), len(column)) for column in zip(*dataset)]\n",
    "    return summaries\n",
    "\n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "    probabilities = dict()\n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        # calculate the prior (# instance of class i / dataset size)\n",
    "        probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "        # for each feature x_i, calculate likelihood & update the current probability (multiply with likelihood)\n",
    "        for i in range(len(class_summaries)):\n",
    "            mean, stdev, _ = class_summaries[i]\n",
    "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "    return probabilities\n",
    "\n",
    "# Predict the class for a given row\n",
    "def predict(summaries, row):\n",
    "    probabilities = calculate_class_probabilities(summaries, row)\n",
    "    best_label, best_prob = None, -1\n",
    "    for class_value, probability in probabilities.items():\n",
    "        if best_label is None or probability > best_prob:\n",
    "            best_prob = probability\n",
    "            best_label = class_value\n",
    "    return best_label\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./g2st.txt') as f:\n",
    "    data = [l for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data_train_test(data, normalized_data=False)\n",
    "train_dat = np.concatenate([X_train, np.array([y_train]).reshape(X_train.shape[0],1)],axis=1)\n",
    "test_dat = np.concatenate([X_test, np.array([y_test]).reshape(X_test.shape[0],1)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = naive_bayes(train_dat, test_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocessing and Spliting data\n",
    "def split_data_train_test_mult_class(data, long_input=True, normalized_data=True, test_size=0.2):\n",
    "    datapoints = [point.split(':') for point in data]\n",
    "    labels_data = [(ord(point[0]) - ord('A')) for point in datapoints]\n",
    "    if long_input:\n",
    "        input_data = [json.loads(point[4]) for point in datapoints]\n",
    "    else:\n",
    "        input_data = [json.loads(point[2]) for point in datapoints]\n",
    "    input_data_pd = pd.DataFrame(input_data)\n",
    "    if not long_input:\n",
    "        input_data_pd[6] = input_data_pd[6].fillna(0)\n",
    "\n",
    "    if normalized_data:\n",
    "        input_data_pd = preprocessing.normalize(input_data_pd)\n",
    "        input_data_pd = pd.DataFrame(input_data_pd, columns=input_data_pd.columns)\n",
    "    \n",
    "    X = input_data_pd.to_numpy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels_data, test_size=0.2)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./g2st2.txt') as f:\n",
    "    data = [l for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data_train_test_mult_class(data, normalized_data=False)\n",
    "train_dat = np.concatenate([X_train, np.array([y_train]).reshape(X_train.shape[0],1)],axis=1)\n",
    "test_dat = np.concatenate([X_test, np.array([y_test]).reshape(X_test.shape[0],1)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = naive_bayes(train_dat, test_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.265"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using naive bayes pre-built library function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 200000 points : 7470\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "#Number of mislabeled points out of a total 75 points : 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bibliography\n",
    "\n",
    "- Application of naive bayes to Sato-Tate problem taken from the following paper: He, YH, Lee, KH, Oliver, T. Machine Learning the Sato-Tate Conjecture. (2020) https://arxiv.org/abs/2010.01213\n",
    "\n",
    "- Implementation of naive bayes is inspired by https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
