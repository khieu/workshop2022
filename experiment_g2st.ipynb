{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from sklearn import svm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from math import sqrt\n",
    "from math import exp\n",
    "from math import pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./g2st.txt') as f:\n",
    "    data = [l for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = [point.split(':') for point in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data = [int(point[0]) for point in datapoints]\n",
    "input_long = [json.loads(point[3]) for point in datapoints]\n",
    "input_short = [json.loads(point[1]) for point in datapoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd = pd.DataFrame(input_short)\n",
    "long_test_pd = pd.DataFrame(input_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long input does not have any NaN\n",
    "# short input has some NaN in the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.count_nonzero(test_pd.isna()[6])\n",
    "test_pd[6] = test_pd[6].fillna(0)\n",
    "# normalized_test_pd = preprocessing.normalize(test_pd)\n",
    "# normalized_test_pd = pd.DataFrame(normalized_test_pd, columns=test_pd.columns)\n",
    "normalized_test_pd = preprocessing.normalize(long_test_pd)\n",
    "normalized_test_pd = pd.DataFrame(normalized_test_pd, columns=long_test_pd.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = normalized_test_pd.to_numpy()\n",
    "test_pd[6] = test_pd[6].fillna(0)\n",
    "\n",
    "X = long_test_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_data_large = preprocessing.normalize(hist_data_large)\n",
    "# X = hist_data_large\n",
    "\n",
    "normalized_test_pd_small = preprocessing.normalize(test_pd)\n",
    "normalized_test_pd_small = pd.DataFrame(normalized_test_pd, columns=test_pd.columns)\n",
    "frames = [normalized_test_pd_small, normalized_test_pd]\n",
    "combined_data = pd.concat(frames,axis=1)\n",
    "X = combined_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalized_test_pd_small.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, labels_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.input_data = X\n",
    "        self.labels = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "train_dataset = PolynomialDataset(X_train, y_train)\n",
    "test_dataset = PolynomialDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000, 7)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_iters = 160000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeedforwardNeuralNetModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        \n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.sigmoid(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "        \n",
    "        \n",
    "        # Define batch norm\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Define proportion or neurons to dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        #out = self.dropout(out)\n",
    "        #out = self.batch_norm(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        #out = self.dropout(out)\n",
    "        #out = self.batch_norm(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        #out = self.dropout(out)\n",
    "        #out = self.batch_norm(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "    \n",
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, batch_size, n_inputs, n_neurons):\n",
    "#         super(RNN, self).__init__()\n",
    "        \n",
    "#         self.rnn = nn.RNNCell(n_inputs, n_neurons)\n",
    "#         self.hx = torch.randn(batch_size, n_neurons) # initialize hidden state\n",
    "        \n",
    "#     def forward(self, X):\n",
    "#         output = []\n",
    "\n",
    "#         # for each time step\n",
    "#         for i in range(2):\n",
    "#             self.hx = self.rnn(X[i], self.hx)\n",
    "#             output.append(self.hx)\n",
    "        \n",
    "#         return output, self.hx\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat((x, hidden_state), 1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
    "\n",
    "\n",
    "class CovNet(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(CovNet, self).__init__()\n",
    "\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(4 * 7 * 7, 2)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 107\n",
    "hidden_dim = 200\n",
    "output_dim = 2\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.691423773765564. Accuracy: 54.737\n",
      "Iteration: 1000. Loss: 0.6857364773750305. Accuracy: 60.8505\n",
      "Iteration: 1500. Loss: 0.6352726221084595. Accuracy: 64.1545\n",
      "Iteration: 2000. Loss: 0.5814011096954346. Accuracy: 67.3205\n",
      "Iteration: 2500. Loss: 0.6371244192123413. Accuracy: 67.408\n",
      "Iteration: 3000. Loss: 0.5633236765861511. Accuracy: 67.3135\n",
      "Iteration: 3500. Loss: 0.6698815822601318. Accuracy: 67.702\n",
      "Iteration: 4000. Loss: 0.5103259086608887. Accuracy: 67.8145\n",
      "Iteration: 4500. Loss: 0.6399386525154114. Accuracy: 67.7375\n",
      "Iteration: 5000. Loss: 0.5155748128890991. Accuracy: 68.59\n",
      "Iteration: 5500. Loss: 0.5165994167327881. Accuracy: 68.555\n",
      "Iteration: 6000. Loss: 0.5869423151016235. Accuracy: 68.7175\n",
      "Iteration: 6500. Loss: 0.5686057806015015. Accuracy: 68.926\n",
      "Iteration: 7000. Loss: 0.5001527667045593. Accuracy: 68.1505\n",
      "Iteration: 7500. Loss: 0.47263866662979126. Accuracy: 69.421\n",
      "Iteration: 8000. Loss: 0.43666785955429077. Accuracy: 68.5475\n",
      "Iteration: 8500. Loss: 0.585555911064148. Accuracy: 69.2375\n",
      "Iteration: 9000. Loss: 0.4842894673347473. Accuracy: 69.873\n",
      "Iteration: 9500. Loss: 0.5823988914489746. Accuracy: 69.288\n",
      "Iteration: 10000. Loss: 0.5625672340393066. Accuracy: 70.1825\n",
      "Iteration: 10500. Loss: 0.44560837745666504. Accuracy: 67.301\n",
      "Iteration: 11000. Loss: 0.6604140400886536. Accuracy: 70.3745\n",
      "Iteration: 11500. Loss: 0.5671414136886597. Accuracy: 70.4195\n",
      "Iteration: 12000. Loss: 0.5103760361671448. Accuracy: 71.181\n",
      "Iteration: 12500. Loss: 0.5540611147880554. Accuracy: 70.538\n",
      "Iteration: 13000. Loss: 0.5035856366157532. Accuracy: 70.827\n",
      "Iteration: 13500. Loss: 0.4884618818759918. Accuracy: 69.9185\n",
      "Iteration: 14000. Loss: 0.5599101781845093. Accuracy: 71.6175\n",
      "Iteration: 14500. Loss: 0.5449138283729553. Accuracy: 71.4275\n",
      "Iteration: 15000. Loss: 0.5861276388168335. Accuracy: 71.576\n",
      "Iteration: 15500. Loss: 0.48709556460380554. Accuracy: 71.645\n",
      "Iteration: 16000. Loss: 0.54869145154953. Accuracy: 72.485\n",
      "Iteration: 16500. Loss: 0.615256667137146. Accuracy: 72.452\n",
      "Iteration: 17000. Loss: 0.5204871296882629. Accuracy: 72.472\n",
      "Iteration: 17500. Loss: 0.5514886975288391. Accuracy: 72.4015\n",
      "Iteration: 18000. Loss: 0.5586115121841431. Accuracy: 72.4945\n",
      "Iteration: 18500. Loss: 0.6453547477722168. Accuracy: 71.336\n",
      "Iteration: 19000. Loss: 0.5272426605224609. Accuracy: 72.961\n",
      "Iteration: 19500. Loss: 0.638007402420044. Accuracy: 72.843\n",
      "Iteration: 20000. Loss: 0.49013134837150574. Accuracy: 72.9935\n",
      "Iteration: 20500. Loss: 0.39131614565849304. Accuracy: 73.4205\n",
      "Iteration: 21000. Loss: 0.5812596678733826. Accuracy: 73.2245\n",
      "Iteration: 21500. Loss: 0.6095449924468994. Accuracy: 71.5425\n",
      "Iteration: 22000. Loss: 0.6004730463027954. Accuracy: 73.0455\n",
      "Iteration: 22500. Loss: 0.540701150894165. Accuracy: 73.783\n",
      "Iteration: 23000. Loss: 0.4410034418106079. Accuracy: 70.982\n",
      "Iteration: 23500. Loss: 0.578942596912384. Accuracy: 72.289\n",
      "Iteration: 24000. Loss: 0.41602814197540283. Accuracy: 73.457\n",
      "Iteration: 24500. Loss: 0.654267430305481. Accuracy: 74.0845\n",
      "Iteration: 25000. Loss: 0.36714211106300354. Accuracy: 73.3575\n",
      "Iteration: 25500. Loss: 0.6674658060073853. Accuracy: 74.1225\n",
      "Iteration: 26000. Loss: 0.46377548575401306. Accuracy: 73.384\n",
      "Iteration: 26500. Loss: 0.42477187514305115. Accuracy: 72.752\n",
      "Iteration: 27000. Loss: 0.5590364933013916. Accuracy: 73.982\n",
      "Iteration: 27500. Loss: 0.493783563375473. Accuracy: 74.1965\n",
      "Iteration: 28000. Loss: 0.5806416273117065. Accuracy: 74.5385\n",
      "Iteration: 28500. Loss: 0.4290231466293335. Accuracy: 74.638\n",
      "Iteration: 29000. Loss: 0.5927522778511047. Accuracy: 74.2775\n",
      "Iteration: 29500. Loss: 0.5315343141555786. Accuracy: 74.12\n",
      "Iteration: 30000. Loss: 0.3841363489627838. Accuracy: 74.6445\n",
      "Iteration: 30500. Loss: 0.4569496512413025. Accuracy: 74.164\n",
      "Iteration: 31000. Loss: 0.4943789541721344. Accuracy: 74.146\n",
      "Iteration: 31500. Loss: 0.4217872619628906. Accuracy: 73.863\n",
      "Iteration: 32000. Loss: 0.4963912069797516. Accuracy: 74.779\n",
      "Iteration: 32500. Loss: 0.4343510866165161. Accuracy: 75.038\n",
      "Iteration: 33000. Loss: 0.4904402196407318. Accuracy: 73.591\n",
      "Iteration: 33500. Loss: 0.6043868064880371. Accuracy: 74.351\n",
      "Iteration: 34000. Loss: 0.4457528591156006. Accuracy: 75.188\n",
      "Iteration: 34500. Loss: 0.4620266854763031. Accuracy: 74.784\n",
      "Iteration: 35000. Loss: 0.42168864607810974. Accuracy: 75.2275\n",
      "Iteration: 35500. Loss: 0.475763738155365. Accuracy: 73.9305\n",
      "Iteration: 36000. Loss: 0.551074206829071. Accuracy: 74.94\n",
      "Iteration: 36500. Loss: 0.4627343714237213. Accuracy: 75.344\n",
      "Iteration: 37000. Loss: 0.37890610098838806. Accuracy: 75.61\n",
      "Iteration: 37500. Loss: 0.42867711186408997. Accuracy: 75.277\n",
      "Iteration: 38000. Loss: 0.3635614812374115. Accuracy: 74.217\n",
      "Iteration: 38500. Loss: 0.5112310647964478. Accuracy: 75.027\n",
      "Iteration: 39000. Loss: 0.6310309171676636. Accuracy: 74.757\n",
      "Iteration: 39500. Loss: 0.5395768880844116. Accuracy: 75.7175\n",
      "Iteration: 40000. Loss: 0.35365813970565796. Accuracy: 75.851\n",
      "Iteration: 40500. Loss: 0.40611836314201355. Accuracy: 75.778\n",
      "Iteration: 41000. Loss: 0.4713003635406494. Accuracy: 74.849\n",
      "Iteration: 41500. Loss: 0.5111833214759827. Accuracy: 75.8065\n",
      "Iteration: 42000. Loss: 0.3494868576526642. Accuracy: 74.967\n",
      "Iteration: 42500. Loss: 0.5363124012947083. Accuracy: 75.7895\n",
      "Iteration: 43000. Loss: 0.40522661805152893. Accuracy: 75.8205\n",
      "Iteration: 43500. Loss: 0.31425806879997253. Accuracy: 74.704\n",
      "Iteration: 44000. Loss: 0.5577707886695862. Accuracy: 76.08\n",
      "Iteration: 44500. Loss: 0.6005925536155701. Accuracy: 76.0985\n",
      "Iteration: 45000. Loss: 0.43958351016044617. Accuracy: 75.2515\n",
      "Iteration: 45500. Loss: 0.5509185791015625. Accuracy: 76.371\n",
      "Iteration: 46000. Loss: 0.5051897168159485. Accuracy: 76.135\n",
      "Iteration: 46500. Loss: 0.5067856311798096. Accuracy: 76.149\n",
      "Iteration: 47000. Loss: 0.5918033719062805. Accuracy: 75.5775\n",
      "Iteration: 47500. Loss: 0.4488118886947632. Accuracy: 76.297\n",
      "Iteration: 48000. Loss: 0.4438992738723755. Accuracy: 76.1235\n",
      "Iteration: 48500. Loss: 0.44562244415283203. Accuracy: 76.387\n",
      "Iteration: 49000. Loss: 0.4972768723964691. Accuracy: 75.988\n",
      "Iteration: 49500. Loss: 0.5607278347015381. Accuracy: 76.196\n",
      "Iteration: 50000. Loss: 0.42133620381355286. Accuracy: 76.297\n",
      "Iteration: 50500. Loss: 0.3597430884838104. Accuracy: 76.103\n",
      "Iteration: 51000. Loss: 0.3582463562488556. Accuracy: 75.987\n",
      "Iteration: 51500. Loss: 0.4371126890182495. Accuracy: 76.19\n",
      "Iteration: 52000. Loss: 0.5264366865158081. Accuracy: 76.7025\n",
      "Iteration: 52500. Loss: 0.38648277521133423. Accuracy: 75.686\n",
      "Iteration: 53000. Loss: 0.40073761343955994. Accuracy: 76.6265\n",
      "Iteration: 53500. Loss: 0.4056969881057739. Accuracy: 76.5815\n",
      "Iteration: 54000. Loss: 0.4186214208602905. Accuracy: 76.447\n",
      "Iteration: 54500. Loss: 0.3668881356716156. Accuracy: 76.3\n",
      "Iteration: 55000. Loss: 0.3229583501815796. Accuracy: 76.1245\n",
      "Iteration: 55500. Loss: 0.5338609218597412. Accuracy: 76.3095\n",
      "Iteration: 56000. Loss: 0.3387967348098755. Accuracy: 76.359\n",
      "Iteration: 56500. Loss: 0.4496077001094818. Accuracy: 76.528\n",
      "Iteration: 57000. Loss: 0.461689293384552. Accuracy: 76.806\n",
      "Iteration: 57500. Loss: 0.5117163062095642. Accuracy: 76.7585\n",
      "Iteration: 58000. Loss: 0.49797335267066956. Accuracy: 76.8975\n",
      "Iteration: 58500. Loss: 0.43777328729629517. Accuracy: 76.2575\n",
      "Iteration: 59000. Loss: 0.4173038601875305. Accuracy: 76.4675\n",
      "Iteration: 59500. Loss: 0.41337889432907104. Accuracy: 76.8345\n",
      "Iteration: 60000. Loss: 0.49134957790374756. Accuracy: 76.8835\n",
      "Iteration: 60500. Loss: 0.469229131937027. Accuracy: 77.1555\n",
      "Iteration: 61000. Loss: 0.5154814124107361. Accuracy: 76.8835\n",
      "Iteration: 61500. Loss: 0.4364134967327118. Accuracy: 76.9035\n",
      "Iteration: 62000. Loss: 0.36004993319511414. Accuracy: 77.089\n",
      "Iteration: 62500. Loss: 0.4734378755092621. Accuracy: 76.8\n",
      "Iteration: 63000. Loss: 0.5001766681671143. Accuracy: 76.935\n",
      "Iteration: 63500. Loss: 0.4185520112514496. Accuracy: 77.1565\n",
      "Iteration: 64000. Loss: 0.467591792345047. Accuracy: 77.0695\n",
      "Iteration: 64500. Loss: 0.37789177894592285. Accuracy: 76.8965\n",
      "Iteration: 65000. Loss: 0.38230228424072266. Accuracy: 77.126\n",
      "Iteration: 65500. Loss: 0.38178738951683044. Accuracy: 77.313\n",
      "Iteration: 66000. Loss: 0.4365648627281189. Accuracy: 77.187\n",
      "Iteration: 66500. Loss: 0.46739333868026733. Accuracy: 76.8055\n",
      "Iteration: 67000. Loss: 0.40867382287979126. Accuracy: 76.597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 67500. Loss: 0.38506218791007996. Accuracy: 77.0445\n",
      "Iteration: 68000. Loss: 0.33549731969833374. Accuracy: 76.1735\n",
      "Iteration: 68500. Loss: 0.37750375270843506. Accuracy: 77.291\n",
      "Iteration: 69000. Loss: 0.42843401432037354. Accuracy: 77.182\n",
      "Iteration: 69500. Loss: 0.41337716579437256. Accuracy: 76.908\n",
      "Iteration: 70000. Loss: 0.42174607515335083. Accuracy: 77.251\n",
      "Iteration: 70500. Loss: 0.4030623733997345. Accuracy: 77.2925\n",
      "Iteration: 71000. Loss: 0.4535084664821625. Accuracy: 77.236\n",
      "Iteration: 71500. Loss: 0.41013476252555847. Accuracy: 77.4695\n",
      "Iteration: 72000. Loss: 0.46011218428611755. Accuracy: 76.462\n",
      "Iteration: 72500. Loss: 0.3696901202201843. Accuracy: 76.613\n",
      "Iteration: 73000. Loss: 0.42884305119514465. Accuracy: 77.552\n",
      "Iteration: 73500. Loss: 0.4614757299423218. Accuracy: 77.4305\n",
      "Iteration: 74000. Loss: 0.3395008444786072. Accuracy: 77.0865\n",
      "Iteration: 74500. Loss: 0.33523574471473694. Accuracy: 77.6105\n",
      "Iteration: 75000. Loss: 0.3789323568344116. Accuracy: 77.431\n",
      "Iteration: 75500. Loss: 0.35753560066223145. Accuracy: 77.222\n",
      "Iteration: 76000. Loss: 0.46088042855262756. Accuracy: 77.5055\n",
      "Iteration: 76500. Loss: 0.37525683641433716. Accuracy: 77.268\n",
      "Iteration: 77000. Loss: 0.45128798484802246. Accuracy: 77.627\n",
      "Iteration: 77500. Loss: 0.3229808509349823. Accuracy: 77.6215\n",
      "Iteration: 78000. Loss: 0.39280226826667786. Accuracy: 77.533\n",
      "Iteration: 78500. Loss: 0.5715952515602112. Accuracy: 77.2475\n",
      "Iteration: 79000. Loss: 0.4870400130748749. Accuracy: 77.173\n",
      "Iteration: 79500. Loss: 0.5054672360420227. Accuracy: 76.748\n",
      "Iteration: 80000. Loss: 0.4911065697669983. Accuracy: 77.5595\n",
      "Iteration: 80500. Loss: 0.3676871061325073. Accuracy: 77.626\n",
      "Iteration: 81000. Loss: 0.35968655347824097. Accuracy: 77.8085\n",
      "Iteration: 81500. Loss: 0.3626663088798523. Accuracy: 77.604\n",
      "Iteration: 82000. Loss: 0.4081268608570099. Accuracy: 77.5375\n",
      "Iteration: 82500. Loss: 0.35826751589775085. Accuracy: 77.589\n",
      "Iteration: 83000. Loss: 0.3888632655143738. Accuracy: 77.6895\n",
      "Iteration: 83500. Loss: 0.4485541880130768. Accuracy: 77.849\n",
      "Iteration: 84000. Loss: 0.48786666989326477. Accuracy: 77.5475\n",
      "Iteration: 84500. Loss: 0.4464053213596344. Accuracy: 77.591\n",
      "Iteration: 85000. Loss: 0.4768763780593872. Accuracy: 77.891\n",
      "Iteration: 85500. Loss: 0.4265584647655487. Accuracy: 77.7445\n",
      "Iteration: 86000. Loss: 0.40174317359924316. Accuracy: 77.641\n",
      "Iteration: 86500. Loss: 0.4698638916015625. Accuracy: 77.7045\n",
      "Iteration: 87000. Loss: 0.40030181407928467. Accuracy: 77.6725\n",
      "Iteration: 87500. Loss: 0.41116002202033997. Accuracy: 77.895\n",
      "Iteration: 88000. Loss: 0.45536643266677856. Accuracy: 77.9605\n",
      "Iteration: 88500. Loss: 0.3450521230697632. Accuracy: 77.611\n",
      "Iteration: 89000. Loss: 0.46096500754356384. Accuracy: 77.8735\n",
      "Iteration: 89500. Loss: 0.47282928228378296. Accuracy: 77.036\n",
      "Iteration: 90000. Loss: 0.41876304149627686. Accuracy: 77.3375\n",
      "Iteration: 90500. Loss: 0.48841384053230286. Accuracy: 77.9445\n",
      "Iteration: 91000. Loss: 0.43639007210731506. Accuracy: 77.976\n",
      "Iteration: 91500. Loss: 0.5536815524101257. Accuracy: 78.094\n",
      "Iteration: 92000. Loss: 0.4785274565219879. Accuracy: 78.028\n",
      "Iteration: 92500. Loss: 0.43170392513275146. Accuracy: 78.155\n",
      "Iteration: 93000. Loss: 0.4243020713329315. Accuracy: 78.223\n",
      "Iteration: 93500. Loss: 0.3463442921638489. Accuracy: 78.1225\n",
      "Iteration: 94000. Loss: 0.3789159655570984. Accuracy: 77.8245\n",
      "Iteration: 94500. Loss: 0.40367111563682556. Accuracy: 77.9145\n",
      "Iteration: 95000. Loss: 0.499104768037796. Accuracy: 78.062\n",
      "Iteration: 95500. Loss: 0.3663290739059448. Accuracy: 77.7725\n",
      "Iteration: 96000. Loss: 0.4044347107410431. Accuracy: 77.8565\n",
      "Iteration: 96500. Loss: 0.2985087037086487. Accuracy: 77.9105\n",
      "Iteration: 97000. Loss: 0.3052647113800049. Accuracy: 78.2685\n",
      "Iteration: 97500. Loss: 0.3901713192462921. Accuracy: 78.1935\n",
      "Iteration: 98000. Loss: 0.5452450513839722. Accuracy: 76.202\n",
      "Iteration: 98500. Loss: 0.24894893169403076. Accuracy: 76.407\n",
      "Iteration: 99000. Loss: 0.46170738339424133. Accuracy: 78.184\n",
      "Iteration: 99500. Loss: 0.3744105398654938. Accuracy: 77.9375\n",
      "Iteration: 100000. Loss: 0.45924490690231323. Accuracy: 78.1225\n",
      "Iteration: 100500. Loss: 0.24392443895339966. Accuracy: 78.3905\n",
      "Iteration: 101000. Loss: 0.5003319382667542. Accuracy: 78.261\n",
      "Iteration: 101500. Loss: 0.3545472323894501. Accuracy: 78.1415\n",
      "Iteration: 102000. Loss: 0.3428294062614441. Accuracy: 77.819\n",
      "Iteration: 102500. Loss: 0.4464338421821594. Accuracy: 78.1985\n",
      "Iteration: 103000. Loss: 0.2869738042354584. Accuracy: 78.0795\n",
      "Iteration: 103500. Loss: 0.40063944458961487. Accuracy: 78.26\n",
      "Iteration: 104000. Loss: 0.40945082902908325. Accuracy: 77.831\n",
      "Iteration: 104500. Loss: 0.3519587814807892. Accuracy: 77.817\n",
      "Iteration: 105000. Loss: 0.37712278962135315. Accuracy: 78.3215\n",
      "Iteration: 105500. Loss: 0.5326106548309326. Accuracy: 78.118\n",
      "Iteration: 106000. Loss: 0.41459542512893677. Accuracy: 78.236\n",
      "Iteration: 106500. Loss: 0.4354385733604431. Accuracy: 78.149\n",
      "Iteration: 107000. Loss: 0.38885533809661865. Accuracy: 78.407\n",
      "Iteration: 107500. Loss: 0.37251389026641846. Accuracy: 77.902\n",
      "Iteration: 108000. Loss: 0.36191532015800476. Accuracy: 77.993\n",
      "Iteration: 108500. Loss: 0.41017886996269226. Accuracy: 78.3815\n",
      "Iteration: 109000. Loss: 0.37742578983306885. Accuracy: 78.3165\n",
      "Iteration: 109500. Loss: 0.5201672911643982. Accuracy: 77.8775\n",
      "Iteration: 110000. Loss: 0.48591792583465576. Accuracy: 77.9095\n",
      "Iteration: 110500. Loss: 0.44488173723220825. Accuracy: 78.339\n",
      "Iteration: 111000. Loss: 0.48730072379112244. Accuracy: 78.4125\n",
      "Iteration: 111500. Loss: 0.43923211097717285. Accuracy: 78.405\n",
      "Iteration: 112000. Loss: 0.3558412194252014. Accuracy: 78.492\n",
      "Iteration: 112500. Loss: 0.34754547476768494. Accuracy: 78.192\n",
      "Iteration: 113000. Loss: 0.32491111755371094. Accuracy: 78.209\n",
      "Iteration: 113500. Loss: 0.4667566418647766. Accuracy: 78.2965\n",
      "Iteration: 114000. Loss: 0.41068917512893677. Accuracy: 78.4375\n",
      "Iteration: 114500. Loss: 0.38678714632987976. Accuracy: 78.156\n",
      "Iteration: 115000. Loss: 0.4122002422809601. Accuracy: 78.463\n",
      "Iteration: 115500. Loss: 0.490646630525589. Accuracy: 78.1515\n",
      "Iteration: 116000. Loss: 0.4312775433063507. Accuracy: 78.5655\n",
      "Iteration: 116500. Loss: 0.38074803352355957. Accuracy: 77.5485\n",
      "Iteration: 117000. Loss: 0.3620191514492035. Accuracy: 78.26\n",
      "Iteration: 117500. Loss: 0.40995246171951294. Accuracy: 78.39\n",
      "Iteration: 118000. Loss: 0.494927316904068. Accuracy: 78.5245\n",
      "Iteration: 118500. Loss: 0.5011979937553406. Accuracy: 77.955\n",
      "Iteration: 119000. Loss: 0.3292447328567505. Accuracy: 78.1005\n",
      "Iteration: 119500. Loss: 0.5784053802490234. Accuracy: 78.328\n",
      "Iteration: 120000. Loss: 0.4722411036491394. Accuracy: 78.5785\n",
      "Iteration: 120500. Loss: 0.30414068698883057. Accuracy: 78.5985\n",
      "Iteration: 121000. Loss: 0.47423991560935974. Accuracy: 78.4755\n",
      "Iteration: 121500. Loss: 0.39048826694488525. Accuracy: 78.389\n",
      "Iteration: 122000. Loss: 0.4040611982345581. Accuracy: 78.307\n",
      "Iteration: 122500. Loss: 0.2848817706108093. Accuracy: 78.6875\n",
      "Iteration: 123000. Loss: 0.43254584074020386. Accuracy: 78.317\n",
      "Iteration: 123500. Loss: 0.44313567876815796. Accuracy: 78.568\n",
      "Iteration: 124000. Loss: 0.42189040780067444. Accuracy: 78.613\n",
      "Iteration: 124500. Loss: 0.506364643573761. Accuracy: 78.3675\n",
      "Iteration: 125000. Loss: 0.2903035283088684. Accuracy: 78.588\n",
      "Iteration: 125500. Loss: 0.3109506070613861. Accuracy: 78.761\n",
      "Iteration: 126000. Loss: 0.5079450607299805. Accuracy: 78.4855\n",
      "Iteration: 126500. Loss: 0.40111175179481506. Accuracy: 78.68\n",
      "Iteration: 127000. Loss: 0.4381062686443329. Accuracy: 78.0055\n",
      "Iteration: 127500. Loss: 0.4281945824623108. Accuracy: 78.6805\n",
      "Iteration: 128000. Loss: 0.39301711320877075. Accuracy: 78.712\n",
      "Iteration: 128500. Loss: 0.4836282432079315. Accuracy: 78.31\n",
      "Iteration: 129000. Loss: 0.33681610226631165. Accuracy: 78.487\n",
      "Iteration: 129500. Loss: 0.6024894118309021. Accuracy: 78.403\n",
      "Iteration: 130000. Loss: 0.48509642481803894. Accuracy: 78.775\n",
      "Iteration: 130500. Loss: 0.4202849268913269. Accuracy: 78.1975\n",
      "Iteration: 131000. Loss: 0.41581031680107117. Accuracy: 78.601\n",
      "Iteration: 131500. Loss: 0.4561401605606079. Accuracy: 78.1105\n",
      "Iteration: 132000. Loss: 0.36386895179748535. Accuracy: 78.543\n",
      "Iteration: 132500. Loss: 0.38611355423927307. Accuracy: 78.6005\n",
      "Iteration: 133000. Loss: 0.4258458614349365. Accuracy: 78.702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 133500. Loss: 0.42356419563293457. Accuracy: 78.117\n",
      "Iteration: 134000. Loss: 0.5119556188583374. Accuracy: 78.7765\n",
      "Iteration: 134500. Loss: 0.43109214305877686. Accuracy: 78.662\n",
      "Iteration: 135000. Loss: 0.3544517159461975. Accuracy: 78.7225\n",
      "Iteration: 135500. Loss: 0.31886863708496094. Accuracy: 78.6725\n",
      "Iteration: 136000. Loss: 0.5705563426017761. Accuracy: 78.5495\n",
      "Iteration: 136500. Loss: 0.3238692581653595. Accuracy: 78.6735\n",
      "Iteration: 137000. Loss: 0.5666868090629578. Accuracy: 78.6235\n",
      "Iteration: 137500. Loss: 0.4550609588623047. Accuracy: 78.0205\n",
      "Iteration: 138000. Loss: 0.2299964874982834. Accuracy: 78.5805\n",
      "Iteration: 138500. Loss: 0.3684117794036865. Accuracy: 78.6675\n",
      "Iteration: 139000. Loss: 0.4715268611907959. Accuracy: 78.609\n",
      "Iteration: 139500. Loss: 0.4127715826034546. Accuracy: 78.722\n",
      "Iteration: 140000. Loss: 0.4094434380531311. Accuracy: 78.486\n",
      "Iteration: 140500. Loss: 0.3832798898220062. Accuracy: 78.8095\n",
      "Iteration: 141000. Loss: 0.4863744080066681. Accuracy: 78.981\n",
      "Iteration: 141500. Loss: 0.43378886580467224. Accuracy: 78.5675\n",
      "Iteration: 142000. Loss: 0.5036327242851257. Accuracy: 78.9735\n",
      "Iteration: 142500. Loss: 0.2869010269641876. Accuracy: 78.8915\n",
      "Iteration: 143000. Loss: 0.33189404010772705. Accuracy: 78.894\n",
      "Iteration: 143500. Loss: 0.4882756769657135. Accuracy: 79.059\n",
      "Iteration: 144000. Loss: 0.39151397347450256. Accuracy: 78.465\n",
      "Iteration: 144500. Loss: 0.34849366545677185. Accuracy: 78.7945\n",
      "Iteration: 145000. Loss: 0.41003167629241943. Accuracy: 78.6345\n",
      "Iteration: 145500. Loss: 0.3278621733188629. Accuracy: 78.702\n",
      "Iteration: 146000. Loss: 0.5810648202896118. Accuracy: 78.9715\n",
      "Iteration: 146500. Loss: 0.4630482792854309. Accuracy: 78.7145\n",
      "Iteration: 147000. Loss: 0.39231953024864197. Accuracy: 78.8205\n",
      "Iteration: 147500. Loss: 0.24528618156909943. Accuracy: 78.724\n",
      "Iteration: 148000. Loss: 0.5307003259658813. Accuracy: 78.853\n",
      "Iteration: 148500. Loss: 0.4146311581134796. Accuracy: 78.3975\n",
      "Iteration: 149000. Loss: 0.4013187885284424. Accuracy: 78.5915\n",
      "Iteration: 149500. Loss: 0.3980952501296997. Accuracy: 78.766\n",
      "Iteration: 150000. Loss: 0.3333946466445923. Accuracy: 78.728\n",
      "Iteration: 150500. Loss: 0.39210712909698486. Accuracy: 78.9385\n",
      "Iteration: 151000. Loss: 0.34513989090919495. Accuracy: 78.6605\n",
      "Iteration: 151500. Loss: 0.4182990789413452. Accuracy: 78.753\n",
      "Iteration: 152000. Loss: 0.41671961545944214. Accuracy: 79.005\n",
      "Iteration: 152500. Loss: 0.3482778072357178. Accuracy: 79.041\n",
      "Iteration: 153000. Loss: 0.4304090142250061. Accuracy: 79.089\n",
      "Iteration: 153500. Loss: 0.5103195309638977. Accuracy: 78.818\n",
      "Iteration: 154000. Loss: 0.3856954574584961. Accuracy: 78.8655\n",
      "Iteration: 154500. Loss: 0.4179074764251709. Accuracy: 78.433\n",
      "Iteration: 155000. Loss: 0.2987402081489563. Accuracy: 78.771\n",
      "Iteration: 155500. Loss: 0.4495810568332672. Accuracy: 78.7345\n",
      "Iteration: 156000. Loss: 0.3994982838630676. Accuracy: 78.6955\n",
      "Iteration: 156500. Loss: 0.4935756027698517. Accuracy: 78.861\n",
      "Iteration: 157000. Loss: 0.4329114854335785. Accuracy: 78.883\n",
      "Iteration: 157500. Loss: 0.31045013666152954. Accuracy: 78.9465\n",
      "Iteration: 158000. Loss: 0.33406490087509155. Accuracy: 78.7215\n",
      "Iteration: 158500. Loss: 0.5009685754776001. Accuracy: 78.523\n",
      "Iteration: 159000. Loss: 0.37880897521972656. Accuracy: 79.012\n",
      "Iteration: 159500. Loss: 0.31042566895484924. Accuracy: 78.64\n",
      "Iteration: 160000. Loss: 0.5804165005683899. Accuracy: 78.755\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = torch.tensor(inputs).requires_grad_()\n",
    "        inputs = inputs.to(device)\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        #images = images.view(-1, 28*28).requires_grad_()\n",
    "        labels = labels.to(device)\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = torch.tensor(inputs).requires_grad_()\n",
    "                inputs = inputs.to(device)\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                #images = images.view(-1, 28*28).requires_grad_()\n",
    "                labels = labels.to(device)\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * float(correct) / float(total)\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hidden layer 50 unit, sigmoid act \n",
    "#Iteration: 82000. Loss: 0.5352512001991272. Accuracy: 68.227\n",
    "# 3 hidden layer 100 unit, sigmoid act \n",
    "#Iteration: 82500. Loss: 0.4631064236164093. Accuracy: 79.512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = RNN(1, hidden_size, 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.14166613,  0.07981981,  0.        ,  0.08785756,  0.07682925,\n",
       "         0.121122  ,  0.13210426], dtype=float32), 1)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [5000/800000], Loss: 0.6329\n",
      "Epoch [1/10], Step [10000/800000], Loss: 0.6487\n",
      "Epoch [1/10], Step [15000/800000], Loss: 0.7472\n",
      "Epoch [1/10], Step [20000/800000], Loss: 0.8218\n",
      "Epoch [1/10], Step [25000/800000], Loss: 0.7266\n",
      "Epoch [1/10], Step [30000/800000], Loss: 0.6513\n",
      "Epoch [1/10], Step [35000/800000], Loss: 0.6752\n",
      "Epoch [1/10], Step [40000/800000], Loss: 0.8303\n",
      "Epoch [1/10], Step [45000/800000], Loss: 0.6324\n",
      "Epoch [1/10], Step [50000/800000], Loss: 0.6298\n",
      "Epoch [1/10], Step [55000/800000], Loss: 0.7727\n",
      "Epoch [1/10], Step [60000/800000], Loss: 0.5949\n",
      "Epoch [1/10], Step [65000/800000], Loss: 0.6619\n",
      "Epoch [1/10], Step [70000/800000], Loss: 0.7425\n",
      "Epoch [1/10], Step [75000/800000], Loss: 0.9420\n",
      "Epoch [1/10], Step [80000/800000], Loss: 0.8581\n",
      "Epoch [1/10], Step [85000/800000], Loss: 0.4874\n",
      "Epoch [1/10], Step [90000/800000], Loss: 0.7714\n",
      "Epoch [1/10], Step [95000/800000], Loss: 0.6274\n",
      "Epoch [1/10], Step [100000/800000], Loss: 0.6098\n",
      "Epoch [1/10], Step [105000/800000], Loss: 0.7929\n",
      "Epoch [1/10], Step [110000/800000], Loss: 0.6222\n",
      "Epoch [1/10], Step [115000/800000], Loss: 0.7641\n",
      "Epoch [1/10], Step [120000/800000], Loss: 0.5663\n",
      "Epoch [1/10], Step [125000/800000], Loss: 0.7566\n",
      "Epoch [1/10], Step [130000/800000], Loss: 0.7523\n",
      "Epoch [1/10], Step [135000/800000], Loss: 0.6573\n",
      "Epoch [1/10], Step [140000/800000], Loss: 0.1949\n",
      "Epoch [1/10], Step [145000/800000], Loss: 0.4816\n",
      "Epoch [1/10], Step [150000/800000], Loss: 0.9178\n",
      "Epoch [1/10], Step [155000/800000], Loss: 0.9900\n",
      "Epoch [1/10], Step [160000/800000], Loss: 0.8594\n",
      "Epoch [1/10], Step [165000/800000], Loss: 0.5919\n",
      "Epoch [1/10], Step [170000/800000], Loss: 0.3702\n",
      "Epoch [1/10], Step [175000/800000], Loss: 0.6079\n",
      "Epoch [1/10], Step [180000/800000], Loss: 0.7006\n",
      "Epoch [1/10], Step [185000/800000], Loss: 0.3869\n",
      "Epoch [1/10], Step [190000/800000], Loss: 1.7723\n",
      "Epoch [1/10], Step [195000/800000], Loss: 0.6470\n",
      "Epoch [1/10], Step [200000/800000], Loss: 0.4987\n",
      "Epoch [1/10], Step [205000/800000], Loss: 0.7095\n",
      "Epoch [1/10], Step [210000/800000], Loss: 0.5029\n",
      "Epoch [1/10], Step [215000/800000], Loss: 0.2075\n",
      "Epoch [1/10], Step [220000/800000], Loss: 0.2843\n",
      "Epoch [1/10], Step [225000/800000], Loss: 1.0965\n",
      "Epoch [1/10], Step [230000/800000], Loss: 1.5759\n",
      "Epoch [1/10], Step [235000/800000], Loss: 0.5814\n",
      "Epoch [1/10], Step [240000/800000], Loss: 0.5938\n",
      "Epoch [1/10], Step [245000/800000], Loss: 0.8596\n",
      "Epoch [1/10], Step [250000/800000], Loss: 0.8050\n",
      "Epoch [1/10], Step [255000/800000], Loss: 0.9421\n",
      "Epoch [1/10], Step [260000/800000], Loss: 0.5143\n",
      "Epoch [1/10], Step [265000/800000], Loss: 0.6152\n",
      "Epoch [1/10], Step [270000/800000], Loss: 0.7541\n",
      "Epoch [1/10], Step [275000/800000], Loss: 0.6286\n",
      "Epoch [1/10], Step [280000/800000], Loss: 0.5194\n",
      "Epoch [1/10], Step [285000/800000], Loss: 0.3102\n",
      "Epoch [1/10], Step [290000/800000], Loss: 1.1980\n",
      "Epoch [1/10], Step [295000/800000], Loss: 0.2424\n",
      "Epoch [1/10], Step [300000/800000], Loss: 0.7274\n",
      "Epoch [1/10], Step [305000/800000], Loss: 0.6263\n",
      "Epoch [1/10], Step [310000/800000], Loss: 1.1357\n",
      "Epoch [1/10], Step [315000/800000], Loss: 0.8539\n",
      "Epoch [1/10], Step [320000/800000], Loss: 1.4578\n",
      "Epoch [1/10], Step [325000/800000], Loss: 2.1152\n",
      "Epoch [1/10], Step [330000/800000], Loss: 0.4884\n",
      "Epoch [1/10], Step [335000/800000], Loss: 1.9984\n",
      "Epoch [1/10], Step [340000/800000], Loss: 0.2108\n",
      "Epoch [1/10], Step [345000/800000], Loss: 0.3630\n",
      "Epoch [1/10], Step [350000/800000], Loss: 0.6280\n",
      "Epoch [1/10], Step [355000/800000], Loss: 0.2802\n",
      "Epoch [1/10], Step [360000/800000], Loss: 0.3773\n",
      "Epoch [1/10], Step [365000/800000], Loss: 0.5403\n",
      "Epoch [1/10], Step [370000/800000], Loss: 0.3799\n",
      "Epoch [1/10], Step [375000/800000], Loss: 0.4501\n",
      "Epoch [1/10], Step [380000/800000], Loss: 1.9850\n",
      "Epoch [1/10], Step [385000/800000], Loss: 0.1493\n",
      "Epoch [1/10], Step [390000/800000], Loss: 0.9923\n",
      "Epoch [1/10], Step [395000/800000], Loss: 0.5198\n",
      "Epoch [1/10], Step [400000/800000], Loss: 0.1210\n",
      "Epoch [1/10], Step [405000/800000], Loss: 0.9474\n",
      "Epoch [1/10], Step [410000/800000], Loss: 0.8054\n",
      "Epoch [1/10], Step [415000/800000], Loss: 0.7789\n",
      "Epoch [1/10], Step [420000/800000], Loss: 0.5689\n",
      "Epoch [1/10], Step [425000/800000], Loss: 0.7089\n",
      "Epoch [1/10], Step [430000/800000], Loss: 1.5156\n",
      "Epoch [1/10], Step [435000/800000], Loss: 0.5826\n",
      "Epoch [1/10], Step [440000/800000], Loss: 0.5293\n",
      "Epoch [1/10], Step [445000/800000], Loss: 0.2047\n",
      "Epoch [1/10], Step [450000/800000], Loss: 0.2041\n",
      "Epoch [1/10], Step [455000/800000], Loss: 0.6055\n",
      "Epoch [1/10], Step [460000/800000], Loss: 0.6818\n",
      "Epoch [1/10], Step [465000/800000], Loss: 0.5711\n",
      "Epoch [1/10], Step [470000/800000], Loss: 0.7474\n",
      "Epoch [1/10], Step [475000/800000], Loss: 0.7268\n",
      "Epoch [1/10], Step [480000/800000], Loss: 1.0074\n",
      "Epoch [1/10], Step [485000/800000], Loss: 0.5585\n",
      "Epoch [1/10], Step [490000/800000], Loss: 0.2343\n",
      "Epoch [1/10], Step [495000/800000], Loss: 0.4266\n",
      "Epoch [1/10], Step [500000/800000], Loss: 0.7185\n",
      "Epoch [1/10], Step [505000/800000], Loss: 1.1079\n",
      "Epoch [1/10], Step [510000/800000], Loss: 0.6740\n",
      "Epoch [1/10], Step [515000/800000], Loss: 1.0473\n",
      "Epoch [1/10], Step [520000/800000], Loss: 0.5245\n",
      "Epoch [1/10], Step [525000/800000], Loss: 0.4673\n",
      "Epoch [1/10], Step [530000/800000], Loss: 0.9783\n",
      "Epoch [1/10], Step [535000/800000], Loss: 0.2847\n",
      "Epoch [1/10], Step [540000/800000], Loss: 0.1970\n",
      "Epoch [1/10], Step [545000/800000], Loss: 0.7025\n",
      "Epoch [1/10], Step [550000/800000], Loss: 0.2727\n",
      "Epoch [1/10], Step [555000/800000], Loss: 0.3863\n",
      "Epoch [1/10], Step [560000/800000], Loss: 1.2561\n",
      "Epoch [1/10], Step [565000/800000], Loss: 0.9838\n",
      "Epoch [1/10], Step [570000/800000], Loss: 0.5077\n",
      "Epoch [1/10], Step [575000/800000], Loss: 0.6668\n",
      "Epoch [1/10], Step [580000/800000], Loss: 2.0142\n",
      "Epoch [1/10], Step [585000/800000], Loss: 0.5371\n",
      "Epoch [1/10], Step [590000/800000], Loss: 0.5789\n",
      "Epoch [1/10], Step [595000/800000], Loss: 1.1399\n",
      "Epoch [1/10], Step [600000/800000], Loss: 0.8710\n",
      "Epoch [1/10], Step [605000/800000], Loss: 0.6309\n",
      "Epoch [1/10], Step [610000/800000], Loss: 0.1182\n",
      "Epoch [1/10], Step [615000/800000], Loss: 0.4997\n",
      "Epoch [1/10], Step [620000/800000], Loss: 0.6385\n",
      "Epoch [1/10], Step [625000/800000], Loss: 0.4372\n",
      "Epoch [1/10], Step [630000/800000], Loss: 0.4519\n",
      "Epoch [1/10], Step [635000/800000], Loss: 1.2033\n",
      "Epoch [1/10], Step [640000/800000], Loss: 0.3052\n",
      "Epoch [1/10], Step [645000/800000], Loss: 0.8331\n",
      "Epoch [1/10], Step [650000/800000], Loss: 0.1061\n",
      "Epoch [1/10], Step [655000/800000], Loss: 0.1403\n",
      "Epoch [1/10], Step [660000/800000], Loss: 0.9442\n",
      "Epoch [1/10], Step [665000/800000], Loss: 1.8774\n",
      "Epoch [1/10], Step [670000/800000], Loss: 0.5641\n",
      "Epoch [1/10], Step [675000/800000], Loss: 0.5844\n",
      "Epoch [1/10], Step [680000/800000], Loss: 2.6096\n",
      "Epoch [1/10], Step [685000/800000], Loss: 0.8852\n",
      "Epoch [1/10], Step [690000/800000], Loss: 0.5713\n",
      "Epoch [1/10], Step [695000/800000], Loss: 1.7990\n",
      "Epoch [1/10], Step [700000/800000], Loss: 0.9556\n",
      "Epoch [1/10], Step [705000/800000], Loss: 0.6894\n",
      "Epoch [1/10], Step [710000/800000], Loss: 0.5580\n",
      "Epoch [1/10], Step [715000/800000], Loss: 0.5507\n",
      "Epoch [1/10], Step [720000/800000], Loss: 0.4190\n",
      "Epoch [1/10], Step [725000/800000], Loss: 0.7557\n",
      "Epoch [1/10], Step [730000/800000], Loss: 0.8062\n",
      "Epoch [1/10], Step [735000/800000], Loss: 0.8806\n",
      "Epoch [1/10], Step [740000/800000], Loss: 0.5454\n",
      "Epoch [1/10], Step [745000/800000], Loss: 0.5933\n",
      "Epoch [1/10], Step [750000/800000], Loss: 1.5977\n",
      "Epoch [1/10], Step [755000/800000], Loss: 0.3948\n",
      "Epoch [1/10], Step [760000/800000], Loss: 0.8581\n",
      "Epoch [1/10], Step [765000/800000], Loss: 0.3722\n",
      "Epoch [1/10], Step [770000/800000], Loss: 1.0347\n",
      "Epoch [1/10], Step [775000/800000], Loss: 1.3361\n",
      "Epoch [1/10], Step [780000/800000], Loss: 0.3895\n",
      "Epoch [1/10], Step [785000/800000], Loss: 0.8833\n",
      "Epoch [1/10], Step [790000/800000], Loss: 0.3647\n",
      "Epoch [1/10], Step [795000/800000], Loss: 0.6303\n",
      "Epoch [1/10], Step [800000/800000], Loss: 0.1973\n",
      "Epoch [2/10], Step [5000/800000], Loss: 0.1597\n",
      "Epoch [2/10], Step [10000/800000], Loss: 0.3793\n",
      "Epoch [2/10], Step [15000/800000], Loss: 2.7026\n",
      "Epoch [2/10], Step [20000/800000], Loss: 0.7996\n",
      "Epoch [2/10], Step [25000/800000], Loss: 0.2680\n",
      "Epoch [2/10], Step [30000/800000], Loss: 0.1136\n",
      "Epoch [2/10], Step [35000/800000], Loss: 1.7838\n",
      "Epoch [2/10], Step [40000/800000], Loss: 2.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [45000/800000], Loss: 0.8980\n",
      "Epoch [2/10], Step [50000/800000], Loss: 0.1172\n",
      "Epoch [2/10], Step [55000/800000], Loss: 0.6187\n",
      "Epoch [2/10], Step [60000/800000], Loss: 0.6052\n",
      "Epoch [2/10], Step [65000/800000], Loss: 0.6550\n",
      "Epoch [2/10], Step [70000/800000], Loss: 0.8027\n",
      "Epoch [2/10], Step [75000/800000], Loss: 0.7547\n",
      "Epoch [2/10], Step [80000/800000], Loss: 0.8986\n",
      "Epoch [2/10], Step [85000/800000], Loss: 0.2695\n",
      "Epoch [2/10], Step [90000/800000], Loss: 1.5037\n",
      "Epoch [2/10], Step [95000/800000], Loss: 0.6570\n",
      "Epoch [2/10], Step [100000/800000], Loss: 1.3170\n",
      "Epoch [2/10], Step [105000/800000], Loss: 1.3473\n",
      "Epoch [2/10], Step [110000/800000], Loss: 0.6607\n",
      "Epoch [2/10], Step [115000/800000], Loss: 1.0768\n",
      "Epoch [2/10], Step [120000/800000], Loss: 0.4651\n",
      "Epoch [2/10], Step [125000/800000], Loss: 0.6591\n",
      "Epoch [2/10], Step [130000/800000], Loss: 0.4936\n",
      "Epoch [2/10], Step [135000/800000], Loss: 0.6508\n",
      "Epoch [2/10], Step [140000/800000], Loss: 0.4281\n",
      "Epoch [2/10], Step [145000/800000], Loss: 0.5456\n",
      "Epoch [2/10], Step [150000/800000], Loss: 0.3674\n",
      "Epoch [2/10], Step [155000/800000], Loss: 0.6713\n",
      "Epoch [2/10], Step [160000/800000], Loss: 1.3756\n",
      "Epoch [2/10], Step [165000/800000], Loss: 1.0613\n",
      "Epoch [2/10], Step [170000/800000], Loss: 0.1108\n",
      "Epoch [2/10], Step [175000/800000], Loss: 0.4396\n",
      "Epoch [2/10], Step [180000/800000], Loss: 2.8545\n",
      "Epoch [2/10], Step [185000/800000], Loss: 0.4454\n",
      "Epoch [2/10], Step [190000/800000], Loss: 0.5062\n",
      "Epoch [2/10], Step [195000/800000], Loss: 1.6460\n",
      "Epoch [2/10], Step [200000/800000], Loss: 0.8015\n",
      "Epoch [2/10], Step [205000/800000], Loss: 0.8067\n",
      "Epoch [2/10], Step [210000/800000], Loss: 0.7209\n",
      "Epoch [2/10], Step [215000/800000], Loss: 0.1823\n",
      "Epoch [2/10], Step [220000/800000], Loss: 0.3019\n",
      "Epoch [2/10], Step [225000/800000], Loss: 1.1164\n",
      "Epoch [2/10], Step [230000/800000], Loss: 2.1319\n",
      "Epoch [2/10], Step [235000/800000], Loss: 0.4377\n",
      "Epoch [2/10], Step [240000/800000], Loss: 1.4060\n",
      "Epoch [2/10], Step [245000/800000], Loss: 1.3724\n",
      "Epoch [2/10], Step [250000/800000], Loss: 1.1585\n",
      "Epoch [2/10], Step [255000/800000], Loss: 1.5669\n",
      "Epoch [2/10], Step [260000/800000], Loss: 0.1788\n",
      "Epoch [2/10], Step [265000/800000], Loss: 0.2783\n",
      "Epoch [2/10], Step [270000/800000], Loss: 0.8960\n",
      "Epoch [2/10], Step [275000/800000], Loss: 0.8927\n",
      "Epoch [2/10], Step [280000/800000], Loss: 0.4194\n",
      "Epoch [2/10], Step [285000/800000], Loss: 0.8411\n",
      "Epoch [2/10], Step [290000/800000], Loss: 0.1421\n",
      "Epoch [2/10], Step [295000/800000], Loss: 0.1956\n",
      "Epoch [2/10], Step [300000/800000], Loss: 0.5016\n",
      "Epoch [2/10], Step [305000/800000], Loss: 0.9476\n",
      "Epoch [2/10], Step [310000/800000], Loss: 0.6048\n",
      "Epoch [2/10], Step [315000/800000], Loss: 0.3823\n",
      "Epoch [2/10], Step [320000/800000], Loss: 1.2106\n",
      "Epoch [2/10], Step [325000/800000], Loss: 0.6069\n",
      "Epoch [2/10], Step [330000/800000], Loss: 1.3038\n",
      "Epoch [2/10], Step [335000/800000], Loss: 0.9491\n",
      "Epoch [2/10], Step [340000/800000], Loss: 0.1879\n",
      "Epoch [2/10], Step [345000/800000], Loss: 0.2977\n",
      "Epoch [2/10], Step [350000/800000], Loss: 0.4055\n",
      "Epoch [2/10], Step [355000/800000], Loss: 0.3105\n",
      "Epoch [2/10], Step [360000/800000], Loss: 0.5536\n",
      "Epoch [2/10], Step [365000/800000], Loss: 0.8502\n",
      "Epoch [2/10], Step [370000/800000], Loss: 0.6310\n",
      "Epoch [2/10], Step [375000/800000], Loss: 0.1693\n",
      "Epoch [2/10], Step [380000/800000], Loss: 1.6230\n",
      "Epoch [2/10], Step [385000/800000], Loss: 0.1288\n",
      "Epoch [2/10], Step [390000/800000], Loss: 1.1406\n",
      "Epoch [2/10], Step [395000/800000], Loss: 0.4936\n",
      "Epoch [2/10], Step [400000/800000], Loss: 0.2318\n",
      "Epoch [2/10], Step [405000/800000], Loss: 0.4007\n",
      "Epoch [2/10], Step [410000/800000], Loss: 0.7899\n",
      "Epoch [2/10], Step [415000/800000], Loss: 0.8575\n",
      "Epoch [2/10], Step [420000/800000], Loss: 1.1536\n",
      "Epoch [2/10], Step [425000/800000], Loss: 1.2303\n",
      "Epoch [2/10], Step [430000/800000], Loss: 1.0162\n",
      "Epoch [2/10], Step [435000/800000], Loss: 0.0796\n",
      "Epoch [2/10], Step [440000/800000], Loss: 0.3044\n",
      "Epoch [2/10], Step [445000/800000], Loss: 2.5714\n",
      "Epoch [2/10], Step [450000/800000], Loss: 0.0925\n",
      "Epoch [2/10], Step [455000/800000], Loss: 0.0818\n",
      "Epoch [2/10], Step [460000/800000], Loss: 0.9965\n",
      "Epoch [2/10], Step [465000/800000], Loss: 0.5197\n",
      "Epoch [2/10], Step [470000/800000], Loss: 0.7699\n",
      "Epoch [2/10], Step [475000/800000], Loss: 0.3946\n",
      "Epoch [2/10], Step [480000/800000], Loss: 0.4689\n",
      "Epoch [2/10], Step [485000/800000], Loss: 0.5368\n",
      "Epoch [2/10], Step [490000/800000], Loss: 0.3113\n",
      "Epoch [2/10], Step [495000/800000], Loss: 0.2768\n",
      "Epoch [2/10], Step [500000/800000], Loss: 0.6161\n",
      "Epoch [2/10], Step [505000/800000], Loss: 2.3837\n",
      "Epoch [2/10], Step [510000/800000], Loss: 1.0018\n",
      "Epoch [2/10], Step [515000/800000], Loss: 1.5029\n",
      "Epoch [2/10], Step [520000/800000], Loss: 0.2312\n",
      "Epoch [2/10], Step [525000/800000], Loss: 1.5865\n",
      "Epoch [2/10], Step [530000/800000], Loss: 2.5877\n",
      "Epoch [2/10], Step [535000/800000], Loss: 1.3785\n",
      "Epoch [2/10], Step [540000/800000], Loss: 0.3247\n",
      "Epoch [2/10], Step [545000/800000], Loss: 0.4387\n",
      "Epoch [2/10], Step [550000/800000], Loss: 0.3421\n",
      "Epoch [2/10], Step [555000/800000], Loss: 0.0786\n",
      "Epoch [2/10], Step [560000/800000], Loss: 1.0565\n",
      "Epoch [2/10], Step [565000/800000], Loss: 0.1819\n",
      "Epoch [2/10], Step [570000/800000], Loss: 1.0053\n",
      "Epoch [2/10], Step [575000/800000], Loss: 0.8134\n",
      "Epoch [2/10], Step [580000/800000], Loss: 1.4112\n",
      "Epoch [2/10], Step [585000/800000], Loss: 0.6021\n",
      "Epoch [2/10], Step [590000/800000], Loss: 1.4770\n",
      "Epoch [2/10], Step [595000/800000], Loss: 2.5879\n",
      "Epoch [2/10], Step [600000/800000], Loss: 1.5244\n",
      "Epoch [2/10], Step [605000/800000], Loss: 0.1761\n",
      "Epoch [2/10], Step [610000/800000], Loss: 0.9085\n",
      "Epoch [2/10], Step [615000/800000], Loss: 0.2842\n",
      "Epoch [2/10], Step [620000/800000], Loss: 0.0967\n",
      "Epoch [2/10], Step [625000/800000], Loss: 2.5058\n",
      "Epoch [2/10], Step [630000/800000], Loss: 0.9176\n",
      "Epoch [2/10], Step [635000/800000], Loss: 2.1888\n",
      "Epoch [2/10], Step [640000/800000], Loss: 0.9027\n",
      "Epoch [2/10], Step [645000/800000], Loss: 0.2165\n",
      "Epoch [2/10], Step [650000/800000], Loss: 0.6290\n",
      "Epoch [2/10], Step [655000/800000], Loss: 0.1825\n",
      "Epoch [2/10], Step [660000/800000], Loss: 0.3132\n",
      "Epoch [2/10], Step [665000/800000], Loss: 0.8147\n",
      "Epoch [2/10], Step [670000/800000], Loss: 1.1874\n",
      "Epoch [2/10], Step [675000/800000], Loss: 0.4253\n",
      "Epoch [2/10], Step [680000/800000], Loss: 0.9554\n",
      "Epoch [2/10], Step [685000/800000], Loss: 1.5634\n",
      "Epoch [2/10], Step [690000/800000], Loss: 0.6819\n",
      "Epoch [2/10], Step [695000/800000], Loss: 0.5721\n",
      "Epoch [2/10], Step [700000/800000], Loss: 1.6775\n",
      "Epoch [2/10], Step [705000/800000], Loss: 0.9748\n",
      "Epoch [2/10], Step [710000/800000], Loss: 0.6212\n",
      "Epoch [2/10], Step [715000/800000], Loss: 0.4440\n",
      "Epoch [2/10], Step [720000/800000], Loss: 0.6567\n",
      "Epoch [2/10], Step [725000/800000], Loss: 0.4766\n",
      "Epoch [2/10], Step [730000/800000], Loss: 0.9966\n",
      "Epoch [2/10], Step [735000/800000], Loss: 0.9958\n",
      "Epoch [2/10], Step [740000/800000], Loss: 1.0629\n",
      "Epoch [2/10], Step [745000/800000], Loss: 0.7840\n",
      "Epoch [2/10], Step [750000/800000], Loss: 1.2009\n",
      "Epoch [2/10], Step [755000/800000], Loss: 0.5702\n",
      "Epoch [2/10], Step [760000/800000], Loss: 0.6706\n",
      "Epoch [2/10], Step [765000/800000], Loss: 0.2311\n",
      "Epoch [2/10], Step [770000/800000], Loss: 1.6493\n",
      "Epoch [2/10], Step [775000/800000], Loss: 0.9587\n",
      "Epoch [2/10], Step [780000/800000], Loss: 1.7442\n",
      "Epoch [2/10], Step [785000/800000], Loss: 0.0734\n",
      "Epoch [2/10], Step [790000/800000], Loss: 0.0518\n",
      "Epoch [2/10], Step [795000/800000], Loss: 0.9342\n",
      "Epoch [2/10], Step [800000/800000], Loss: 1.4752\n",
      "Epoch [3/10], Step [5000/800000], Loss: 0.3283\n",
      "Epoch [3/10], Step [10000/800000], Loss: 0.5177\n",
      "Epoch [3/10], Step [15000/800000], Loss: 0.6432\n",
      "Epoch [3/10], Step [20000/800000], Loss: 1.8847\n",
      "Epoch [3/10], Step [25000/800000], Loss: 1.0753\n",
      "Epoch [3/10], Step [30000/800000], Loss: 0.4208\n",
      "Epoch [3/10], Step [35000/800000], Loss: 1.9746\n",
      "Epoch [3/10], Step [40000/800000], Loss: 1.7949\n",
      "Epoch [3/10], Step [45000/800000], Loss: 0.4848\n",
      "Epoch [3/10], Step [50000/800000], Loss: 0.2400\n",
      "Epoch [3/10], Step [55000/800000], Loss: 1.3134\n",
      "Epoch [3/10], Step [60000/800000], Loss: 0.9298\n",
      "Epoch [3/10], Step [65000/800000], Loss: 0.5560\n",
      "Epoch [3/10], Step [70000/800000], Loss: 0.4756\n",
      "Epoch [3/10], Step [75000/800000], Loss: 0.4715\n",
      "Epoch [3/10], Step [80000/800000], Loss: 0.4498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [85000/800000], Loss: 0.1474\n",
      "Epoch [3/10], Step [90000/800000], Loss: 0.7674\n",
      "Epoch [3/10], Step [95000/800000], Loss: 0.4719\n",
      "Epoch [3/10], Step [100000/800000], Loss: 0.5977\n",
      "Epoch [3/10], Step [105000/800000], Loss: 0.2345\n",
      "Epoch [3/10], Step [110000/800000], Loss: 0.4220\n",
      "Epoch [3/10], Step [115000/800000], Loss: 0.8335\n",
      "Epoch [3/10], Step [120000/800000], Loss: 0.3593\n",
      "Epoch [3/10], Step [125000/800000], Loss: 0.6792\n",
      "Epoch [3/10], Step [130000/800000], Loss: 0.5850\n",
      "Epoch [3/10], Step [135000/800000], Loss: 0.5780\n",
      "Epoch [3/10], Step [140000/800000], Loss: 0.3556\n",
      "Epoch [3/10], Step [145000/800000], Loss: 0.5742\n",
      "Epoch [3/10], Step [150000/800000], Loss: 0.7013\n",
      "Epoch [3/10], Step [155000/800000], Loss: 0.3379\n",
      "Epoch [3/10], Step [160000/800000], Loss: 0.6204\n",
      "Epoch [3/10], Step [165000/800000], Loss: 0.8273\n",
      "Epoch [3/10], Step [170000/800000], Loss: 0.1710\n",
      "Epoch [3/10], Step [175000/800000], Loss: 0.5763\n",
      "Epoch [3/10], Step [180000/800000], Loss: 1.7285\n",
      "Epoch [3/10], Step [185000/800000], Loss: 0.3942\n",
      "Epoch [3/10], Step [190000/800000], Loss: 0.9666\n",
      "Epoch [3/10], Step [195000/800000], Loss: 1.2326\n",
      "Epoch [3/10], Step [200000/800000], Loss: 0.2820\n",
      "Epoch [3/10], Step [205000/800000], Loss: 0.9082\n",
      "Epoch [3/10], Step [210000/800000], Loss: 4.0088\n",
      "Epoch [3/10], Step [215000/800000], Loss: 0.1114\n",
      "Epoch [3/10], Step [220000/800000], Loss: 0.4736\n",
      "Epoch [3/10], Step [225000/800000], Loss: 0.6774\n",
      "Epoch [3/10], Step [230000/800000], Loss: 1.8009\n",
      "Epoch [3/10], Step [235000/800000], Loss: 0.2595\n",
      "Epoch [3/10], Step [240000/800000], Loss: 2.1911\n",
      "Epoch [3/10], Step [245000/800000], Loss: 0.8717\n",
      "Epoch [3/10], Step [250000/800000], Loss: 0.2182\n",
      "Epoch [3/10], Step [255000/800000], Loss: 2.0310\n",
      "Epoch [3/10], Step [260000/800000], Loss: 0.2063\n",
      "Epoch [3/10], Step [265000/800000], Loss: 0.4598\n",
      "Epoch [3/10], Step [270000/800000], Loss: 0.7869\n",
      "Epoch [3/10], Step [275000/800000], Loss: 1.2904\n",
      "Epoch [3/10], Step [280000/800000], Loss: 0.1070\n",
      "Epoch [3/10], Step [285000/800000], Loss: 0.9192\n",
      "Epoch [3/10], Step [290000/800000], Loss: 0.1904\n",
      "Epoch [3/10], Step [295000/800000], Loss: 0.4355\n",
      "Epoch [3/10], Step [300000/800000], Loss: 1.5931\n",
      "Epoch [3/10], Step [305000/800000], Loss: 0.6574\n",
      "Epoch [3/10], Step [310000/800000], Loss: 1.6845\n",
      "Epoch [3/10], Step [315000/800000], Loss: 0.6696\n",
      "Epoch [3/10], Step [320000/800000], Loss: 1.2026\n",
      "Epoch [3/10], Step [325000/800000], Loss: 1.5733\n",
      "Epoch [3/10], Step [330000/800000], Loss: 0.6356\n",
      "Epoch [3/10], Step [335000/800000], Loss: 3.5760\n",
      "Epoch [3/10], Step [340000/800000], Loss: 0.2494\n",
      "Epoch [3/10], Step [345000/800000], Loss: 0.3457\n",
      "Epoch [3/10], Step [350000/800000], Loss: 0.6273\n",
      "Epoch [3/10], Step [355000/800000], Loss: 1.0664\n",
      "Epoch [3/10], Step [360000/800000], Loss: 0.4513\n",
      "Epoch [3/10], Step [365000/800000], Loss: 0.4188\n",
      "Epoch [3/10], Step [370000/800000], Loss: 0.2708\n",
      "Epoch [3/10], Step [375000/800000], Loss: 0.4024\n",
      "Epoch [3/10], Step [380000/800000], Loss: 1.1260\n",
      "Epoch [3/10], Step [385000/800000], Loss: 0.4521\n",
      "Epoch [3/10], Step [390000/800000], Loss: 1.7829\n",
      "Epoch [3/10], Step [395000/800000], Loss: 0.4540\n",
      "Epoch [3/10], Step [400000/800000], Loss: 0.0885\n",
      "Epoch [3/10], Step [405000/800000], Loss: 0.8290\n",
      "Epoch [3/10], Step [410000/800000], Loss: 1.0025\n",
      "Epoch [3/10], Step [415000/800000], Loss: 0.2002\n",
      "Epoch [3/10], Step [420000/800000], Loss: 0.5292\n",
      "Epoch [3/10], Step [425000/800000], Loss: 0.3312\n",
      "Epoch [3/10], Step [430000/800000], Loss: 1.0273\n",
      "Epoch [3/10], Step [435000/800000], Loss: 0.5177\n",
      "Epoch [3/10], Step [440000/800000], Loss: 0.8720\n",
      "Epoch [3/10], Step [445000/800000], Loss: 0.8604\n",
      "Epoch [3/10], Step [450000/800000], Loss: 0.5589\n",
      "Epoch [3/10], Step [455000/800000], Loss: 1.0109\n",
      "Epoch [3/10], Step [460000/800000], Loss: 0.7014\n",
      "Epoch [3/10], Step [465000/800000], Loss: 0.3274\n",
      "Epoch [3/10], Step [470000/800000], Loss: 0.4178\n",
      "Epoch [3/10], Step [475000/800000], Loss: 0.5229\n",
      "Epoch [3/10], Step [480000/800000], Loss: 0.8285\n",
      "Epoch [3/10], Step [485000/800000], Loss: 0.3828\n",
      "Epoch [3/10], Step [490000/800000], Loss: 0.2943\n",
      "Epoch [3/10], Step [495000/800000], Loss: 0.8176\n",
      "Epoch [3/10], Step [500000/800000], Loss: 0.3124\n",
      "Epoch [3/10], Step [505000/800000], Loss: 1.7051\n",
      "Epoch [3/10], Step [510000/800000], Loss: 0.9355\n",
      "Epoch [3/10], Step [515000/800000], Loss: 0.8925\n",
      "Epoch [3/10], Step [520000/800000], Loss: 0.4545\n",
      "Epoch [3/10], Step [525000/800000], Loss: 0.7945\n",
      "Epoch [3/10], Step [530000/800000], Loss: 1.5020\n",
      "Epoch [3/10], Step [535000/800000], Loss: 0.6317\n",
      "Epoch [3/10], Step [540000/800000], Loss: 0.2066\n",
      "Epoch [3/10], Step [545000/800000], Loss: 0.8923\n",
      "Epoch [3/10], Step [550000/800000], Loss: 0.3141\n",
      "Epoch [3/10], Step [555000/800000], Loss: 0.3719\n",
      "Epoch [3/10], Step [560000/800000], Loss: 0.9935\n",
      "Epoch [3/10], Step [565000/800000], Loss: 0.7925\n",
      "Epoch [3/10], Step [570000/800000], Loss: 0.4670\n",
      "Epoch [3/10], Step [575000/800000], Loss: 0.4610\n",
      "Epoch [3/10], Step [580000/800000], Loss: 3.0804\n",
      "Epoch [3/10], Step [585000/800000], Loss: 1.0731\n",
      "Epoch [3/10], Step [590000/800000], Loss: 0.2091\n",
      "Epoch [3/10], Step [595000/800000], Loss: 2.3033\n",
      "Epoch [3/10], Step [600000/800000], Loss: 1.1300\n",
      "Epoch [3/10], Step [605000/800000], Loss: 0.2186\n",
      "Epoch [3/10], Step [610000/800000], Loss: 0.4634\n",
      "Epoch [3/10], Step [615000/800000], Loss: 0.8670\n",
      "Epoch [3/10], Step [620000/800000], Loss: 0.6884\n",
      "Epoch [3/10], Step [625000/800000], Loss: 0.3487\n",
      "Epoch [3/10], Step [630000/800000], Loss: 1.1478\n",
      "Epoch [3/10], Step [635000/800000], Loss: 1.9096\n",
      "Epoch [3/10], Step [640000/800000], Loss: 0.3568\n",
      "Epoch [3/10], Step [645000/800000], Loss: 0.8974\n",
      "Epoch [3/10], Step [650000/800000], Loss: 0.2028\n",
      "Epoch [3/10], Step [655000/800000], Loss: 0.0762\n",
      "Epoch [3/10], Step [660000/800000], Loss: 0.6091\n",
      "Epoch [3/10], Step [665000/800000], Loss: 1.0142\n",
      "Epoch [3/10], Step [670000/800000], Loss: 0.9161\n",
      "Epoch [3/10], Step [675000/800000], Loss: 0.4667\n",
      "Epoch [3/10], Step [680000/800000], Loss: 0.4858\n",
      "Epoch [3/10], Step [685000/800000], Loss: 1.0961\n",
      "Epoch [3/10], Step [690000/800000], Loss: 0.2920\n",
      "Epoch [3/10], Step [695000/800000], Loss: 1.8376\n",
      "Epoch [3/10], Step [700000/800000], Loss: 1.1969\n",
      "Epoch [3/10], Step [705000/800000], Loss: 0.4198\n",
      "Epoch [3/10], Step [710000/800000], Loss: 0.4959\n",
      "Epoch [3/10], Step [715000/800000], Loss: 0.2972\n",
      "Epoch [3/10], Step [720000/800000], Loss: 0.4209\n",
      "Epoch [3/10], Step [725000/800000], Loss: 0.6030\n",
      "Epoch [3/10], Step [730000/800000], Loss: 1.0848\n",
      "Epoch [3/10], Step [735000/800000], Loss: 1.3648\n",
      "Epoch [3/10], Step [740000/800000], Loss: 0.4489\n",
      "Epoch [3/10], Step [745000/800000], Loss: 0.4291\n",
      "Epoch [3/10], Step [750000/800000], Loss: 0.7912\n",
      "Epoch [3/10], Step [755000/800000], Loss: 0.8169\n",
      "Epoch [3/10], Step [760000/800000], Loss: 0.8409\n",
      "Epoch [3/10], Step [765000/800000], Loss: 0.6519\n",
      "Epoch [3/10], Step [770000/800000], Loss: 1.3484\n",
      "Epoch [3/10], Step [775000/800000], Loss: 0.5694\n",
      "Epoch [3/10], Step [780000/800000], Loss: 0.1178\n",
      "Epoch [3/10], Step [785000/800000], Loss: 0.2312\n",
      "Epoch [3/10], Step [790000/800000], Loss: 0.3023\n",
      "Epoch [3/10], Step [795000/800000], Loss: 0.5297\n",
      "Epoch [3/10], Step [800000/800000], Loss: 0.6500\n",
      "Epoch [4/10], Step [5000/800000], Loss: 0.8532\n",
      "Epoch [4/10], Step [10000/800000], Loss: 1.3449\n",
      "Epoch [4/10], Step [15000/800000], Loss: 0.6713\n",
      "Epoch [4/10], Step [20000/800000], Loss: 0.7955\n",
      "Epoch [4/10], Step [25000/800000], Loss: 0.3452\n",
      "Epoch [4/10], Step [30000/800000], Loss: 0.4020\n",
      "Epoch [4/10], Step [35000/800000], Loss: 1.3890\n",
      "Epoch [4/10], Step [40000/800000], Loss: 1.3219\n",
      "Epoch [4/10], Step [45000/800000], Loss: 0.6140\n",
      "Epoch [4/10], Step [50000/800000], Loss: 0.1347\n",
      "Epoch [4/10], Step [55000/800000], Loss: 0.9664\n",
      "Epoch [4/10], Step [60000/800000], Loss: 0.4673\n",
      "Epoch [4/10], Step [65000/800000], Loss: 0.3559\n",
      "Epoch [4/10], Step [70000/800000], Loss: 1.2746\n",
      "Epoch [4/10], Step [75000/800000], Loss: 0.8171\n",
      "Epoch [4/10], Step [80000/800000], Loss: 1.0012\n",
      "Epoch [4/10], Step [85000/800000], Loss: 0.4233\n",
      "Epoch [4/10], Step [90000/800000], Loss: 1.2117\n",
      "Epoch [4/10], Step [95000/800000], Loss: 0.7616\n",
      "Epoch [4/10], Step [100000/800000], Loss: 0.7683\n",
      "Epoch [4/10], Step [105000/800000], Loss: 0.9749\n",
      "Epoch [4/10], Step [110000/800000], Loss: 0.8669\n",
      "Epoch [4/10], Step [115000/800000], Loss: 1.1685\n",
      "Epoch [4/10], Step [120000/800000], Loss: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [125000/800000], Loss: 0.8849\n",
      "Epoch [4/10], Step [130000/800000], Loss: 0.2872\n",
      "Epoch [4/10], Step [135000/800000], Loss: 0.3609\n",
      "Epoch [4/10], Step [140000/800000], Loss: 0.5454\n",
      "Epoch [4/10], Step [145000/800000], Loss: 0.4992\n",
      "Epoch [4/10], Step [150000/800000], Loss: 0.8926\n",
      "Epoch [4/10], Step [155000/800000], Loss: 0.4288\n",
      "Epoch [4/10], Step [160000/800000], Loss: 1.0726\n",
      "Epoch [4/10], Step [165000/800000], Loss: 0.9524\n",
      "Epoch [4/10], Step [170000/800000], Loss: 0.4012\n",
      "Epoch [4/10], Step [175000/800000], Loss: 1.7717\n",
      "Epoch [4/10], Step [180000/800000], Loss: 1.6723\n",
      "Epoch [4/10], Step [185000/800000], Loss: 0.4828\n",
      "Epoch [4/10], Step [190000/800000], Loss: 0.5812\n",
      "Epoch [4/10], Step [195000/800000], Loss: 0.6662\n",
      "Epoch [4/10], Step [200000/800000], Loss: 0.7631\n",
      "Epoch [4/10], Step [205000/800000], Loss: 0.6789\n",
      "Epoch [4/10], Step [210000/800000], Loss: 1.3662\n",
      "Epoch [4/10], Step [215000/800000], Loss: 0.1320\n",
      "Epoch [4/10], Step [220000/800000], Loss: 1.1024\n",
      "Epoch [4/10], Step [225000/800000], Loss: 0.6244\n",
      "Epoch [4/10], Step [230000/800000], Loss: 1.2359\n",
      "Epoch [4/10], Step [235000/800000], Loss: 0.8126\n",
      "Epoch [4/10], Step [240000/800000], Loss: 0.6460\n",
      "Epoch [4/10], Step [245000/800000], Loss: 1.7495\n",
      "Epoch [4/10], Step [250000/800000], Loss: 0.4038\n",
      "Epoch [4/10], Step [255000/800000], Loss: 1.7919\n",
      "Epoch [4/10], Step [260000/800000], Loss: 0.2346\n",
      "Epoch [4/10], Step [265000/800000], Loss: 0.6657\n",
      "Epoch [4/10], Step [270000/800000], Loss: 0.5913\n",
      "Epoch [4/10], Step [275000/800000], Loss: 0.7464\n",
      "Epoch [4/10], Step [280000/800000], Loss: 0.5659\n",
      "Epoch [4/10], Step [285000/800000], Loss: 0.7611\n",
      "Epoch [4/10], Step [290000/800000], Loss: 0.2268\n",
      "Epoch [4/10], Step [295000/800000], Loss: 0.2085\n",
      "Epoch [4/10], Step [300000/800000], Loss: 0.6965\n",
      "Epoch [4/10], Step [305000/800000], Loss: 0.6386\n",
      "Epoch [4/10], Step [310000/800000], Loss: 0.3674\n",
      "Epoch [4/10], Step [315000/800000], Loss: 1.2038\n",
      "Epoch [4/10], Step [320000/800000], Loss: 1.3963\n",
      "Epoch [4/10], Step [325000/800000], Loss: 1.3761\n",
      "Epoch [4/10], Step [330000/800000], Loss: 0.2296\n",
      "Epoch [4/10], Step [335000/800000], Loss: 1.2034\n",
      "Epoch [4/10], Step [340000/800000], Loss: 0.2319\n",
      "Epoch [4/10], Step [345000/800000], Loss: 0.5221\n",
      "Epoch [4/10], Step [350000/800000], Loss: 0.2786\n",
      "Epoch [4/10], Step [355000/800000], Loss: 0.5970\n",
      "Epoch [4/10], Step [360000/800000], Loss: 0.2402\n",
      "Epoch [4/10], Step [365000/800000], Loss: 0.1991\n",
      "Epoch [4/10], Step [370000/800000], Loss: 0.4237\n",
      "Epoch [4/10], Step [375000/800000], Loss: 0.3968\n",
      "Epoch [4/10], Step [380000/800000], Loss: 0.9612\n",
      "Epoch [4/10], Step [385000/800000], Loss: 0.6449\n",
      "Epoch [4/10], Step [390000/800000], Loss: 0.7535\n",
      "Epoch [4/10], Step [395000/800000], Loss: 0.2912\n",
      "Epoch [4/10], Step [400000/800000], Loss: 0.1730\n",
      "Epoch [4/10], Step [405000/800000], Loss: 1.0724\n",
      "Epoch [4/10], Step [410000/800000], Loss: 1.0487\n",
      "Epoch [4/10], Step [415000/800000], Loss: 0.6972\n",
      "Epoch [4/10], Step [420000/800000], Loss: 0.8138\n",
      "Epoch [4/10], Step [425000/800000], Loss: 0.7584\n",
      "Epoch [4/10], Step [430000/800000], Loss: 1.1000\n",
      "Epoch [4/10], Step [435000/800000], Loss: 0.4510\n",
      "Epoch [4/10], Step [440000/800000], Loss: 0.2900\n",
      "Epoch [4/10], Step [445000/800000], Loss: 0.3375\n",
      "Epoch [4/10], Step [450000/800000], Loss: 0.2656\n",
      "Epoch [4/10], Step [455000/800000], Loss: 0.7097\n",
      "Epoch [4/10], Step [460000/800000], Loss: 0.3437\n",
      "Epoch [4/10], Step [465000/800000], Loss: 0.8517\n",
      "Epoch [4/10], Step [470000/800000], Loss: 0.3828\n",
      "Epoch [4/10], Step [475000/800000], Loss: 0.7335\n",
      "Epoch [4/10], Step [480000/800000], Loss: 0.7973\n",
      "Epoch [4/10], Step [485000/800000], Loss: 0.9358\n",
      "Epoch [4/10], Step [490000/800000], Loss: 0.3411\n",
      "Epoch [4/10], Step [495000/800000], Loss: 0.4355\n",
      "Epoch [4/10], Step [500000/800000], Loss: 0.3492\n",
      "Epoch [4/10], Step [505000/800000], Loss: 2.0830\n",
      "Epoch [4/10], Step [510000/800000], Loss: 0.1614\n",
      "Epoch [4/10], Step [515000/800000], Loss: 2.5748\n",
      "Epoch [4/10], Step [520000/800000], Loss: 0.7020\n",
      "Epoch [4/10], Step [525000/800000], Loss: 0.4158\n",
      "Epoch [4/10], Step [530000/800000], Loss: 0.8427\n",
      "Epoch [4/10], Step [535000/800000], Loss: 0.4878\n",
      "Epoch [4/10], Step [540000/800000], Loss: 0.5162\n",
      "Epoch [4/10], Step [545000/800000], Loss: 0.4001\n",
      "Epoch [4/10], Step [550000/800000], Loss: 0.6130\n",
      "Epoch [4/10], Step [555000/800000], Loss: 0.5233\n",
      "Epoch [4/10], Step [560000/800000], Loss: 1.1883\n",
      "Epoch [4/10], Step [565000/800000], Loss: 0.7198\n",
      "Epoch [4/10], Step [570000/800000], Loss: 0.7909\n",
      "Epoch [4/10], Step [575000/800000], Loss: 0.7996\n",
      "Epoch [4/10], Step [580000/800000], Loss: 1.6766\n",
      "Epoch [4/10], Step [585000/800000], Loss: 1.1444\n",
      "Epoch [4/10], Step [590000/800000], Loss: 0.3964\n",
      "Epoch [4/10], Step [595000/800000], Loss: 1.7252\n",
      "Epoch [4/10], Step [600000/800000], Loss: 1.1088\n",
      "Epoch [4/10], Step [605000/800000], Loss: 0.3760\n",
      "Epoch [4/10], Step [610000/800000], Loss: 0.5482\n",
      "Epoch [4/10], Step [615000/800000], Loss: 0.6864\n",
      "Epoch [4/10], Step [620000/800000], Loss: 0.6896\n",
      "Epoch [4/10], Step [625000/800000], Loss: 0.4822\n",
      "Epoch [4/10], Step [630000/800000], Loss: 0.6281\n",
      "Epoch [4/10], Step [635000/800000], Loss: 1.1023\n",
      "Epoch [4/10], Step [640000/800000], Loss: 0.5626\n",
      "Epoch [4/10], Step [645000/800000], Loss: 0.4268\n",
      "Epoch [4/10], Step [650000/800000], Loss: 0.2550\n",
      "Epoch [4/10], Step [655000/800000], Loss: 0.1010\n",
      "Epoch [4/10], Step [660000/800000], Loss: 0.9360\n",
      "Epoch [4/10], Step [665000/800000], Loss: 0.3306\n",
      "Epoch [4/10], Step [670000/800000], Loss: 0.1441\n",
      "Epoch [4/10], Step [675000/800000], Loss: 0.5086\n",
      "Epoch [4/10], Step [680000/800000], Loss: 0.5366\n",
      "Epoch [4/10], Step [685000/800000], Loss: 0.3373\n",
      "Epoch [4/10], Step [690000/800000], Loss: 0.5241\n",
      "Epoch [4/10], Step [695000/800000], Loss: 1.2115\n",
      "Epoch [4/10], Step [700000/800000], Loss: 0.9890\n",
      "Epoch [4/10], Step [705000/800000], Loss: 1.7368\n",
      "Epoch [4/10], Step [710000/800000], Loss: 0.5019\n",
      "Epoch [4/10], Step [715000/800000], Loss: 0.0346\n",
      "Epoch [4/10], Step [720000/800000], Loss: 0.6090\n",
      "Epoch [4/10], Step [725000/800000], Loss: 1.2526\n",
      "Epoch [4/10], Step [730000/800000], Loss: 0.8798\n",
      "Epoch [4/10], Step [735000/800000], Loss: 1.8184\n",
      "Epoch [4/10], Step [740000/800000], Loss: 0.8605\n",
      "Epoch [4/10], Step [745000/800000], Loss: 0.2381\n",
      "Epoch [4/10], Step [750000/800000], Loss: 1.1784\n",
      "Epoch [4/10], Step [755000/800000], Loss: 0.2270\n",
      "Epoch [4/10], Step [760000/800000], Loss: 0.9118\n",
      "Epoch [4/10], Step [765000/800000], Loss: 0.1366\n",
      "Epoch [4/10], Step [770000/800000], Loss: 1.3305\n",
      "Epoch [4/10], Step [775000/800000], Loss: 1.1463\n",
      "Epoch [4/10], Step [780000/800000], Loss: 0.2507\n",
      "Epoch [4/10], Step [785000/800000], Loss: 0.8462\n",
      "Epoch [4/10], Step [790000/800000], Loss: 0.2340\n",
      "Epoch [4/10], Step [795000/800000], Loss: 0.9435\n",
      "Epoch [4/10], Step [800000/800000], Loss: 0.8408\n",
      "Epoch [5/10], Step [5000/800000], Loss: 0.4966\n",
      "Epoch [5/10], Step [10000/800000], Loss: 0.6002\n",
      "Epoch [5/10], Step [15000/800000], Loss: 0.5366\n",
      "Epoch [5/10], Step [20000/800000], Loss: 1.8485\n",
      "Epoch [5/10], Step [25000/800000], Loss: 0.9099\n",
      "Epoch [5/10], Step [30000/800000], Loss: 0.3077\n",
      "Epoch [5/10], Step [35000/800000], Loss: 2.6691\n",
      "Epoch [5/10], Step [40000/800000], Loss: 1.0955\n",
      "Epoch [5/10], Step [45000/800000], Loss: 0.3793\n",
      "Epoch [5/10], Step [50000/800000], Loss: 0.1756\n",
      "Epoch [5/10], Step [55000/800000], Loss: 0.5926\n",
      "Epoch [5/10], Step [60000/800000], Loss: 0.4063\n",
      "Epoch [5/10], Step [65000/800000], Loss: 0.7568\n",
      "Epoch [5/10], Step [70000/800000], Loss: 0.6521\n",
      "Epoch [5/10], Step [75000/800000], Loss: 0.4244\n",
      "Epoch [5/10], Step [80000/800000], Loss: 0.9051\n",
      "Epoch [5/10], Step [85000/800000], Loss: 0.4607\n",
      "Epoch [5/10], Step [90000/800000], Loss: 1.1209\n",
      "Epoch [5/10], Step [95000/800000], Loss: 0.5779\n",
      "Epoch [5/10], Step [100000/800000], Loss: 0.7637\n",
      "Epoch [5/10], Step [105000/800000], Loss: 0.7373\n",
      "Epoch [5/10], Step [110000/800000], Loss: 0.4591\n",
      "Epoch [5/10], Step [115000/800000], Loss: 0.8190\n",
      "Epoch [5/10], Step [120000/800000], Loss: 0.7397\n",
      "Epoch [5/10], Step [125000/800000], Loss: 0.8702\n",
      "Epoch [5/10], Step [130000/800000], Loss: 0.7855\n",
      "Epoch [5/10], Step [135000/800000], Loss: 0.5413\n",
      "Epoch [5/10], Step [140000/800000], Loss: 0.7548\n",
      "Epoch [5/10], Step [145000/800000], Loss: 0.5934\n",
      "Epoch [5/10], Step [150000/800000], Loss: 1.2564\n",
      "Epoch [5/10], Step [155000/800000], Loss: 0.1487\n",
      "Epoch [5/10], Step [160000/800000], Loss: 1.3557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [165000/800000], Loss: 0.7462\n",
      "Epoch [5/10], Step [170000/800000], Loss: 0.2074\n",
      "Epoch [5/10], Step [175000/800000], Loss: 0.9839\n",
      "Epoch [5/10], Step [180000/800000], Loss: 1.7769\n",
      "Epoch [5/10], Step [185000/800000], Loss: 0.6502\n",
      "Epoch [5/10], Step [190000/800000], Loss: 0.5439\n",
      "Epoch [5/10], Step [195000/800000], Loss: 1.2408\n",
      "Epoch [5/10], Step [200000/800000], Loss: 0.5064\n",
      "Epoch [5/10], Step [205000/800000], Loss: 1.0898\n",
      "Epoch [5/10], Step [210000/800000], Loss: 1.2199\n",
      "Epoch [5/10], Step [215000/800000], Loss: 0.1971\n",
      "Epoch [5/10], Step [220000/800000], Loss: 0.0046\n",
      "Epoch [5/10], Step [225000/800000], Loss: 0.6198\n",
      "Epoch [5/10], Step [230000/800000], Loss: 1.6649\n",
      "Epoch [5/10], Step [235000/800000], Loss: 1.1510\n",
      "Epoch [5/10], Step [240000/800000], Loss: 0.9598\n",
      "Epoch [5/10], Step [245000/800000], Loss: 1.2943\n",
      "Epoch [5/10], Step [250000/800000], Loss: 0.9154\n",
      "Epoch [5/10], Step [255000/800000], Loss: 1.5134\n",
      "Epoch [5/10], Step [260000/800000], Loss: 1.8769\n",
      "Epoch [5/10], Step [265000/800000], Loss: 0.3873\n",
      "Epoch [5/10], Step [270000/800000], Loss: 0.4886\n",
      "Epoch [5/10], Step [275000/800000], Loss: 0.5077\n",
      "Epoch [5/10], Step [280000/800000], Loss: 0.1187\n",
      "Epoch [5/10], Step [285000/800000], Loss: 0.5001\n",
      "Epoch [5/10], Step [290000/800000], Loss: 0.7686\n",
      "Epoch [5/10], Step [295000/800000], Loss: 0.1634\n",
      "Epoch [5/10], Step [300000/800000], Loss: 0.6367\n",
      "Epoch [5/10], Step [305000/800000], Loss: 0.6315\n",
      "Epoch [5/10], Step [310000/800000], Loss: 1.0285\n",
      "Epoch [5/10], Step [315000/800000], Loss: 1.5273\n",
      "Epoch [5/10], Step [320000/800000], Loss: 0.9293\n",
      "Epoch [5/10], Step [325000/800000], Loss: 1.8272\n",
      "Epoch [5/10], Step [330000/800000], Loss: 0.3278\n",
      "Epoch [5/10], Step [335000/800000], Loss: 3.0214\n",
      "Epoch [5/10], Step [340000/800000], Loss: 0.3456\n",
      "Epoch [5/10], Step [345000/800000], Loss: 0.2382\n",
      "Epoch [5/10], Step [350000/800000], Loss: 0.1007\n",
      "Epoch [5/10], Step [355000/800000], Loss: 0.5283\n",
      "Epoch [5/10], Step [360000/800000], Loss: 0.2523\n",
      "Epoch [5/10], Step [365000/800000], Loss: 0.3738\n",
      "Epoch [5/10], Step [370000/800000], Loss: 0.3419\n",
      "Epoch [5/10], Step [375000/800000], Loss: 0.3246\n",
      "Epoch [5/10], Step [380000/800000], Loss: 1.6229\n",
      "Epoch [5/10], Step [385000/800000], Loss: 0.5865\n",
      "Epoch [5/10], Step [390000/800000], Loss: 0.9106\n",
      "Epoch [5/10], Step [395000/800000], Loss: 0.6823\n",
      "Epoch [5/10], Step [400000/800000], Loss: 0.2203\n",
      "Epoch [5/10], Step [405000/800000], Loss: 0.5712\n",
      "Epoch [5/10], Step [410000/800000], Loss: 1.7297\n",
      "Epoch [5/10], Step [415000/800000], Loss: 0.8209\n",
      "Epoch [5/10], Step [420000/800000], Loss: 0.7855\n",
      "Epoch [5/10], Step [425000/800000], Loss: 1.1415\n",
      "Epoch [5/10], Step [430000/800000], Loss: 1.5495\n",
      "Epoch [5/10], Step [435000/800000], Loss: 0.2133\n",
      "Epoch [5/10], Step [440000/800000], Loss: 1.2918\n",
      "Epoch [5/10], Step [445000/800000], Loss: 0.4458\n",
      "Epoch [5/10], Step [450000/800000], Loss: 0.1687\n",
      "Epoch [5/10], Step [455000/800000], Loss: 1.0083\n",
      "Epoch [5/10], Step [460000/800000], Loss: 0.7082\n",
      "Epoch [5/10], Step [465000/800000], Loss: 0.3331\n",
      "Epoch [5/10], Step [470000/800000], Loss: 0.3512\n",
      "Epoch [5/10], Step [475000/800000], Loss: 0.4232\n",
      "Epoch [5/10], Step [480000/800000], Loss: 0.0158\n",
      "Epoch [5/10], Step [485000/800000], Loss: 0.2608\n",
      "Epoch [5/10], Step [490000/800000], Loss: 0.1810\n",
      "Epoch [5/10], Step [495000/800000], Loss: 0.2074\n",
      "Epoch [5/10], Step [500000/800000], Loss: 0.3219\n",
      "Epoch [5/10], Step [505000/800000], Loss: 0.9530\n",
      "Epoch [5/10], Step [510000/800000], Loss: 0.9182\n",
      "Epoch [5/10], Step [515000/800000], Loss: 2.2589\n",
      "Epoch [5/10], Step [520000/800000], Loss: 1.3830\n",
      "Epoch [5/10], Step [525000/800000], Loss: 0.8247\n",
      "Epoch [5/10], Step [530000/800000], Loss: 1.6434\n",
      "Epoch [5/10], Step [535000/800000], Loss: 1.4227\n",
      "Epoch [5/10], Step [540000/800000], Loss: 0.2887\n",
      "Epoch [5/10], Step [545000/800000], Loss: 0.4432\n",
      "Epoch [5/10], Step [550000/800000], Loss: 0.2449\n",
      "Epoch [5/10], Step [555000/800000], Loss: 0.2428\n",
      "Epoch [5/10], Step [560000/800000], Loss: 1.1788\n",
      "Epoch [5/10], Step [565000/800000], Loss: 0.7652\n",
      "Epoch [5/10], Step [570000/800000], Loss: 0.4654\n",
      "Epoch [5/10], Step [575000/800000], Loss: 0.7302\n",
      "Epoch [5/10], Step [580000/800000], Loss: 1.7290\n",
      "Epoch [5/10], Step [585000/800000], Loss: 0.6394\n",
      "Epoch [5/10], Step [590000/800000], Loss: 0.1458\n",
      "Epoch [5/10], Step [595000/800000], Loss: 0.7690\n",
      "Epoch [5/10], Step [600000/800000], Loss: 1.0759\n",
      "Epoch [5/10], Step [605000/800000], Loss: 0.3534\n",
      "Epoch [5/10], Step [610000/800000], Loss: 1.2025\n",
      "Epoch [5/10], Step [615000/800000], Loss: 0.6066\n",
      "Epoch [5/10], Step [620000/800000], Loss: 0.5127\n",
      "Epoch [5/10], Step [625000/800000], Loss: 0.6180\n",
      "Epoch [5/10], Step [630000/800000], Loss: 0.8184\n",
      "Epoch [5/10], Step [635000/800000], Loss: 0.5340\n",
      "Epoch [5/10], Step [640000/800000], Loss: 0.8260\n",
      "Epoch [5/10], Step [645000/800000], Loss: 0.3665\n",
      "Epoch [5/10], Step [650000/800000], Loss: 0.4228\n",
      "Epoch [5/10], Step [655000/800000], Loss: 0.2974\n",
      "Epoch [5/10], Step [660000/800000], Loss: 0.7006\n",
      "Epoch [5/10], Step [665000/800000], Loss: 0.8299\n",
      "Epoch [5/10], Step [670000/800000], Loss: 0.4912\n",
      "Epoch [5/10], Step [675000/800000], Loss: 0.8302\n",
      "Epoch [5/10], Step [680000/800000], Loss: 1.0966\n",
      "Epoch [5/10], Step [685000/800000], Loss: 0.8737\n",
      "Epoch [5/10], Step [690000/800000], Loss: 0.5221\n",
      "Epoch [5/10], Step [695000/800000], Loss: 0.7961\n",
      "Epoch [5/10], Step [700000/800000], Loss: 0.7297\n",
      "Epoch [5/10], Step [705000/800000], Loss: 0.6580\n",
      "Epoch [5/10], Step [710000/800000], Loss: 0.7185\n",
      "Epoch [5/10], Step [715000/800000], Loss: 0.1738\n",
      "Epoch [5/10], Step [720000/800000], Loss: 0.3601\n",
      "Epoch [5/10], Step [725000/800000], Loss: 1.0779\n",
      "Epoch [5/10], Step [730000/800000], Loss: 0.8815\n",
      "Epoch [5/10], Step [735000/800000], Loss: 1.0628\n",
      "Epoch [5/10], Step [740000/800000], Loss: 0.4677\n",
      "Epoch [5/10], Step [745000/800000], Loss: 0.2597\n",
      "Epoch [5/10], Step [750000/800000], Loss: 0.8375\n",
      "Epoch [5/10], Step [755000/800000], Loss: 0.5762\n",
      "Epoch [5/10], Step [760000/800000], Loss: 0.4964\n",
      "Epoch [5/10], Step [765000/800000], Loss: 0.3147\n",
      "Epoch [5/10], Step [770000/800000], Loss: 1.0009\n",
      "Epoch [5/10], Step [775000/800000], Loss: 0.9304\n",
      "Epoch [5/10], Step [780000/800000], Loss: 0.5896\n",
      "Epoch [5/10], Step [785000/800000], Loss: 1.0296\n",
      "Epoch [5/10], Step [790000/800000], Loss: 1.9858\n",
      "Epoch [5/10], Step [795000/800000], Loss: 1.3726\n",
      "Epoch [5/10], Step [800000/800000], Loss: 0.5878\n",
      "Epoch [6/10], Step [5000/800000], Loss: 0.6429\n",
      "Epoch [6/10], Step [10000/800000], Loss: 0.7519\n",
      "Epoch [6/10], Step [15000/800000], Loss: 0.7996\n",
      "Epoch [6/10], Step [20000/800000], Loss: 0.5596\n",
      "Epoch [6/10], Step [25000/800000], Loss: 1.4156\n",
      "Epoch [6/10], Step [30000/800000], Loss: 0.3065\n",
      "Epoch [6/10], Step [35000/800000], Loss: 0.9880\n",
      "Epoch [6/10], Step [40000/800000], Loss: 3.1363\n",
      "Epoch [6/10], Step [45000/800000], Loss: 0.4187\n",
      "Epoch [6/10], Step [50000/800000], Loss: 0.1055\n",
      "Epoch [6/10], Step [55000/800000], Loss: 0.9097\n",
      "Epoch [6/10], Step [60000/800000], Loss: 0.0078\n",
      "Epoch [6/10], Step [65000/800000], Loss: 0.7144\n",
      "Epoch [6/10], Step [70000/800000], Loss: 1.0486\n",
      "Epoch [6/10], Step [75000/800000], Loss: 0.6250\n",
      "Epoch [6/10], Step [80000/800000], Loss: 0.5177\n",
      "Epoch [6/10], Step [85000/800000], Loss: 0.2881\n",
      "Epoch [6/10], Step [90000/800000], Loss: 1.2191\n",
      "Epoch [6/10], Step [95000/800000], Loss: 0.2863\n",
      "Epoch [6/10], Step [100000/800000], Loss: 0.3015\n",
      "Epoch [6/10], Step [105000/800000], Loss: 0.7985\n",
      "Epoch [6/10], Step [110000/800000], Loss: 0.2732\n",
      "Epoch [6/10], Step [115000/800000], Loss: 0.9794\n",
      "Epoch [6/10], Step [120000/800000], Loss: 0.1487\n",
      "Epoch [6/10], Step [125000/800000], Loss: 0.8973\n",
      "Epoch [6/10], Step [130000/800000], Loss: 0.9108\n",
      "Epoch [6/10], Step [135000/800000], Loss: 0.4800\n",
      "Epoch [6/10], Step [140000/800000], Loss: 0.1645\n",
      "Epoch [6/10], Step [145000/800000], Loss: 0.6230\n",
      "Epoch [6/10], Step [150000/800000], Loss: 0.5094\n",
      "Epoch [6/10], Step [155000/800000], Loss: 0.8517\n",
      "Epoch [6/10], Step [160000/800000], Loss: 0.9769\n",
      "Epoch [6/10], Step [165000/800000], Loss: 0.9719\n",
      "Epoch [6/10], Step [170000/800000], Loss: 0.1703\n",
      "Epoch [6/10], Step [175000/800000], Loss: 0.6954\n",
      "Epoch [6/10], Step [180000/800000], Loss: 2.2379\n",
      "Epoch [6/10], Step [185000/800000], Loss: 0.3062\n",
      "Epoch [6/10], Step [190000/800000], Loss: 0.6140\n",
      "Epoch [6/10], Step [195000/800000], Loss: 0.7701\n",
      "Epoch [6/10], Step [200000/800000], Loss: 0.3079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [205000/800000], Loss: 0.7897\n",
      "Epoch [6/10], Step [210000/800000], Loss: 0.5303\n",
      "Epoch [6/10], Step [215000/800000], Loss: 0.2092\n",
      "Epoch [6/10], Step [220000/800000], Loss: 0.0740\n",
      "Epoch [6/10], Step [225000/800000], Loss: 1.4213\n",
      "Epoch [6/10], Step [230000/800000], Loss: 2.7253\n",
      "Epoch [6/10], Step [235000/800000], Loss: 0.5462\n",
      "Epoch [6/10], Step [240000/800000], Loss: 0.7052\n",
      "Epoch [6/10], Step [245000/800000], Loss: 0.8615\n",
      "Epoch [6/10], Step [250000/800000], Loss: 1.3473\n",
      "Epoch [6/10], Step [255000/800000], Loss: 1.8966\n",
      "Epoch [6/10], Step [260000/800000], Loss: 0.2072\n",
      "Epoch [6/10], Step [265000/800000], Loss: 0.3995\n",
      "Epoch [6/10], Step [270000/800000], Loss: 0.4670\n",
      "Epoch [6/10], Step [275000/800000], Loss: 0.6554\n",
      "Epoch [6/10], Step [280000/800000], Loss: 0.7876\n",
      "Epoch [6/10], Step [285000/800000], Loss: 0.8267\n",
      "Epoch [6/10], Step [290000/800000], Loss: 0.5326\n",
      "Epoch [6/10], Step [295000/800000], Loss: 0.2622\n",
      "Epoch [6/10], Step [300000/800000], Loss: 0.6994\n",
      "Epoch [6/10], Step [305000/800000], Loss: 0.7316\n",
      "Epoch [6/10], Step [310000/800000], Loss: 0.5062\n",
      "Epoch [6/10], Step [315000/800000], Loss: 0.6671\n",
      "Epoch [6/10], Step [320000/800000], Loss: 0.8645\n",
      "Epoch [6/10], Step [325000/800000], Loss: 1.5973\n",
      "Epoch [6/10], Step [330000/800000], Loss: 0.8071\n",
      "Epoch [6/10], Step [335000/800000], Loss: 1.6448\n",
      "Epoch [6/10], Step [340000/800000], Loss: 0.6186\n",
      "Epoch [6/10], Step [345000/800000], Loss: 0.2682\n",
      "Epoch [6/10], Step [350000/800000], Loss: 0.5836\n",
      "Epoch [6/10], Step [355000/800000], Loss: 0.6410\n",
      "Epoch [6/10], Step [360000/800000], Loss: 0.5071\n",
      "Epoch [6/10], Step [365000/800000], Loss: 0.5527\n",
      "Epoch [6/10], Step [370000/800000], Loss: 0.5848\n",
      "Epoch [6/10], Step [375000/800000], Loss: 0.3518\n",
      "Epoch [6/10], Step [380000/800000], Loss: 1.6375\n",
      "Epoch [6/10], Step [385000/800000], Loss: 3.0215\n",
      "Epoch [6/10], Step [390000/800000], Loss: 0.9240\n",
      "Epoch [6/10], Step [395000/800000], Loss: 0.7676\n",
      "Epoch [6/10], Step [400000/800000], Loss: 0.1397\n",
      "Epoch [6/10], Step [405000/800000], Loss: 0.6940\n",
      "Epoch [6/10], Step [410000/800000], Loss: 0.8215\n",
      "Epoch [6/10], Step [415000/800000], Loss: 1.2466\n",
      "Epoch [6/10], Step [420000/800000], Loss: 0.6087\n",
      "Epoch [6/10], Step [425000/800000], Loss: 0.7169\n",
      "Epoch [6/10], Step [430000/800000], Loss: 0.3326\n",
      "Epoch [6/10], Step [435000/800000], Loss: 0.2602\n",
      "Epoch [6/10], Step [440000/800000], Loss: 0.7609\n",
      "Epoch [6/10], Step [445000/800000], Loss: 0.8611\n",
      "Epoch [6/10], Step [450000/800000], Loss: 0.6604\n",
      "Epoch [6/10], Step [455000/800000], Loss: 1.1933\n",
      "Epoch [6/10], Step [460000/800000], Loss: 0.4849\n",
      "Epoch [6/10], Step [465000/800000], Loss: 0.9949\n",
      "Epoch [6/10], Step [470000/800000], Loss: 0.4957\n",
      "Epoch [6/10], Step [475000/800000], Loss: 0.6325\n",
      "Epoch [6/10], Step [480000/800000], Loss: 1.0011\n",
      "Epoch [6/10], Step [485000/800000], Loss: 0.4676\n",
      "Epoch [6/10], Step [490000/800000], Loss: 0.6585\n",
      "Epoch [6/10], Step [495000/800000], Loss: 0.2128\n",
      "Epoch [6/10], Step [500000/800000], Loss: 0.3018\n",
      "Epoch [6/10], Step [505000/800000], Loss: 1.1246\n",
      "Epoch [6/10], Step [510000/800000], Loss: 1.3230\n",
      "Epoch [6/10], Step [515000/800000], Loss: 1.4520\n",
      "Epoch [6/10], Step [520000/800000], Loss: 0.7236\n",
      "Epoch [6/10], Step [525000/800000], Loss: 0.5592\n",
      "Epoch [6/10], Step [530000/800000], Loss: 0.7619\n",
      "Epoch [6/10], Step [535000/800000], Loss: 0.8033\n",
      "Epoch [6/10], Step [540000/800000], Loss: 0.1761\n",
      "Epoch [6/10], Step [545000/800000], Loss: 0.7869\n",
      "Epoch [6/10], Step [550000/800000], Loss: 0.2043\n",
      "Epoch [6/10], Step [555000/800000], Loss: 0.4638\n",
      "Epoch [6/10], Step [560000/800000], Loss: 1.3192\n",
      "Epoch [6/10], Step [565000/800000], Loss: 0.5764\n",
      "Epoch [6/10], Step [570000/800000], Loss: 0.3618\n",
      "Epoch [6/10], Step [575000/800000], Loss: 1.1228\n",
      "Epoch [6/10], Step [580000/800000], Loss: 0.9791\n",
      "Epoch [6/10], Step [585000/800000], Loss: 0.5025\n",
      "Epoch [6/10], Step [590000/800000], Loss: 0.5226\n",
      "Epoch [6/10], Step [595000/800000], Loss: 0.8874\n",
      "Epoch [6/10], Step [600000/800000], Loss: 1.6610\n",
      "Epoch [6/10], Step [605000/800000], Loss: 0.5412\n",
      "Epoch [6/10], Step [610000/800000], Loss: 0.8453\n",
      "Epoch [6/10], Step [615000/800000], Loss: 0.6335\n",
      "Epoch [6/10], Step [620000/800000], Loss: 0.7551\n",
      "Epoch [6/10], Step [625000/800000], Loss: 0.4962\n",
      "Epoch [6/10], Step [630000/800000], Loss: 0.5042\n",
      "Epoch [6/10], Step [635000/800000], Loss: 0.7958\n",
      "Epoch [6/10], Step [640000/800000], Loss: 0.8105\n",
      "Epoch [6/10], Step [645000/800000], Loss: 0.4518\n",
      "Epoch [6/10], Step [650000/800000], Loss: 0.6710\n",
      "Epoch [6/10], Step [655000/800000], Loss: 0.5186\n",
      "Epoch [6/10], Step [660000/800000], Loss: 0.6972\n",
      "Epoch [6/10], Step [665000/800000], Loss: 0.5233\n",
      "Epoch [6/10], Step [670000/800000], Loss: 0.8662\n",
      "Epoch [6/10], Step [675000/800000], Loss: 0.4746\n",
      "Epoch [6/10], Step [680000/800000], Loss: 0.7512\n",
      "Epoch [6/10], Step [685000/800000], Loss: 0.5277\n",
      "Epoch [6/10], Step [690000/800000], Loss: 0.6191\n",
      "Epoch [6/10], Step [695000/800000], Loss: 0.5180\n",
      "Epoch [6/10], Step [700000/800000], Loss: 0.8383\n",
      "Epoch [6/10], Step [705000/800000], Loss: 1.9302\n",
      "Epoch [6/10], Step [710000/800000], Loss: 0.8447\n",
      "Epoch [6/10], Step [715000/800000], Loss: 0.5155\n",
      "Epoch [6/10], Step [720000/800000], Loss: 0.5650\n",
      "Epoch [6/10], Step [725000/800000], Loss: 0.7331\n",
      "Epoch [6/10], Step [730000/800000], Loss: 0.7354\n",
      "Epoch [6/10], Step [735000/800000], Loss: 2.8878\n",
      "Epoch [6/10], Step [740000/800000], Loss: 1.0709\n",
      "Epoch [6/10], Step [745000/800000], Loss: 0.8453\n",
      "Epoch [6/10], Step [750000/800000], Loss: 0.8132\n",
      "Epoch [6/10], Step [755000/800000], Loss: 0.6234\n",
      "Epoch [6/10], Step [760000/800000], Loss: 0.4282\n",
      "Epoch [6/10], Step [765000/800000], Loss: 0.7723\n",
      "Epoch [6/10], Step [770000/800000], Loss: 0.8062\n",
      "Epoch [6/10], Step [775000/800000], Loss: 0.6846\n",
      "Epoch [6/10], Step [780000/800000], Loss: 0.0675\n",
      "Epoch [6/10], Step [785000/800000], Loss: 1.1057\n",
      "Epoch [6/10], Step [790000/800000], Loss: 0.2251\n",
      "Epoch [6/10], Step [795000/800000], Loss: 0.4595\n",
      "Epoch [6/10], Step [800000/800000], Loss: 0.6043\n",
      "Epoch [7/10], Step [5000/800000], Loss: 0.5883\n",
      "Epoch [7/10], Step [10000/800000], Loss: 0.4628\n",
      "Epoch [7/10], Step [15000/800000], Loss: 0.7161\n",
      "Epoch [7/10], Step [20000/800000], Loss: 0.6767\n",
      "Epoch [7/10], Step [25000/800000], Loss: 0.6372\n",
      "Epoch [7/10], Step [30000/800000], Loss: 0.6408\n",
      "Epoch [7/10], Step [35000/800000], Loss: 1.3723\n",
      "Epoch [7/10], Step [40000/800000], Loss: 0.8465\n",
      "Epoch [7/10], Step [45000/800000], Loss: 0.4762\n",
      "Epoch [7/10], Step [50000/800000], Loss: 0.0803\n",
      "Epoch [7/10], Step [55000/800000], Loss: 0.7749\n",
      "Epoch [7/10], Step [60000/800000], Loss: 0.5863\n",
      "Epoch [7/10], Step [65000/800000], Loss: 0.5422\n",
      "Epoch [7/10], Step [70000/800000], Loss: 0.5200\n",
      "Epoch [7/10], Step [75000/800000], Loss: 0.6555\n",
      "Epoch [7/10], Step [80000/800000], Loss: 0.8293\n",
      "Epoch [7/10], Step [85000/800000], Loss: 0.5051\n",
      "Epoch [7/10], Step [90000/800000], Loss: 1.0042\n",
      "Epoch [7/10], Step [95000/800000], Loss: 1.0128\n",
      "Epoch [7/10], Step [100000/800000], Loss: 0.4280\n",
      "Epoch [7/10], Step [105000/800000], Loss: 0.6662\n",
      "Epoch [7/10], Step [110000/800000], Loss: 0.3988\n",
      "Epoch [7/10], Step [115000/800000], Loss: 0.7458\n",
      "Epoch [7/10], Step [120000/800000], Loss: 1.4636\n",
      "Epoch [7/10], Step [125000/800000], Loss: 0.8419\n",
      "Epoch [7/10], Step [130000/800000], Loss: 0.6843\n",
      "Epoch [7/10], Step [135000/800000], Loss: 0.6420\n",
      "Epoch [7/10], Step [140000/800000], Loss: 0.7144\n",
      "Epoch [7/10], Step [145000/800000], Loss: 0.7007\n",
      "Epoch [7/10], Step [150000/800000], Loss: 0.9814\n",
      "Epoch [7/10], Step [155000/800000], Loss: 0.6332\n",
      "Epoch [7/10], Step [160000/800000], Loss: 2.1166\n",
      "Epoch [7/10], Step [165000/800000], Loss: 1.0006\n",
      "Epoch [7/10], Step [170000/800000], Loss: 0.3421\n",
      "Epoch [7/10], Step [175000/800000], Loss: 0.0010\n",
      "Epoch [7/10], Step [180000/800000], Loss: 1.6998\n",
      "Epoch [7/10], Step [185000/800000], Loss: 0.5504\n",
      "Epoch [7/10], Step [190000/800000], Loss: 0.8945\n",
      "Epoch [7/10], Step [195000/800000], Loss: 0.6388\n",
      "Epoch [7/10], Step [200000/800000], Loss: 0.3095\n",
      "Epoch [7/10], Step [205000/800000], Loss: 0.4761\n",
      "Epoch [7/10], Step [210000/800000], Loss: 1.5705\n",
      "Epoch [7/10], Step [215000/800000], Loss: 0.1593\n",
      "Epoch [7/10], Step [220000/800000], Loss: 0.2980\n",
      "Epoch [7/10], Step [225000/800000], Loss: 1.0992\n",
      "Epoch [7/10], Step [230000/800000], Loss: 2.0628\n",
      "Epoch [7/10], Step [235000/800000], Loss: 0.7380\n",
      "Epoch [7/10], Step [240000/800000], Loss: 0.6006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [245000/800000], Loss: 0.9024\n",
      "Epoch [7/10], Step [250000/800000], Loss: 0.8084\n",
      "Epoch [7/10], Step [255000/800000], Loss: 1.8044\n",
      "Epoch [7/10], Step [260000/800000], Loss: 0.5607\n",
      "Epoch [7/10], Step [265000/800000], Loss: 0.4089\n",
      "Epoch [7/10], Step [270000/800000], Loss: 0.8777\n",
      "Epoch [7/10], Step [275000/800000], Loss: 0.8606\n",
      "Epoch [7/10], Step [280000/800000], Loss: 0.5404\n",
      "Epoch [7/10], Step [285000/800000], Loss: 0.7792\n",
      "Epoch [7/10], Step [290000/800000], Loss: 0.6660\n",
      "Epoch [7/10], Step [295000/800000], Loss: 0.5221\n",
      "Epoch [7/10], Step [300000/800000], Loss: 1.0045\n",
      "Epoch [7/10], Step [305000/800000], Loss: 0.6359\n",
      "Epoch [7/10], Step [310000/800000], Loss: 0.3895\n",
      "Epoch [7/10], Step [315000/800000], Loss: 0.7984\n",
      "Epoch [7/10], Step [320000/800000], Loss: 0.6166\n",
      "Epoch [7/10], Step [325000/800000], Loss: 1.5890\n",
      "Epoch [7/10], Step [330000/800000], Loss: 0.8052\n",
      "Epoch [7/10], Step [335000/800000], Loss: 1.2217\n",
      "Epoch [7/10], Step [340000/800000], Loss: 0.6069\n",
      "Epoch [7/10], Step [345000/800000], Loss: 0.3058\n",
      "Epoch [7/10], Step [350000/800000], Loss: 1.2195\n",
      "Epoch [7/10], Step [355000/800000], Loss: 0.5282\n",
      "Epoch [7/10], Step [360000/800000], Loss: 0.1926\n",
      "Epoch [7/10], Step [365000/800000], Loss: 0.2566\n",
      "Epoch [7/10], Step [370000/800000], Loss: 0.5614\n",
      "Epoch [7/10], Step [375000/800000], Loss: 0.1487\n",
      "Epoch [7/10], Step [380000/800000], Loss: 1.2444\n",
      "Epoch [7/10], Step [385000/800000], Loss: 0.3597\n",
      "Epoch [7/10], Step [390000/800000], Loss: 0.1989\n",
      "Epoch [7/10], Step [395000/800000], Loss: 0.5704\n",
      "Epoch [7/10], Step [400000/800000], Loss: 0.4158\n",
      "Epoch [7/10], Step [405000/800000], Loss: 0.4795\n",
      "Epoch [7/10], Step [410000/800000], Loss: 0.5351\n",
      "Epoch [7/10], Step [415000/800000], Loss: 1.1977\n",
      "Epoch [7/10], Step [420000/800000], Loss: 0.4019\n",
      "Epoch [7/10], Step [425000/800000], Loss: 0.6802\n",
      "Epoch [7/10], Step [430000/800000], Loss: 1.3021\n",
      "Epoch [7/10], Step [435000/800000], Loss: 0.3510\n",
      "Epoch [7/10], Step [440000/800000], Loss: 0.1648\n",
      "Epoch [7/10], Step [445000/800000], Loss: 1.0910\n",
      "Epoch [7/10], Step [450000/800000], Loss: 0.5815\n",
      "Epoch [7/10], Step [455000/800000], Loss: 1.4142\n",
      "Epoch [7/10], Step [460000/800000], Loss: 0.7785\n",
      "Epoch [7/10], Step [465000/800000], Loss: 0.6089\n",
      "Epoch [7/10], Step [470000/800000], Loss: 0.5380\n",
      "Epoch [7/10], Step [475000/800000], Loss: 0.9462\n",
      "Epoch [7/10], Step [480000/800000], Loss: 0.4462\n",
      "Epoch [7/10], Step [485000/800000], Loss: 0.6234\n",
      "Epoch [7/10], Step [490000/800000], Loss: 0.6033\n",
      "Epoch [7/10], Step [495000/800000], Loss: 0.3512\n",
      "Epoch [7/10], Step [500000/800000], Loss: 0.7869\n",
      "Epoch [7/10], Step [505000/800000], Loss: 2.6043\n",
      "Epoch [7/10], Step [510000/800000], Loss: 1.8693\n",
      "Epoch [7/10], Step [515000/800000], Loss: 1.0904\n",
      "Epoch [7/10], Step [520000/800000], Loss: 1.3620\n",
      "Epoch [7/10], Step [525000/800000], Loss: 0.4585\n",
      "Epoch [7/10], Step [530000/800000], Loss: 1.2262\n",
      "Epoch [7/10], Step [535000/800000], Loss: 0.6995\n",
      "Epoch [7/10], Step [540000/800000], Loss: 0.4244\n",
      "Epoch [7/10], Step [545000/800000], Loss: 0.6282\n",
      "Epoch [7/10], Step [550000/800000], Loss: 0.6649\n",
      "Epoch [7/10], Step [555000/800000], Loss: 0.6832\n",
      "Epoch [7/10], Step [560000/800000], Loss: 0.9455\n",
      "Epoch [7/10], Step [565000/800000], Loss: 0.5987\n",
      "Epoch [7/10], Step [570000/800000], Loss: 0.2525\n",
      "Epoch [7/10], Step [575000/800000], Loss: 0.8128\n",
      "Epoch [7/10], Step [580000/800000], Loss: 1.1896\n",
      "Epoch [7/10], Step [585000/800000], Loss: 0.7261\n",
      "Epoch [7/10], Step [590000/800000], Loss: 0.4469\n",
      "Epoch [7/10], Step [595000/800000], Loss: 0.9408\n",
      "Epoch [7/10], Step [600000/800000], Loss: 0.6829\n",
      "Epoch [7/10], Step [605000/800000], Loss: 0.5458\n",
      "Epoch [7/10], Step [610000/800000], Loss: 1.6932\n",
      "Epoch [7/10], Step [615000/800000], Loss: 0.5440\n",
      "Epoch [7/10], Step [620000/800000], Loss: 0.6965\n",
      "Epoch [7/10], Step [625000/800000], Loss: 0.6276\n",
      "Epoch [7/10], Step [630000/800000], Loss: 0.2754\n",
      "Epoch [7/10], Step [635000/800000], Loss: 0.9702\n",
      "Epoch [7/10], Step [640000/800000], Loss: 0.9857\n",
      "Epoch [7/10], Step [645000/800000], Loss: 0.5431\n",
      "Epoch [7/10], Step [650000/800000], Loss: 0.4165\n",
      "Epoch [7/10], Step [655000/800000], Loss: 0.2888\n",
      "Epoch [7/10], Step [660000/800000], Loss: 0.0676\n",
      "Epoch [7/10], Step [665000/800000], Loss: 0.6986\n",
      "Epoch [7/10], Step [670000/800000], Loss: 0.6302\n",
      "Epoch [7/10], Step [675000/800000], Loss: 0.4639\n",
      "Epoch [7/10], Step [680000/800000], Loss: 0.2697\n",
      "Epoch [7/10], Step [685000/800000], Loss: 0.8260\n",
      "Epoch [7/10], Step [690000/800000], Loss: 0.7024\n",
      "Epoch [7/10], Step [695000/800000], Loss: 1.5740\n",
      "Epoch [7/10], Step [700000/800000], Loss: 1.1213\n",
      "Epoch [7/10], Step [705000/800000], Loss: 0.6135\n",
      "Epoch [7/10], Step [710000/800000], Loss: 0.7361\n",
      "Epoch [7/10], Step [715000/800000], Loss: 1.0507\n",
      "Epoch [7/10], Step [720000/800000], Loss: 0.8995\n",
      "Epoch [7/10], Step [725000/800000], Loss: 1.2065\n",
      "Epoch [7/10], Step [730000/800000], Loss: 1.1859\n",
      "Epoch [7/10], Step [735000/800000], Loss: 1.0000\n",
      "Epoch [7/10], Step [740000/800000], Loss: 0.6916\n",
      "Epoch [7/10], Step [745000/800000], Loss: 0.4859\n",
      "Epoch [7/10], Step [750000/800000], Loss: 1.6735\n",
      "Epoch [7/10], Step [755000/800000], Loss: 0.7566\n",
      "Epoch [7/10], Step [760000/800000], Loss: 0.4950\n",
      "Epoch [7/10], Step [765000/800000], Loss: 0.4697\n",
      "Epoch [7/10], Step [770000/800000], Loss: 0.7944\n",
      "Epoch [7/10], Step [775000/800000], Loss: 1.1995\n",
      "Epoch [7/10], Step [780000/800000], Loss: 0.6012\n",
      "Epoch [7/10], Step [785000/800000], Loss: 1.1945\n",
      "Epoch [7/10], Step [790000/800000], Loss: 0.4552\n",
      "Epoch [7/10], Step [795000/800000], Loss: 0.6883\n",
      "Epoch [7/10], Step [800000/800000], Loss: 0.5392\n",
      "Epoch [8/10], Step [5000/800000], Loss: 0.4268\n",
      "Epoch [8/10], Step [10000/800000], Loss: 1.1773\n",
      "Epoch [8/10], Step [15000/800000], Loss: 0.9190\n",
      "Epoch [8/10], Step [20000/800000], Loss: 0.7613\n",
      "Epoch [8/10], Step [25000/800000], Loss: 0.7094\n",
      "Epoch [8/10], Step [30000/800000], Loss: 0.6301\n",
      "Epoch [8/10], Step [35000/800000], Loss: 1.0814\n",
      "Epoch [8/10], Step [40000/800000], Loss: 1.1762\n",
      "Epoch [8/10], Step [45000/800000], Loss: 0.9108\n",
      "Epoch [8/10], Step [50000/800000], Loss: 0.1640\n",
      "Epoch [8/10], Step [55000/800000], Loss: 0.6739\n",
      "Epoch [8/10], Step [60000/800000], Loss: 0.6126\n",
      "Epoch [8/10], Step [65000/800000], Loss: 0.4237\n",
      "Epoch [8/10], Step [70000/800000], Loss: 0.8712\n",
      "Epoch [8/10], Step [75000/800000], Loss: 0.4271\n",
      "Epoch [8/10], Step [80000/800000], Loss: 0.5981\n",
      "Epoch [8/10], Step [85000/800000], Loss: 0.5936\n",
      "Epoch [8/10], Step [90000/800000], Loss: 0.8326\n",
      "Epoch [8/10], Step [95000/800000], Loss: 0.5713\n",
      "Epoch [8/10], Step [100000/800000], Loss: 0.6562\n",
      "Epoch [8/10], Step [105000/800000], Loss: 0.7697\n",
      "Epoch [8/10], Step [110000/800000], Loss: 0.6385\n",
      "Epoch [8/10], Step [115000/800000], Loss: 0.5709\n",
      "Epoch [8/10], Step [120000/800000], Loss: 0.6862\n",
      "Epoch [8/10], Step [125000/800000], Loss: 0.5877\n",
      "Epoch [8/10], Step [130000/800000], Loss: 0.7433\n",
      "Epoch [8/10], Step [135000/800000], Loss: 0.6303\n",
      "Epoch [8/10], Step [140000/800000], Loss: 0.7430\n",
      "Epoch [8/10], Step [145000/800000], Loss: 0.4025\n",
      "Epoch [8/10], Step [150000/800000], Loss: 1.1960\n",
      "Epoch [8/10], Step [155000/800000], Loss: 0.8586\n",
      "Epoch [8/10], Step [160000/800000], Loss: 0.8794\n",
      "Epoch [8/10], Step [165000/800000], Loss: 0.9150\n",
      "Epoch [8/10], Step [170000/800000], Loss: 0.1555\n",
      "Epoch [8/10], Step [175000/800000], Loss: 0.6815\n",
      "Epoch [8/10], Step [180000/800000], Loss: 1.6236\n",
      "Epoch [8/10], Step [185000/800000], Loss: 0.6374\n",
      "Epoch [8/10], Step [190000/800000], Loss: 0.6586\n",
      "Epoch [8/10], Step [195000/800000], Loss: 0.6878\n",
      "Epoch [8/10], Step [200000/800000], Loss: 0.7924\n",
      "Epoch [8/10], Step [205000/800000], Loss: 0.3931\n",
      "Epoch [8/10], Step [210000/800000], Loss: 0.6473\n",
      "Epoch [8/10], Step [215000/800000], Loss: 0.4683\n",
      "Epoch [8/10], Step [220000/800000], Loss: 0.4307\n",
      "Epoch [8/10], Step [225000/800000], Loss: 0.6344\n",
      "Epoch [8/10], Step [230000/800000], Loss: 1.4103\n",
      "Epoch [8/10], Step [235000/800000], Loss: 0.1972\n",
      "Epoch [8/10], Step [240000/800000], Loss: 1.8160\n",
      "Epoch [8/10], Step [245000/800000], Loss: 1.5262\n",
      "Epoch [8/10], Step [250000/800000], Loss: 0.7754\n",
      "Epoch [8/10], Step [255000/800000], Loss: 1.0643\n",
      "Epoch [8/10], Step [260000/800000], Loss: 0.6810\n",
      "Epoch [8/10], Step [265000/800000], Loss: 0.7113\n",
      "Epoch [8/10], Step [270000/800000], Loss: 0.6486\n",
      "Epoch [8/10], Step [275000/800000], Loss: 0.8092\n",
      "Epoch [8/10], Step [280000/800000], Loss: 0.5660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [285000/800000], Loss: 0.7108\n",
      "Epoch [8/10], Step [290000/800000], Loss: 0.2625\n",
      "Epoch [8/10], Step [295000/800000], Loss: 0.2352\n",
      "Epoch [8/10], Step [300000/800000], Loss: 0.7551\n",
      "Epoch [8/10], Step [305000/800000], Loss: 1.1831\n",
      "Epoch [8/10], Step [310000/800000], Loss: 0.2664\n",
      "Epoch [8/10], Step [315000/800000], Loss: 0.5826\n",
      "Epoch [8/10], Step [320000/800000], Loss: 0.5966\n",
      "Epoch [8/10], Step [325000/800000], Loss: 0.4195\n",
      "Epoch [8/10], Step [330000/800000], Loss: 1.0917\n",
      "Epoch [8/10], Step [335000/800000], Loss: 0.9955\n",
      "Epoch [8/10], Step [340000/800000], Loss: 0.3219\n",
      "Epoch [8/10], Step [345000/800000], Loss: 0.5765\n",
      "Epoch [8/10], Step [350000/800000], Loss: 0.6131\n",
      "Epoch [8/10], Step [355000/800000], Loss: 0.4231\n",
      "Epoch [8/10], Step [360000/800000], Loss: 0.2741\n",
      "Epoch [8/10], Step [365000/800000], Loss: 0.4995\n",
      "Epoch [8/10], Step [370000/800000], Loss: 0.5175\n",
      "Epoch [8/10], Step [375000/800000], Loss: 0.4212\n",
      "Epoch [8/10], Step [380000/800000], Loss: 0.7276\n",
      "Epoch [8/10], Step [385000/800000], Loss: 0.7785\n",
      "Epoch [8/10], Step [390000/800000], Loss: 0.7936\n",
      "Epoch [8/10], Step [395000/800000], Loss: 0.6691\n",
      "Epoch [8/10], Step [400000/800000], Loss: 0.1622\n",
      "Epoch [8/10], Step [405000/800000], Loss: 0.7632\n",
      "Epoch [8/10], Step [410000/800000], Loss: 1.1007\n",
      "Epoch [8/10], Step [415000/800000], Loss: 0.8126\n",
      "Epoch [8/10], Step [420000/800000], Loss: 0.4086\n",
      "Epoch [8/10], Step [425000/800000], Loss: 0.3277\n",
      "Epoch [8/10], Step [430000/800000], Loss: 2.1421\n",
      "Epoch [8/10], Step [435000/800000], Loss: 0.0991\n",
      "Epoch [8/10], Step [440000/800000], Loss: 0.4590\n",
      "Epoch [8/10], Step [445000/800000], Loss: 0.6963\n",
      "Epoch [8/10], Step [450000/800000], Loss: 0.2762\n",
      "Epoch [8/10], Step [455000/800000], Loss: 0.8063\n",
      "Epoch [8/10], Step [460000/800000], Loss: 0.5052\n",
      "Epoch [8/10], Step [465000/800000], Loss: 1.3645\n",
      "Epoch [8/10], Step [470000/800000], Loss: 0.5377\n",
      "Epoch [8/10], Step [475000/800000], Loss: 0.4187\n",
      "Epoch [8/10], Step [480000/800000], Loss: 1.2091\n",
      "Epoch [8/10], Step [485000/800000], Loss: 0.6992\n",
      "Epoch [8/10], Step [490000/800000], Loss: 0.5790\n",
      "Epoch [8/10], Step [495000/800000], Loss: 0.4273\n",
      "Epoch [8/10], Step [500000/800000], Loss: 0.3209\n",
      "Epoch [8/10], Step [505000/800000], Loss: 1.9352\n",
      "Epoch [8/10], Step [510000/800000], Loss: 1.4380\n",
      "Epoch [8/10], Step [515000/800000], Loss: 0.7678\n",
      "Epoch [8/10], Step [520000/800000], Loss: 0.2974\n",
      "Epoch [8/10], Step [525000/800000], Loss: 0.6841\n",
      "Epoch [8/10], Step [530000/800000], Loss: 0.7233\n",
      "Epoch [8/10], Step [535000/800000], Loss: 0.6030\n",
      "Epoch [8/10], Step [540000/800000], Loss: 0.5021\n",
      "Epoch [8/10], Step [545000/800000], Loss: 0.5728\n",
      "Epoch [8/10], Step [550000/800000], Loss: 0.3664\n",
      "Epoch [8/10], Step [555000/800000], Loss: 0.2343\n",
      "Epoch [8/10], Step [560000/800000], Loss: 0.8859\n",
      "Epoch [8/10], Step [565000/800000], Loss: 0.6762\n",
      "Epoch [8/10], Step [570000/800000], Loss: 0.2612\n",
      "Epoch [8/10], Step [575000/800000], Loss: 0.8402\n",
      "Epoch [8/10], Step [580000/800000], Loss: 1.2411\n",
      "Epoch [8/10], Step [585000/800000], Loss: 0.3113\n",
      "Epoch [8/10], Step [590000/800000], Loss: 0.6965\n",
      "Epoch [8/10], Step [595000/800000], Loss: 0.6751\n",
      "Epoch [8/10], Step [600000/800000], Loss: 0.9375\n",
      "Epoch [8/10], Step [605000/800000], Loss: 0.5077\n",
      "Epoch [8/10], Step [610000/800000], Loss: 0.7020\n",
      "Epoch [8/10], Step [615000/800000], Loss: 0.6552\n",
      "Epoch [8/10], Step [620000/800000], Loss: 0.3499\n",
      "Epoch [8/10], Step [625000/800000], Loss: 0.9641\n",
      "Epoch [8/10], Step [630000/800000], Loss: 0.8577\n",
      "Epoch [8/10], Step [635000/800000], Loss: 1.5365\n",
      "Epoch [8/10], Step [640000/800000], Loss: 0.6154\n",
      "Epoch [8/10], Step [645000/800000], Loss: 0.5188\n",
      "Epoch [8/10], Step [650000/800000], Loss: 0.6001\n",
      "Epoch [8/10], Step [655000/800000], Loss: 0.3851\n",
      "Epoch [8/10], Step [660000/800000], Loss: 0.9554\n",
      "Epoch [8/10], Step [665000/800000], Loss: 0.4472\n",
      "Epoch [8/10], Step [670000/800000], Loss: 0.6492\n",
      "Epoch [8/10], Step [675000/800000], Loss: 0.6440\n",
      "Epoch [8/10], Step [680000/800000], Loss: 0.7806\n",
      "Epoch [8/10], Step [685000/800000], Loss: 1.1113\n",
      "Epoch [8/10], Step [690000/800000], Loss: 0.6433\n",
      "Epoch [8/10], Step [695000/800000], Loss: 0.6401\n",
      "Epoch [8/10], Step [700000/800000], Loss: 0.9662\n",
      "Epoch [8/10], Step [705000/800000], Loss: 0.7603\n",
      "Epoch [8/10], Step [710000/800000], Loss: 0.6641\n",
      "Epoch [8/10], Step [715000/800000], Loss: 0.6216\n",
      "Epoch [8/10], Step [720000/800000], Loss: 0.7145\n",
      "Epoch [8/10], Step [725000/800000], Loss: 1.0241\n",
      "Epoch [8/10], Step [730000/800000], Loss: 0.9079\n",
      "Epoch [8/10], Step [735000/800000], Loss: 0.7471\n",
      "Epoch [8/10], Step [740000/800000], Loss: 0.6194\n",
      "Epoch [8/10], Step [745000/800000], Loss: 0.6505\n",
      "Epoch [8/10], Step [750000/800000], Loss: 0.7229\n",
      "Epoch [8/10], Step [755000/800000], Loss: 0.7405\n",
      "Epoch [8/10], Step [760000/800000], Loss: 0.6654\n",
      "Epoch [8/10], Step [765000/800000], Loss: 0.4570\n",
      "Epoch [8/10], Step [770000/800000], Loss: 0.7105\n",
      "Epoch [8/10], Step [775000/800000], Loss: 1.0451\n",
      "Epoch [8/10], Step [780000/800000], Loss: 0.4139\n",
      "Epoch [8/10], Step [785000/800000], Loss: 0.7487\n",
      "Epoch [8/10], Step [790000/800000], Loss: 0.3514\n",
      "Epoch [8/10], Step [795000/800000], Loss: 0.5092\n",
      "Epoch [8/10], Step [800000/800000], Loss: 0.8421\n",
      "Epoch [9/10], Step [5000/800000], Loss: 0.7260\n",
      "Epoch [9/10], Step [10000/800000], Loss: 0.6383\n",
      "Epoch [9/10], Step [15000/800000], Loss: 0.9094\n",
      "Epoch [9/10], Step [20000/800000], Loss: 0.7256\n",
      "Epoch [9/10], Step [25000/800000], Loss: 0.7402\n",
      "Epoch [9/10], Step [30000/800000], Loss: 0.3363\n",
      "Epoch [9/10], Step [35000/800000], Loss: 1.0098\n",
      "Epoch [9/10], Step [40000/800000], Loss: 1.1209\n",
      "Epoch [9/10], Step [45000/800000], Loss: 0.2310\n",
      "Epoch [9/10], Step [50000/800000], Loss: 0.0741\n",
      "Epoch [9/10], Step [55000/800000], Loss: 0.7514\n",
      "Epoch [9/10], Step [60000/800000], Loss: 0.5805\n",
      "Epoch [9/10], Step [65000/800000], Loss: 2.0060\n",
      "Epoch [9/10], Step [70000/800000], Loss: 0.7405\n",
      "Epoch [9/10], Step [75000/800000], Loss: 0.3650\n",
      "Epoch [9/10], Step [80000/800000], Loss: 2.2821\n",
      "Epoch [9/10], Step [85000/800000], Loss: 0.3681\n",
      "Epoch [9/10], Step [90000/800000], Loss: 1.4729\n",
      "Epoch [9/10], Step [95000/800000], Loss: 0.5890\n",
      "Epoch [9/10], Step [100000/800000], Loss: 0.4024\n",
      "Epoch [9/10], Step [105000/800000], Loss: 1.4950\n",
      "Epoch [9/10], Step [110000/800000], Loss: 0.8749\n",
      "Epoch [9/10], Step [115000/800000], Loss: 0.9214\n",
      "Epoch [9/10], Step [120000/800000], Loss: 0.2484\n",
      "Epoch [9/10], Step [125000/800000], Loss: 0.8922\n",
      "Epoch [9/10], Step [130000/800000], Loss: 0.7337\n",
      "Epoch [9/10], Step [135000/800000], Loss: 0.6928\n",
      "Epoch [9/10], Step [140000/800000], Loss: 0.3339\n",
      "Epoch [9/10], Step [145000/800000], Loss: 0.2416\n",
      "Epoch [9/10], Step [150000/800000], Loss: 0.4910\n",
      "Epoch [9/10], Step [155000/800000], Loss: 0.2040\n",
      "Epoch [9/10], Step [160000/800000], Loss: 0.7801\n",
      "Epoch [9/10], Step [165000/800000], Loss: 1.0087\n",
      "Epoch [9/10], Step [170000/800000], Loss: 0.3880\n",
      "Epoch [9/10], Step [175000/800000], Loss: 0.4141\n",
      "Epoch [9/10], Step [180000/800000], Loss: 1.9297\n",
      "Epoch [9/10], Step [185000/800000], Loss: 0.7036\n",
      "Epoch [9/10], Step [190000/800000], Loss: 1.0168\n",
      "Epoch [9/10], Step [195000/800000], Loss: 0.6143\n",
      "Epoch [9/10], Step [200000/800000], Loss: 0.7005\n",
      "Epoch [9/10], Step [205000/800000], Loss: 0.4793\n",
      "Epoch [9/10], Step [210000/800000], Loss: 1.0615\n",
      "Epoch [9/10], Step [215000/800000], Loss: 0.1161\n",
      "Epoch [9/10], Step [220000/800000], Loss: 0.3382\n",
      "Epoch [9/10], Step [225000/800000], Loss: 0.9108\n",
      "Epoch [9/10], Step [230000/800000], Loss: 1.0642\n",
      "Epoch [9/10], Step [235000/800000], Loss: 0.2565\n",
      "Epoch [9/10], Step [240000/800000], Loss: 0.6471\n",
      "Epoch [9/10], Step [245000/800000], Loss: 0.9948\n",
      "Epoch [9/10], Step [250000/800000], Loss: 1.0093\n",
      "Epoch [9/10], Step [255000/800000], Loss: 0.9123\n",
      "Epoch [9/10], Step [260000/800000], Loss: 0.2567\n",
      "Epoch [9/10], Step [265000/800000], Loss: 0.6498\n",
      "Epoch [9/10], Step [270000/800000], Loss: 0.2881\n",
      "Epoch [9/10], Step [275000/800000], Loss: 0.0900\n",
      "Epoch [9/10], Step [280000/800000], Loss: 0.6380\n",
      "Epoch [9/10], Step [285000/800000], Loss: 0.6722\n",
      "Epoch [9/10], Step [290000/800000], Loss: 0.5943\n",
      "Epoch [9/10], Step [295000/800000], Loss: 0.5758\n",
      "Epoch [9/10], Step [300000/800000], Loss: 0.1956\n",
      "Epoch [9/10], Step [305000/800000], Loss: 0.7120\n",
      "Epoch [9/10], Step [310000/800000], Loss: 0.9858\n",
      "Epoch [9/10], Step [315000/800000], Loss: 0.3086\n",
      "Epoch [9/10], Step [320000/800000], Loss: 0.3293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [325000/800000], Loss: 1.9356\n",
      "Epoch [9/10], Step [330000/800000], Loss: 0.2724\n",
      "Epoch [9/10], Step [335000/800000], Loss: 0.8248\n",
      "Epoch [9/10], Step [340000/800000], Loss: 0.3136\n",
      "Epoch [9/10], Step [345000/800000], Loss: 0.5346\n",
      "Epoch [9/10], Step [350000/800000], Loss: 0.6270\n",
      "Epoch [9/10], Step [355000/800000], Loss: 0.6123\n",
      "Epoch [9/10], Step [360000/800000], Loss: 0.5238\n",
      "Epoch [9/10], Step [365000/800000], Loss: 0.5681\n",
      "Epoch [9/10], Step [370000/800000], Loss: 0.4750\n",
      "Epoch [9/10], Step [375000/800000], Loss: 0.1480\n",
      "Epoch [9/10], Step [380000/800000], Loss: 0.6361\n",
      "Epoch [9/10], Step [385000/800000], Loss: 0.3618\n",
      "Epoch [9/10], Step [390000/800000], Loss: 1.5310\n",
      "Epoch [9/10], Step [395000/800000], Loss: 0.7351\n",
      "Epoch [9/10], Step [400000/800000], Loss: 0.2604\n",
      "Epoch [9/10], Step [405000/800000], Loss: 0.7971\n",
      "Epoch [9/10], Step [410000/800000], Loss: 0.9702\n",
      "Epoch [9/10], Step [415000/800000], Loss: 0.7821\n",
      "Epoch [9/10], Step [420000/800000], Loss: 0.4988\n",
      "Epoch [9/10], Step [425000/800000], Loss: 0.6970\n",
      "Epoch [9/10], Step [430000/800000], Loss: 2.8987\n",
      "Epoch [9/10], Step [435000/800000], Loss: 0.6351\n",
      "Epoch [9/10], Step [440000/800000], Loss: 0.1024\n",
      "Epoch [9/10], Step [445000/800000], Loss: 0.4449\n",
      "Epoch [9/10], Step [450000/800000], Loss: 0.4493\n",
      "Epoch [9/10], Step [455000/800000], Loss: 1.6440\n",
      "Epoch [9/10], Step [460000/800000], Loss: 0.7390\n",
      "Epoch [9/10], Step [465000/800000], Loss: 1.2184\n",
      "Epoch [9/10], Step [470000/800000], Loss: 0.8327\n",
      "Epoch [9/10], Step [475000/800000], Loss: 0.3415\n",
      "Epoch [9/10], Step [480000/800000], Loss: 0.5403\n",
      "Epoch [9/10], Step [485000/800000], Loss: 0.6934\n",
      "Epoch [9/10], Step [490000/800000], Loss: 0.4128\n",
      "Epoch [9/10], Step [495000/800000], Loss: 0.4787\n",
      "Epoch [9/10], Step [500000/800000], Loss: 0.6264\n",
      "Epoch [9/10], Step [505000/800000], Loss: 1.1052\n",
      "Epoch [9/10], Step [510000/800000], Loss: 0.7359\n",
      "Epoch [9/10], Step [515000/800000], Loss: 1.1652\n",
      "Epoch [9/10], Step [520000/800000], Loss: 0.8936\n",
      "Epoch [9/10], Step [525000/800000], Loss: 0.7600\n",
      "Epoch [9/10], Step [530000/800000], Loss: 0.7659\n",
      "Epoch [9/10], Step [535000/800000], Loss: 0.5394\n",
      "Epoch [9/10], Step [540000/800000], Loss: 0.4685\n",
      "Epoch [9/10], Step [545000/800000], Loss: 0.6582\n",
      "Epoch [9/10], Step [550000/800000], Loss: 0.6066\n",
      "Epoch [9/10], Step [555000/800000], Loss: 0.5039\n",
      "Epoch [9/10], Step [560000/800000], Loss: 1.2265\n",
      "Epoch [9/10], Step [565000/800000], Loss: 0.7619\n",
      "Epoch [9/10], Step [570000/800000], Loss: 0.2462\n",
      "Epoch [9/10], Step [575000/800000], Loss: 0.6594\n",
      "Epoch [9/10], Step [580000/800000], Loss: 1.0788\n",
      "Epoch [9/10], Step [585000/800000], Loss: 0.8213\n",
      "Epoch [9/10], Step [590000/800000], Loss: 0.6009\n",
      "Epoch [9/10], Step [595000/800000], Loss: 0.7656\n",
      "Epoch [9/10], Step [600000/800000], Loss: 0.7925\n",
      "Epoch [9/10], Step [605000/800000], Loss: 0.8878\n",
      "Epoch [9/10], Step [610000/800000], Loss: 0.6743\n",
      "Epoch [9/10], Step [615000/800000], Loss: 0.7053\n",
      "Epoch [9/10], Step [620000/800000], Loss: 0.6792\n",
      "Epoch [9/10], Step [625000/800000], Loss: 0.5097\n",
      "Epoch [9/10], Step [630000/800000], Loss: 0.6055\n",
      "Epoch [9/10], Step [635000/800000], Loss: 0.6384\n",
      "Epoch [9/10], Step [640000/800000], Loss: 0.7120\n",
      "Epoch [9/10], Step [645000/800000], Loss: 0.5757\n",
      "Epoch [9/10], Step [650000/800000], Loss: 0.9242\n",
      "Epoch [9/10], Step [655000/800000], Loss: 0.4348\n",
      "Epoch [9/10], Step [660000/800000], Loss: 0.5838\n",
      "Epoch [9/10], Step [665000/800000], Loss: 0.6946\n",
      "Epoch [9/10], Step [670000/800000], Loss: 0.7550\n",
      "Epoch [9/10], Step [675000/800000], Loss: 0.6895\n",
      "Epoch [9/10], Step [680000/800000], Loss: 0.8154\n",
      "Epoch [9/10], Step [685000/800000], Loss: 0.7194\n",
      "Epoch [9/10], Step [690000/800000], Loss: 0.6584\n",
      "Epoch [9/10], Step [695000/800000], Loss: 0.6883\n",
      "Epoch [9/10], Step [700000/800000], Loss: 0.7901\n",
      "Epoch [9/10], Step [705000/800000], Loss: 0.8310\n",
      "Epoch [9/10], Step [710000/800000], Loss: 0.7642\n",
      "Epoch [9/10], Step [715000/800000], Loss: 0.5197\n",
      "Epoch [9/10], Step [720000/800000], Loss: 0.5774\n",
      "Epoch [9/10], Step [725000/800000], Loss: 1.0499\n",
      "Epoch [9/10], Step [730000/800000], Loss: 1.0852\n",
      "Epoch [9/10], Step [735000/800000], Loss: 0.9087\n",
      "Epoch [9/10], Step [740000/800000], Loss: 0.3924\n",
      "Epoch [9/10], Step [745000/800000], Loss: 0.5770\n",
      "Epoch [9/10], Step [750000/800000], Loss: 0.7020\n",
      "Epoch [9/10], Step [755000/800000], Loss: 0.7192\n",
      "Epoch [9/10], Step [760000/800000], Loss: 0.5221\n",
      "Epoch [9/10], Step [765000/800000], Loss: 0.5062\n",
      "Epoch [9/10], Step [770000/800000], Loss: 1.0007\n",
      "Epoch [9/10], Step [775000/800000], Loss: 1.3094\n",
      "Epoch [9/10], Step [780000/800000], Loss: 0.7169\n",
      "Epoch [9/10], Step [785000/800000], Loss: 0.9149\n",
      "Epoch [9/10], Step [790000/800000], Loss: 0.3318\n",
      "Epoch [9/10], Step [795000/800000], Loss: 0.9212\n",
      "Epoch [9/10], Step [800000/800000], Loss: 0.3320\n",
      "Epoch [10/10], Step [5000/800000], Loss: 0.5608\n",
      "Epoch [10/10], Step [10000/800000], Loss: 0.6592\n",
      "Epoch [10/10], Step [15000/800000], Loss: 0.6715\n",
      "Epoch [10/10], Step [20000/800000], Loss: 0.8142\n",
      "Epoch [10/10], Step [25000/800000], Loss: 0.6703\n",
      "Epoch [10/10], Step [30000/800000], Loss: 0.5059\n",
      "Epoch [10/10], Step [35000/800000], Loss: 0.6299\n",
      "Epoch [10/10], Step [40000/800000], Loss: 0.8281\n",
      "Epoch [10/10], Step [45000/800000], Loss: 1.0185\n",
      "Epoch [10/10], Step [50000/800000], Loss: 0.0297\n",
      "Epoch [10/10], Step [55000/800000], Loss: 0.7973\n",
      "Epoch [10/10], Step [60000/800000], Loss: 0.1095\n",
      "Epoch [10/10], Step [65000/800000], Loss: 4.8316\n",
      "Epoch [10/10], Step [70000/800000], Loss: 0.7599\n",
      "Epoch [10/10], Step [75000/800000], Loss: 0.9120\n",
      "Epoch [10/10], Step [80000/800000], Loss: 0.8214\n",
      "Epoch [10/10], Step [85000/800000], Loss: 0.3672\n",
      "Epoch [10/10], Step [90000/800000], Loss: 0.9901\n",
      "Epoch [10/10], Step [95000/800000], Loss: 0.5795\n",
      "Epoch [10/10], Step [100000/800000], Loss: 0.4626\n",
      "Epoch [10/10], Step [105000/800000], Loss: 0.8194\n",
      "Epoch [10/10], Step [110000/800000], Loss: 0.5820\n",
      "Epoch [10/10], Step [115000/800000], Loss: 0.4320\n",
      "Epoch [10/10], Step [120000/800000], Loss: 0.6485\n",
      "Epoch [10/10], Step [125000/800000], Loss: 0.8183\n",
      "Epoch [10/10], Step [130000/800000], Loss: 0.6583\n",
      "Epoch [10/10], Step [135000/800000], Loss: 0.1980\n",
      "Epoch [10/10], Step [140000/800000], Loss: 0.7444\n",
      "Epoch [10/10], Step [145000/800000], Loss: 0.7154\n",
      "Epoch [10/10], Step [150000/800000], Loss: 0.8010\n",
      "Epoch [10/10], Step [155000/800000], Loss: 1.6359\n",
      "Epoch [10/10], Step [160000/800000], Loss: 0.5423\n",
      "Epoch [10/10], Step [165000/800000], Loss: 0.7644\n",
      "Epoch [10/10], Step [170000/800000], Loss: 0.3540\n",
      "Epoch [10/10], Step [175000/800000], Loss: 0.6066\n",
      "Epoch [10/10], Step [180000/800000], Loss: 0.9809\n",
      "Epoch [10/10], Step [185000/800000], Loss: 0.6639\n",
      "Epoch [10/10], Step [190000/800000], Loss: 0.8059\n",
      "Epoch [10/10], Step [195000/800000], Loss: 0.5937\n",
      "Epoch [10/10], Step [200000/800000], Loss: 0.5973\n",
      "Epoch [10/10], Step [205000/800000], Loss: 0.7384\n",
      "Epoch [10/10], Step [210000/800000], Loss: 1.8034\n",
      "Epoch [10/10], Step [215000/800000], Loss: 0.3600\n",
      "Epoch [10/10], Step [220000/800000], Loss: 0.5285\n",
      "Epoch [10/10], Step [225000/800000], Loss: 1.5962\n",
      "Epoch [10/10], Step [230000/800000], Loss: 1.4205\n",
      "Epoch [10/10], Step [235000/800000], Loss: 0.2981\n",
      "Epoch [10/10], Step [240000/800000], Loss: 1.7417\n",
      "Epoch [10/10], Step [245000/800000], Loss: 0.8389\n",
      "Epoch [10/10], Step [250000/800000], Loss: 0.8883\n",
      "Epoch [10/10], Step [255000/800000], Loss: 1.1976\n",
      "Epoch [10/10], Step [260000/800000], Loss: 0.5302\n",
      "Epoch [10/10], Step [265000/800000], Loss: 0.6310\n",
      "Epoch [10/10], Step [270000/800000], Loss: 0.7519\n",
      "Epoch [10/10], Step [275000/800000], Loss: 0.6888\n",
      "Epoch [10/10], Step [280000/800000], Loss: 0.5680\n",
      "Epoch [10/10], Step [285000/800000], Loss: 0.2140\n",
      "Epoch [10/10], Step [290000/800000], Loss: 0.3560\n",
      "Epoch [10/10], Step [295000/800000], Loss: 0.5386\n",
      "Epoch [10/10], Step [300000/800000], Loss: 1.0043\n",
      "Epoch [10/10], Step [305000/800000], Loss: 0.6842\n",
      "Epoch [10/10], Step [310000/800000], Loss: 0.2875\n",
      "Epoch [10/10], Step [315000/800000], Loss: 0.6194\n",
      "Epoch [10/10], Step [320000/800000], Loss: 1.5392\n",
      "Epoch [10/10], Step [325000/800000], Loss: 1.3276\n",
      "Epoch [10/10], Step [330000/800000], Loss: 0.7714\n",
      "Epoch [10/10], Step [335000/800000], Loss: 1.0835\n",
      "Epoch [10/10], Step [340000/800000], Loss: 0.5813\n",
      "Epoch [10/10], Step [345000/800000], Loss: 0.5023\n",
      "Epoch [10/10], Step [350000/800000], Loss: 0.7030\n",
      "Epoch [10/10], Step [355000/800000], Loss: 0.5593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [360000/800000], Loss: 0.4704\n",
      "Epoch [10/10], Step [365000/800000], Loss: 0.5698\n",
      "Epoch [10/10], Step [370000/800000], Loss: 0.3799\n",
      "Epoch [10/10], Step [375000/800000], Loss: 0.5221\n",
      "Epoch [10/10], Step [380000/800000], Loss: 2.0788\n",
      "Epoch [10/10], Step [385000/800000], Loss: 0.5488\n",
      "Epoch [10/10], Step [390000/800000], Loss: 0.8392\n",
      "Epoch [10/10], Step [395000/800000], Loss: 0.6325\n",
      "Epoch [10/10], Step [400000/800000], Loss: 0.3038\n",
      "Epoch [10/10], Step [405000/800000], Loss: 0.5114\n",
      "Epoch [10/10], Step [410000/800000], Loss: 0.7879\n",
      "Epoch [10/10], Step [415000/800000], Loss: 0.7201\n",
      "Epoch [10/10], Step [420000/800000], Loss: 3.5857\n",
      "Epoch [10/10], Step [425000/800000], Loss: 0.7996\n",
      "Epoch [10/10], Step [430000/800000], Loss: 2.0716\n",
      "Epoch [10/10], Step [435000/800000], Loss: 0.6943\n",
      "Epoch [10/10], Step [440000/800000], Loss: 0.3130\n",
      "Epoch [10/10], Step [445000/800000], Loss: 0.7865\n",
      "Epoch [10/10], Step [450000/800000], Loss: 0.2051\n",
      "Epoch [10/10], Step [455000/800000], Loss: 0.8318\n",
      "Epoch [10/10], Step [460000/800000], Loss: 0.3440\n",
      "Epoch [10/10], Step [465000/800000], Loss: 0.6229\n",
      "Epoch [10/10], Step [470000/800000], Loss: 0.5383\n",
      "Epoch [10/10], Step [475000/800000], Loss: 0.6899\n",
      "Epoch [10/10], Step [480000/800000], Loss: 0.6325\n",
      "Epoch [10/10], Step [485000/800000], Loss: 0.6787\n",
      "Epoch [10/10], Step [490000/800000], Loss: 0.4272\n",
      "Epoch [10/10], Step [495000/800000], Loss: 0.4513\n",
      "Epoch [10/10], Step [500000/800000], Loss: 0.8126\n",
      "Epoch [10/10], Step [505000/800000], Loss: 1.2563\n",
      "Epoch [10/10], Step [510000/800000], Loss: 0.6055\n",
      "Epoch [10/10], Step [515000/800000], Loss: 0.9817\n",
      "Epoch [10/10], Step [520000/800000], Loss: 0.9096\n",
      "Epoch [10/10], Step [525000/800000], Loss: 0.7114\n",
      "Epoch [10/10], Step [530000/800000], Loss: 1.2914\n",
      "Epoch [10/10], Step [535000/800000], Loss: 0.5963\n",
      "Epoch [10/10], Step [540000/800000], Loss: 0.8083\n",
      "Epoch [10/10], Step [545000/800000], Loss: 0.8244\n",
      "Epoch [10/10], Step [550000/800000], Loss: 0.7170\n",
      "Epoch [10/10], Step [555000/800000], Loss: 0.2307\n",
      "Epoch [10/10], Step [560000/800000], Loss: 0.8583\n",
      "Epoch [10/10], Step [565000/800000], Loss: 0.7006\n",
      "Epoch [10/10], Step [570000/800000], Loss: 0.7559\n",
      "Epoch [10/10], Step [575000/800000], Loss: 0.7921\n",
      "Epoch [10/10], Step [580000/800000], Loss: 1.0779\n",
      "Epoch [10/10], Step [585000/800000], Loss: 0.8172\n",
      "Epoch [10/10], Step [590000/800000], Loss: 0.4102\n",
      "Epoch [10/10], Step [595000/800000], Loss: 0.7899\n",
      "Epoch [10/10], Step [600000/800000], Loss: 1.1013\n",
      "Epoch [10/10], Step [605000/800000], Loss: 0.5404\n",
      "Epoch [10/10], Step [610000/800000], Loss: 0.6460\n",
      "Epoch [10/10], Step [615000/800000], Loss: 0.7964\n",
      "Epoch [10/10], Step [620000/800000], Loss: 0.5974\n",
      "Epoch [10/10], Step [625000/800000], Loss: 0.6250\n",
      "Epoch [10/10], Step [630000/800000], Loss: 0.8402\n",
      "Epoch [10/10], Step [635000/800000], Loss: 0.7176\n",
      "Epoch [10/10], Step [640000/800000], Loss: 0.7943\n",
      "Epoch [10/10], Step [645000/800000], Loss: 0.3437\n",
      "Epoch [10/10], Step [650000/800000], Loss: 0.6482\n",
      "Epoch [10/10], Step [655000/800000], Loss: 0.3035\n",
      "Epoch [10/10], Step [660000/800000], Loss: 0.7966\n",
      "Epoch [10/10], Step [665000/800000], Loss: 0.4783\n",
      "Epoch [10/10], Step [670000/800000], Loss: 0.8006\n",
      "Epoch [10/10], Step [675000/800000], Loss: 0.7037\n",
      "Epoch [10/10], Step [680000/800000], Loss: 0.7707\n",
      "Epoch [10/10], Step [685000/800000], Loss: 0.8972\n",
      "Epoch [10/10], Step [690000/800000], Loss: 0.5790\n",
      "Epoch [10/10], Step [695000/800000], Loss: 0.6434\n",
      "Epoch [10/10], Step [700000/800000], Loss: 0.8054\n",
      "Epoch [10/10], Step [705000/800000], Loss: 0.7384\n",
      "Epoch [10/10], Step [710000/800000], Loss: 1.0255\n",
      "Epoch [10/10], Step [715000/800000], Loss: 0.5894\n",
      "Epoch [10/10], Step [720000/800000], Loss: 0.7180\n",
      "Epoch [10/10], Step [725000/800000], Loss: 1.3755\n",
      "Epoch [10/10], Step [730000/800000], Loss: 0.8167\n",
      "Epoch [10/10], Step [735000/800000], Loss: 0.6215\n",
      "Epoch [10/10], Step [740000/800000], Loss: 0.5734\n",
      "Epoch [10/10], Step [745000/800000], Loss: 0.6369\n",
      "Epoch [10/10], Step [750000/800000], Loss: 0.7612\n",
      "Epoch [10/10], Step [755000/800000], Loss: 0.7082\n",
      "Epoch [10/10], Step [760000/800000], Loss: 0.6613\n",
      "Epoch [10/10], Step [765000/800000], Loss: 0.2471\n",
      "Epoch [10/10], Step [770000/800000], Loss: 0.7286\n",
      "Epoch [10/10], Step [775000/800000], Loss: 1.3942\n",
      "Epoch [10/10], Step [780000/800000], Loss: 0.4646\n",
      "Epoch [10/10], Step [785000/800000], Loss: 0.6061\n",
      "Epoch [10/10], Step [790000/800000], Loss: 0.3573\n",
      "Epoch [10/10], Step [795000/800000], Loss: 1.1105\n",
      "Epoch [10/10], Step [800000/800000], Loss: 0.5953\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "print_interval = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #random.shuffle(train_dataset)\n",
    "    for i, (inputs, label) in enumerate(train_dataset):\n",
    "        hidden_state = model.init_hidden()\n",
    "        label = torch.tensor([label])\n",
    "        for coeff in inputs:\n",
    "            coeff = torch.Tensor([[coeff]])\n",
    "#             print(type(hidden_state))\n",
    "#             print(hidden_state)\n",
    "#             print('####')\n",
    "#             print(torch.cat((coeff, hidden_state),1))\n",
    "            output, hidden_state = model(coeff, hidden_state)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % print_interval == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                f\"Step [{i + 1}/{len(train_dataset)}], \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.7410%\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_samples = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, label in test_dataset:\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            coeff = torch.Tensor([[char]])\n",
    "            output, hidden_state = model(coeff, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == label)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNeuralNetModel(\n",
      "  (fc1): Linear(in_features=7, out_features=100, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc4): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (batch_norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## CovNet\n",
    "\n",
    "\n",
    "# defining the model\n",
    "covModel = CovNet()\n",
    "# defining the optimizer\n",
    "optimizer = torch.optim.Adam(covModel.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    # getting the training set\n",
    "    x_train, y_train = torch.autograd.Variable(train_x), torch.autograd.Variable(train_y)\n",
    "    # getting the validation set\n",
    "    x_val, y_val = torch.autograd.Variable(val_x), torch.autograd.Variable(val_y)\n",
    "    # converting the data into GPU format\n",
    "    if torch.cuda.is_available():\n",
    "        x_train = x_train.cuda()\n",
    "        y_train = y_train.cuda()\n",
    "        x_val = x_val.cuda()\n",
    "        y_val = y_val.cuda()\n",
    "\n",
    "    # clearing the Gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # prediction for training and validation set\n",
    "    output_train = model(x_train)\n",
    "    output_val = model(x_val)\n",
    "\n",
    "    # computing the training and validation loss\n",
    "    loss_train = criterion(output_train, y_train)\n",
    "    loss_val = criterion(output_val, y_val)\n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "\n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "    if epoch%2 == 0:\n",
    "        # printing the validation loss\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-4192b243c486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-127-072bba51b12f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# prediction for training and validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutput_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moutput_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch2/hle/py3_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-7da63b0bd2c5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m#out = self.dropout(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#out = self.batch_norm(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch2/hle/py3_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch2/hle/py3_env/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch2/hle/py3_env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "train_x = torch.from_numpy(X_train)\n",
    "train_y = torch.from_numpy(np.array(y_train))\n",
    "val_x = torch.from_numpy(X_test)\n",
    "val_y = torch.from_numpy(np.array(y_test))\n",
    "# defining the number of epochs\n",
    "n_epochs = 10\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# empty list to store validation losses\n",
    "val_losses = []\n",
    "# training the model\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='rbf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_X = X_train[:100000,:]\n",
    "chunk_Y = y_train[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model using the training sets\n",
    "clf.fit(chunk_X, chunk_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_chunk = X_test[:40000,:]\n",
    "y_pred = clf.predict(X_test_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.723625\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test[:40000], y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVM rbf kernel:\n",
    "##### training data 100k -> acc: 72.3625%\n",
    "## SVM poly kernel, degree = 7: \n",
    "##### training data 40k -> acc: 75.5%\n",
    "##### training data 60k -> acc: 77.6225%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7c5b39524a43c688ef5c225785c79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d0c9f9d391477cb166c756d5e3b444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=569.0, style=ProgressStyle(description_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c957728f28c446097af2e3671911322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a28b5ee14e94718813ffa58faf212a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440514422.0, style=ProgressStyle(descri"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tbs17/MathBERT-custom were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tbs17/MathBERT-custom\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"tbs17/MathBERT-custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model taking both inputs \n",
    "#combine short AND long.\n",
    "#histogram of data as input\n",
    "# polynomial SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.random.laplace(loc=15, scale=3, size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist, bin_edges = np.histogram(long_test_pd.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.670652</td>\n",
       "      <td>-3.670652</td>\n",
       "      <td>-3.713907</td>\n",
       "      <td>-3.753259</td>\n",
       "      <td>-3.753259</td>\n",
       "      <td>-3.753259</td>\n",
       "      <td>-3.753259</td>\n",
       "      <td>-3.846096</td>\n",
       "      <td>-3.846096</td>\n",
       "      <td>-3.846096</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.987661</td>\n",
       "      <td>-3.939421</td>\n",
       "      <td>-3.955389</td>\n",
       "      <td>-3.955389</td>\n",
       "      <td>-3.955389</td>\n",
       "      <td>-3.955389</td>\n",
       "      <td>-3.955389</td>\n",
       "      <td>-3.955389</td>\n",
       "      <td>-3.962344</td>\n",
       "      <td>-3.962344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.670652</td>\n",
       "      <td>3.670652</td>\n",
       "      <td>3.713907</td>\n",
       "      <td>3.753259</td>\n",
       "      <td>3.753259</td>\n",
       "      <td>3.753259</td>\n",
       "      <td>3.753259</td>\n",
       "      <td>3.753259</td>\n",
       "      <td>3.846096</td>\n",
       "      <td>3.846096</td>\n",
       "      <td>...</td>\n",
       "      <td>3.987661</td>\n",
       "      <td>3.939421</td>\n",
       "      <td>3.942972</td>\n",
       "      <td>3.942972</td>\n",
       "      <td>3.955389</td>\n",
       "      <td>3.955389</td>\n",
       "      <td>3.955389</td>\n",
       "      <td>3.955389</td>\n",
       "      <td>3.933636</td>\n",
       "      <td>3.962344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "min -3.670652 -3.670652 -3.713907 -3.753259 -3.753259 -3.753259 -3.753259   \n",
       "max  3.670652  3.670652  3.713907  3.753259  3.753259  3.753259  3.753259   \n",
       "\n",
       "           7         8         9   ...        90        91        92  \\\n",
       "min -3.846096 -3.846096 -3.846096  ... -3.987661 -3.939421 -3.955389   \n",
       "max  3.753259  3.846096  3.846096  ...  3.987661  3.939421  3.942972   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "min -3.955389 -3.955389 -3.955389 -3.955389 -3.955389 -3.962344 -3.962344  \n",
       "max  3.942972  3.955389  3.955389  3.955389  3.955389  3.933636  3.962344  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#min(long_test_pd.iloc[0])\n",
    "def minMax(x):\n",
    "    return pd.Series(index=['min','max'],data=[x.min(),x.max()])\n",
    "\n",
    "long_test_pd.apply(minMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000000/1000000 [03:43<00:00, 4482.49it/s]\n"
     ]
    }
   ],
   "source": [
    "hist_data_large = []\n",
    "for i in tqdm(range(1000000)):\n",
    "    hist, bin_edges = np.histogram(long_test_pd.iloc[i])\n",
    "    hist_data_large.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hist_data_large:\n",
    "    if len(i) != 10:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 30.0)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=long_test_pd.iloc[0], bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('My Very Own Histogram')\n",
    "plt.text(23, 45, r'$\\mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./g2st2.txt') as f:\n",
    "    data = [l for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = [point.split(':') for point in data]\n",
    "labels_data = [(ord(point[0]) - ord('A')) for point in datapoints]\n",
    "input_long = [json.loads(point[4]) for point in datapoints]\n",
    "input_short = [json.loads(point[2]) for point in datapoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd[6] = test_pd[6].fillna(0)\n",
    "# normalized_test_pd = preprocessing.normalize(test_pd)\n",
    "# normalized_test_pd = pd.DataFrame(normalized_test_pd, columns=test_pd.columns)\n",
    "normalized_test_pd = preprocessing.normalize(test_pd)\n",
    "normalized_test_pd = pd.DataFrame(normalized_test_pd, columns=test_pd.columns)\n",
    "X = normalized_test_pd.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels_data, test_size=0.2)\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "train_dataset = PolynomialDataset(X_train, y_train)\n",
    "test_dataset = PolynomialDataset(X_test, y_test)\n",
    "batch_size = 50\n",
    "n_iters = 160000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n",
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.7957994341850281. Accuracy: 49.9715\n",
      "Iteration: 1000. Loss: 0.9078439474105835. Accuracy: 46.905\n",
      "Iteration: 1500. Loss: 0.7555695176124573. Accuracy: 46.905\n",
      "Iteration: 2000. Loss: 0.8731020092964172. Accuracy: 49.9715\n",
      "Iteration: 2500. Loss: 0.7843074202537537. Accuracy: 49.9715\n",
      "Iteration: 3000. Loss: 0.7954131960868835. Accuracy: 49.9715\n",
      "Iteration: 3500. Loss: 0.843656599521637. Accuracy: 46.905\n",
      "Iteration: 4000. Loss: 0.8131417036056519. Accuracy: 49.9715\n",
      "Iteration: 4500. Loss: 0.8284525275230408. Accuracy: 49.9715\n",
      "Iteration: 5000. Loss: 0.8486237525939941. Accuracy: 49.9695\n",
      "Iteration: 5500. Loss: 1.0566089153289795. Accuracy: 49.9715\n",
      "Iteration: 6000. Loss: 0.820784330368042. Accuracy: 49.9715\n",
      "Iteration: 6500. Loss: 0.7080662250518799. Accuracy: 49.9715\n",
      "Iteration: 7000. Loss: 0.9724759459495544. Accuracy: 49.8615\n",
      "Iteration: 7500. Loss: 0.7862006425857544. Accuracy: 49.9715\n",
      "Iteration: 8000. Loss: 0.8680818676948547. Accuracy: 49.9715\n",
      "Iteration: 8500. Loss: 0.780670166015625. Accuracy: 49.9715\n",
      "Iteration: 9000. Loss: 0.7792513370513916. Accuracy: 49.9715\n",
      "Iteration: 9500. Loss: 0.7920272350311279. Accuracy: 49.9715\n",
      "Iteration: 10000. Loss: 0.7721797823905945. Accuracy: 49.9715\n",
      "Iteration: 10500. Loss: 0.9349650740623474. Accuracy: 49.9715\n",
      "Iteration: 11000. Loss: 0.7493674755096436. Accuracy: 49.9715\n",
      "Iteration: 11500. Loss: 0.7495529651641846. Accuracy: 49.9715\n",
      "Iteration: 12000. Loss: 0.7678554058074951. Accuracy: 49.9715\n",
      "Iteration: 12500. Loss: 0.7921971678733826. Accuracy: 49.151\n",
      "Iteration: 13000. Loss: 0.7287131547927856. Accuracy: 49.9715\n",
      "Iteration: 13500. Loss: 0.7270888686180115. Accuracy: 49.9715\n",
      "Iteration: 14000. Loss: 0.7821293473243713. Accuracy: 49.9715\n",
      "Iteration: 14500. Loss: 0.7823628187179565. Accuracy: 46.905\n",
      "Iteration: 15000. Loss: 0.8469603061676025. Accuracy: 49.9715\n",
      "Iteration: 15500. Loss: 0.8693147301673889. Accuracy: 49.9715\n",
      "Iteration: 16000. Loss: 0.9941092133522034. Accuracy: 46.905\n",
      "Iteration: 16500. Loss: 0.873989462852478. Accuracy: 46.905\n",
      "Iteration: 17000. Loss: 0.8402838110923767. Accuracy: 49.9715\n",
      "Iteration: 17500. Loss: 0.8422821164131165. Accuracy: 49.9715\n",
      "Iteration: 18000. Loss: 0.8408359289169312. Accuracy: 48.699\n",
      "Iteration: 18500. Loss: 0.7198050022125244. Accuracy: 49.9715\n",
      "Iteration: 19000. Loss: 0.9917534589767456. Accuracy: 46.905\n",
      "Iteration: 19500. Loss: 0.7719869017601013. Accuracy: 49.9715\n",
      "Iteration: 20000. Loss: 0.8425962328910828. Accuracy: 46.905\n",
      "Iteration: 20500. Loss: 0.8191717267036438. Accuracy: 46.905\n",
      "Iteration: 21000. Loss: 0.8949663639068604. Accuracy: 46.905\n",
      "Iteration: 21500. Loss: 0.8523200154304504. Accuracy: 49.3465\n",
      "Iteration: 22000. Loss: 0.7828121185302734. Accuracy: 49.9715\n",
      "Iteration: 22500. Loss: 0.7822717428207397. Accuracy: 49.9715\n",
      "Iteration: 23000. Loss: 0.7857516407966614. Accuracy: 49.9715\n",
      "Iteration: 23500. Loss: 0.7208495140075684. Accuracy: 46.905\n",
      "Iteration: 24000. Loss: 0.7269922494888306. Accuracy: 48.543\n",
      "Iteration: 24500. Loss: 0.9534702301025391. Accuracy: 46.905\n",
      "Iteration: 25000. Loss: 0.7884832620620728. Accuracy: 49.9715\n",
      "Iteration: 25500. Loss: 0.9025110602378845. Accuracy: 49.9715\n",
      "Iteration: 26000. Loss: 0.9563252329826355. Accuracy: 49.9715\n",
      "Iteration: 26500. Loss: 0.8819432258605957. Accuracy: 49.9715\n",
      "Iteration: 27000. Loss: 0.7262272834777832. Accuracy: 46.905\n",
      "Iteration: 27500. Loss: 0.8398587703704834. Accuracy: 49.9715\n",
      "Iteration: 28000. Loss: 0.8983865976333618. Accuracy: 49.9715\n",
      "Iteration: 28500. Loss: 0.9602221846580505. Accuracy: 49.9715\n",
      "Iteration: 29000. Loss: 0.8127751350402832. Accuracy: 49.9715\n",
      "Iteration: 29500. Loss: 0.7663465142250061. Accuracy: 49.9715\n",
      "Iteration: 30000. Loss: 0.936810314655304. Accuracy: 49.9715\n",
      "Iteration: 30500. Loss: 0.9009305834770203. Accuracy: 49.9715\n",
      "Iteration: 31000. Loss: 0.7227647304534912. Accuracy: 49.9715\n",
      "Iteration: 31500. Loss: 0.8419353365898132. Accuracy: 49.9715\n",
      "Iteration: 32000. Loss: 0.8656381964683533. Accuracy: 49.9715\n",
      "Iteration: 32500. Loss: 0.8997024297714233. Accuracy: 46.905\n",
      "Iteration: 33000. Loss: 0.782789409160614. Accuracy: 46.905\n",
      "Iteration: 33500. Loss: 0.7850609421730042. Accuracy: 49.9715\n",
      "Iteration: 34000. Loss: 0.8957653641700745. Accuracy: 49.9715\n",
      "Iteration: 34500. Loss: 1.0194272994995117. Accuracy: 49.9715\n",
      "Iteration: 35000. Loss: 0.9015044569969177. Accuracy: 46.905\n",
      "Iteration: 35500. Loss: 0.8452475070953369. Accuracy: 49.9715\n",
      "Iteration: 36000. Loss: 0.7288069128990173. Accuracy: 46.905\n",
      "Iteration: 36500. Loss: 0.8462663888931274. Accuracy: 49.9715\n",
      "Iteration: 37000. Loss: 0.7905892133712769. Accuracy: 49.9715\n",
      "Iteration: 37500. Loss: 0.7789100408554077. Accuracy: 49.9715\n",
      "Iteration: 38000. Loss: 1.032673954963684. Accuracy: 49.9715\n",
      "Iteration: 38500. Loss: 0.907091498374939. Accuracy: 49.9715\n",
      "Iteration: 39000. Loss: 0.8746240139007568. Accuracy: 49.9715\n",
      "Iteration: 39500. Loss: 0.8222562670707703. Accuracy: 49.9715\n",
      "Iteration: 40000. Loss: 0.8769810199737549. Accuracy: 49.9715\n",
      "Iteration: 40500. Loss: 0.894317626953125. Accuracy: 49.9715\n",
      "Iteration: 41000. Loss: 0.778555691242218. Accuracy: 49.9715\n",
      "Iteration: 41500. Loss: 0.7072669267654419. Accuracy: 49.9715\n",
      "Iteration: 42000. Loss: 0.8408094048500061. Accuracy: 46.905\n",
      "Iteration: 42500. Loss: 0.7827406525611877. Accuracy: 49.9715\n",
      "Iteration: 43000. Loss: 0.7262769341468811. Accuracy: 49.9715\n",
      "Iteration: 43500. Loss: 0.8538892269134521. Accuracy: 46.905\n",
      "Iteration: 44000. Loss: 0.8979753255844116. Accuracy: 49.9715\n",
      "Iteration: 44500. Loss: 0.8962613940238953. Accuracy: 49.9715\n",
      "Iteration: 45000. Loss: 0.9014620780944824. Accuracy: 49.9715\n",
      "Iteration: 45500. Loss: 0.7844529151916504. Accuracy: 49.9715\n",
      "Iteration: 46000. Loss: 0.8712214827537537. Accuracy: 49.9715\n",
      "Iteration: 46500. Loss: 0.9293439388275146. Accuracy: 49.9715\n",
      "Iteration: 47000. Loss: 0.7424536943435669. Accuracy: 49.9715\n",
      "Iteration: 47500. Loss: 0.8530294299125671. Accuracy: 49.9715\n",
      "Iteration: 48000. Loss: 0.7795037627220154. Accuracy: 49.9715\n",
      "Iteration: 48500. Loss: 0.7182551622390747. Accuracy: 49.9715\n",
      "Iteration: 49000. Loss: 0.9339035153388977. Accuracy: 49.9715\n",
      "Iteration: 49500. Loss: 0.7982798218727112. Accuracy: 49.9715\n",
      "Iteration: 50000. Loss: 0.7913969159126282. Accuracy: 49.9715\n",
      "Iteration: 50500. Loss: 0.9108829498291016. Accuracy: 49.9715\n",
      "Iteration: 51000. Loss: 0.8801714181900024. Accuracy: 49.9715\n",
      "Iteration: 51500. Loss: 0.8711528778076172. Accuracy: 49.9715\n",
      "Iteration: 52000. Loss: 0.8218680620193481. Accuracy: 49.9715\n",
      "Iteration: 52500. Loss: 0.8572011590003967. Accuracy: 49.9715\n",
      "Iteration: 53000. Loss: 0.8207152485847473. Accuracy: 49.9715\n",
      "Iteration: 53500. Loss: 0.9031901359558105. Accuracy: 49.9715\n",
      "Iteration: 54000. Loss: 0.8700150847434998. Accuracy: 49.9715\n",
      "Iteration: 54500. Loss: 0.7837494611740112. Accuracy: 49.9715\n",
      "Iteration: 55000. Loss: 0.7274530529975891. Accuracy: 49.9715\n",
      "Iteration: 55500. Loss: 0.7273242473602295. Accuracy: 49.9715\n",
      "Iteration: 56000. Loss: 0.8109847903251648. Accuracy: 49.9715\n",
      "Iteration: 56500. Loss: 0.7863102555274963. Accuracy: 49.9715\n",
      "Iteration: 57000. Loss: 0.7355514764785767. Accuracy: 49.9715\n",
      "Iteration: 57500. Loss: 0.9930368661880493. Accuracy: 49.9715\n",
      "Iteration: 58000. Loss: 0.9613233208656311. Accuracy: 46.905\n",
      "Iteration: 58500. Loss: 0.7828314900398254. Accuracy: 49.46\n",
      "Iteration: 59000. Loss: 0.9918657541275024. Accuracy: 49.905\n",
      "Iteration: 59500. Loss: 0.7790693640708923. Accuracy: 49.9715\n",
      "Iteration: 60000. Loss: 0.8417039513587952. Accuracy: 49.9715\n",
      "Iteration: 60500. Loss: 0.7883701920509338. Accuracy: 49.9715\n",
      "Iteration: 61000. Loss: 0.9346643090248108. Accuracy: 46.905\n",
      "Iteration: 61500. Loss: 0.8417916893959045. Accuracy: 49.9715\n",
      "Iteration: 62000. Loss: 0.7215263843536377. Accuracy: 49.9715\n",
      "Iteration: 62500. Loss: 0.7222515344619751. Accuracy: 49.9715\n",
      "Iteration: 63000. Loss: 0.8397921919822693. Accuracy: 49.9715\n",
      "Iteration: 63500. Loss: 0.7278556227684021. Accuracy: 49.9715\n",
      "Iteration: 64000. Loss: 0.7829492092132568. Accuracy: 46.905\n",
      "Iteration: 64500. Loss: 0.7353469133377075. Accuracy: 49.9715\n",
      "Iteration: 65000. Loss: 0.7797396183013916. Accuracy: 49.9715\n",
      "Iteration: 65500. Loss: 0.9586200714111328. Accuracy: 46.905\n",
      "Iteration: 66000. Loss: 0.8373978137969971. Accuracy: 49.9715\n",
      "Iteration: 66500. Loss: 0.7902885675430298. Accuracy: 49.9715\n",
      "Iteration: 67000. Loss: 0.777543842792511. Accuracy: 49.9715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 67500. Loss: 0.7801868319511414. Accuracy: 49.9715\n",
      "Iteration: 68000. Loss: 0.7866240739822388. Accuracy: 49.9715\n",
      "Iteration: 68500. Loss: 0.9021982550621033. Accuracy: 49.9715\n",
      "Iteration: 69000. Loss: 0.9380286931991577. Accuracy: 49.9715\n",
      "Iteration: 69500. Loss: 0.7183753848075867. Accuracy: 49.9715\n",
      "Iteration: 70000. Loss: 0.9590854048728943. Accuracy: 46.905\n",
      "Iteration: 70500. Loss: 0.7295603156089783. Accuracy: 49.9715\n",
      "Iteration: 71000. Loss: 0.7778328657150269. Accuracy: 46.905\n",
      "Iteration: 71500. Loss: 0.8106796741485596. Accuracy: 49.9715\n",
      "Iteration: 72000. Loss: 0.7850358486175537. Accuracy: 46.905\n",
      "Iteration: 72500. Loss: 0.8471395373344421. Accuracy: 49.9715\n",
      "Iteration: 73000. Loss: 0.9037904143333435. Accuracy: 49.9715\n",
      "Iteration: 73500. Loss: 0.7847880721092224. Accuracy: 49.9715\n",
      "Iteration: 74000. Loss: 0.9342987537384033. Accuracy: 49.9715\n",
      "Iteration: 74500. Loss: 0.9613833427429199. Accuracy: 49.9625\n",
      "Iteration: 75000. Loss: 0.7769737839698792. Accuracy: 49.9715\n",
      "Iteration: 75500. Loss: 0.8629921078681946. Accuracy: 49.9715\n",
      "Iteration: 76000. Loss: 0.8179768323898315. Accuracy: 49.9715\n",
      "Iteration: 76500. Loss: 0.9121692180633545. Accuracy: 49.9715\n",
      "Iteration: 77000. Loss: 0.9463329315185547. Accuracy: 49.9715\n",
      "Iteration: 77500. Loss: 0.7355937361717224. Accuracy: 49.9715\n",
      "Iteration: 78000. Loss: 0.7817943692207336. Accuracy: 46.905\n",
      "Iteration: 78500. Loss: 0.9427425265312195. Accuracy: 49.9715\n",
      "Iteration: 79000. Loss: 0.7963324785232544. Accuracy: 49.9715\n",
      "Iteration: 79500. Loss: 0.785240888595581. Accuracy: 49.008\n",
      "Iteration: 80000. Loss: 0.8593868017196655. Accuracy: 49.9715\n",
      "Iteration: 80500. Loss: 0.7847856283187866. Accuracy: 49.9715\n",
      "Iteration: 81000. Loss: 0.7815802097320557. Accuracy: 46.905\n",
      "Iteration: 81500. Loss: 0.7887473106384277. Accuracy: 49.9715\n",
      "Iteration: 82000. Loss: 0.9339405298233032. Accuracy: 49.9715\n",
      "Iteration: 82500. Loss: 0.8788896203041077. Accuracy: 49.9715\n",
      "Iteration: 83000. Loss: 0.8717226982116699. Accuracy: 49.9715\n",
      "Iteration: 83500. Loss: 0.8700268268585205. Accuracy: 49.9715\n",
      "Iteration: 84000. Loss: 0.7814115881919861. Accuracy: 49.9715\n",
      "Iteration: 84500. Loss: 0.7851274013519287. Accuracy: 49.9715\n",
      "Iteration: 85000. Loss: 0.7369096279144287. Accuracy: 49.9715\n",
      "Iteration: 85500. Loss: 0.7818505167961121. Accuracy: 49.9715\n",
      "Iteration: 86000. Loss: 0.876984715461731. Accuracy: 49.9715\n",
      "Iteration: 86500. Loss: 0.7812268137931824. Accuracy: 49.9715\n",
      "Iteration: 87000. Loss: 0.8405346870422363. Accuracy: 46.905\n",
      "Iteration: 87500. Loss: 0.8970061540603638. Accuracy: 49.9715\n",
      "Iteration: 88000. Loss: 0.7838062047958374. Accuracy: 49.9715\n",
      "Iteration: 88500. Loss: 0.8976535201072693. Accuracy: 49.9715\n",
      "Iteration: 89000. Loss: 0.8492688536643982. Accuracy: 49.7555\n",
      "Iteration: 89500. Loss: 0.7824946641921997. Accuracy: 46.905\n",
      "Iteration: 90000. Loss: 0.8659331798553467. Accuracy: 49.9715\n",
      "Iteration: 90500. Loss: 0.8731049299240112. Accuracy: 49.9715\n",
      "Iteration: 91000. Loss: 0.7831302881240845. Accuracy: 49.9715\n",
      "Iteration: 91500. Loss: 0.8165014386177063. Accuracy: 49.9715\n",
      "Iteration: 92000. Loss: 0.9343499541282654. Accuracy: 49.9715\n",
      "Iteration: 92500. Loss: 0.7800428867340088. Accuracy: 49.9715\n",
      "Iteration: 93000. Loss: 0.7835777401924133. Accuracy: 49.615\n",
      "Iteration: 93500. Loss: 0.8412522673606873. Accuracy: 46.905\n",
      "Iteration: 94000. Loss: 0.785298764705658. Accuracy: 49.9715\n",
      "Iteration: 94500. Loss: 0.9019709229469299. Accuracy: 49.6505\n",
      "Iteration: 95000. Loss: 0.8411241173744202. Accuracy: 49.9715\n",
      "Iteration: 95500. Loss: 0.730512261390686. Accuracy: 46.905\n",
      "Iteration: 96000. Loss: 0.8289459347724915. Accuracy: 49.9715\n",
      "Iteration: 96500. Loss: 0.7815892100334167. Accuracy: 49.9715\n",
      "Iteration: 97000. Loss: 0.8991056084632874. Accuracy: 49.9715\n",
      "Iteration: 97500. Loss: 0.7823226451873779. Accuracy: 49.9715\n",
      "Iteration: 98000. Loss: 0.8356350064277649. Accuracy: 49.9715\n",
      "Iteration: 98500. Loss: 0.8438660502433777. Accuracy: 49.977\n",
      "Iteration: 99000. Loss: 0.783362627029419. Accuracy: 49.9715\n",
      "Iteration: 99500. Loss: 0.8999340534210205. Accuracy: 49.9715\n",
      "Iteration: 100000. Loss: 0.8707436919212341. Accuracy: 49.9715\n",
      "Iteration: 100500. Loss: 0.8520768880844116. Accuracy: 49.9715\n",
      "Iteration: 101000. Loss: 0.7877953052520752. Accuracy: 49.9715\n",
      "Iteration: 101500. Loss: 0.8436002135276794. Accuracy: 49.9715\n",
      "Iteration: 102000. Loss: 0.819588840007782. Accuracy: 49.9715\n",
      "Iteration: 102500. Loss: 0.9460362792015076. Accuracy: 49.9715\n",
      "Iteration: 103000. Loss: 1.0964370965957642. Accuracy: 49.9715\n",
      "Iteration: 103500. Loss: 0.8477805852890015. Accuracy: 49.9715\n",
      "Iteration: 104000. Loss: 0.8781613707542419. Accuracy: 49.9715\n",
      "Iteration: 104500. Loss: 0.9924785494804382. Accuracy: 49.9715\n",
      "Iteration: 105000. Loss: 0.8416436910629272. Accuracy: 49.9715\n",
      "Iteration: 105500. Loss: 0.7218940854072571. Accuracy: 49.9715\n",
      "Iteration: 106000. Loss: 0.8293620944023132. Accuracy: 49.9715\n",
      "Iteration: 106500. Loss: 0.8406345844268799. Accuracy: 46.905\n",
      "Iteration: 107000. Loss: 0.9626666903495789. Accuracy: 49.9715\n",
      "Iteration: 107500. Loss: 0.8300614356994629. Accuracy: 49.9715\n",
      "Iteration: 108000. Loss: 0.7662584781646729. Accuracy: 49.9715\n",
      "Iteration: 108500. Loss: 0.7888562679290771. Accuracy: 49.9715\n",
      "Iteration: 109000. Loss: 0.8491222262382507. Accuracy: 47.3095\n",
      "Iteration: 109500. Loss: 0.9674598574638367. Accuracy: 46.905\n",
      "Iteration: 110000. Loss: 0.7137985229492188. Accuracy: 49.9715\n",
      "Iteration: 110500. Loss: 0.8962949514389038. Accuracy: 49.9715\n",
      "Iteration: 111000. Loss: 0.8414379954338074. Accuracy: 49.9715\n",
      "Iteration: 111500. Loss: 0.8341473340988159. Accuracy: 49.9715\n",
      "Iteration: 112000. Loss: 0.7237998843193054. Accuracy: 49.315\n",
      "Iteration: 112500. Loss: 0.8354408740997314. Accuracy: 49.9715\n",
      "Iteration: 113000. Loss: 0.7325401306152344. Accuracy: 49.9715\n",
      "Iteration: 113500. Loss: 0.9510135054588318. Accuracy: 46.905\n",
      "Iteration: 114000. Loss: 0.8403314352035522. Accuracy: 49.9715\n",
      "Iteration: 114500. Loss: 0.9393566846847534. Accuracy: 46.905\n",
      "Iteration: 115000. Loss: 0.8018657565116882. Accuracy: 49.9715\n",
      "Iteration: 115500. Loss: 0.7833027839660645. Accuracy: 49.9715\n",
      "Iteration: 116000. Loss: 0.7256453037261963. Accuracy: 49.9715\n",
      "Iteration: 116500. Loss: 0.9525800347328186. Accuracy: 46.905\n",
      "Iteration: 117000. Loss: 0.7842060923576355. Accuracy: 46.905\n",
      "Iteration: 117500. Loss: 0.9919736385345459. Accuracy: 49.9715\n",
      "Iteration: 118000. Loss: 0.7832230925559998. Accuracy: 49.9715\n",
      "Iteration: 118500. Loss: 0.8549894690513611. Accuracy: 49.9715\n",
      "Iteration: 119000. Loss: 0.8057476878166199. Accuracy: 49.9715\n",
      "Iteration: 119500. Loss: 0.7259012460708618. Accuracy: 46.905\n",
      "Iteration: 120000. Loss: 0.8927884101867676. Accuracy: 49.9715\n",
      "Iteration: 120500. Loss: 0.7285231947898865. Accuracy: 46.905\n",
      "Iteration: 121000. Loss: 0.8130723834037781. Accuracy: 49.9715\n",
      "Iteration: 121500. Loss: 0.7259768843650818. Accuracy: 46.905\n",
      "Iteration: 122000. Loss: 0.9658955931663513. Accuracy: 49.9715\n",
      "Iteration: 122500. Loss: 0.7693844437599182. Accuracy: 49.9715\n",
      "Iteration: 123000. Loss: 0.7842816710472107. Accuracy: 47.5315\n",
      "Iteration: 123500. Loss: 0.8393208384513855. Accuracy: 49.9715\n",
      "Iteration: 124000. Loss: 0.8119471669197083. Accuracy: 49.9715\n",
      "Iteration: 124500. Loss: 0.7892840504646301. Accuracy: 46.905\n",
      "Iteration: 125000. Loss: 0.7696416974067688. Accuracy: 49.9715\n",
      "Iteration: 125500. Loss: 0.7867191433906555. Accuracy: 49.9715\n",
      "Iteration: 126000. Loss: 0.8402709364891052. Accuracy: 46.905\n",
      "Iteration: 126500. Loss: 0.8436432480812073. Accuracy: 49.9715\n",
      "Iteration: 127000. Loss: 0.8845915198326111. Accuracy: 49.9715\n",
      "Iteration: 127500. Loss: 0.7852143049240112. Accuracy: 49.9715\n",
      "Iteration: 128000. Loss: 0.8743675947189331. Accuracy: 49.9715\n",
      "Iteration: 128500. Loss: 0.7787303924560547. Accuracy: 49.9715\n",
      "Iteration: 129000. Loss: 0.9670912027359009. Accuracy: 49.9715\n",
      "Iteration: 129500. Loss: 0.9031643867492676. Accuracy: 49.9715\n",
      "Iteration: 130000. Loss: 0.839051365852356. Accuracy: 49.9715\n",
      "Iteration: 130500. Loss: 0.7844132781028748. Accuracy: 49.9715\n",
      "Iteration: 131000. Loss: 0.7277742624282837. Accuracy: 49.9715\n",
      "Iteration: 131500. Loss: 0.738926112651825. Accuracy: 49.9715\n",
      "Iteration: 132000. Loss: 1.1232268810272217. Accuracy: 46.905\n",
      "Iteration: 132500. Loss: 0.8359319567680359. Accuracy: 49.9715\n",
      "Iteration: 133000. Loss: 0.8158455491065979. Accuracy: 49.9715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 133500. Loss: 0.8659034967422485. Accuracy: 49.9715\n",
      "Iteration: 134000. Loss: 0.7837755084037781. Accuracy: 46.976\n",
      "Iteration: 134500. Loss: 0.7911128997802734. Accuracy: 49.9715\n",
      "Iteration: 135000. Loss: 0.7808331847190857. Accuracy: 49.9715\n",
      "Iteration: 135500. Loss: 0.8160150647163391. Accuracy: 49.9715\n",
      "Iteration: 136000. Loss: 0.7839273810386658. Accuracy: 49.9715\n",
      "Iteration: 136500. Loss: 0.8397297859191895. Accuracy: 49.9715\n",
      "Iteration: 137000. Loss: 0.8941104412078857. Accuracy: 49.9715\n",
      "Iteration: 137500. Loss: 0.8747947812080383. Accuracy: 49.9715\n",
      "Iteration: 138000. Loss: 0.7625395059585571. Accuracy: 49.9715\n",
      "Iteration: 138500. Loss: 0.8950851559638977. Accuracy: 49.9715\n",
      "Iteration: 139000. Loss: 0.7295624613761902. Accuracy: 49.9715\n",
      "Iteration: 139500. Loss: 0.9674691557884216. Accuracy: 49.9715\n",
      "Iteration: 140000. Loss: 0.7163895964622498. Accuracy: 49.9715\n",
      "Iteration: 140500. Loss: 0.8740801811218262. Accuracy: 49.977\n",
      "Iteration: 141000. Loss: 0.7193737030029297. Accuracy: 49.9715\n",
      "Iteration: 141500. Loss: 0.8358278870582581. Accuracy: 49.9715\n",
      "Iteration: 142000. Loss: 0.8608774542808533. Accuracy: 49.9715\n",
      "Iteration: 142500. Loss: 0.8427507281303406. Accuracy: 46.905\n",
      "Iteration: 143000. Loss: 0.8925134539604187. Accuracy: 49.9715\n",
      "Iteration: 143500. Loss: 0.7860419750213623. Accuracy: 49.9715\n",
      "Iteration: 144000. Loss: 0.8493319153785706. Accuracy: 46.905\n",
      "Iteration: 144500. Loss: 0.9341721534729004. Accuracy: 49.9715\n",
      "Iteration: 145000. Loss: 0.7252731323242188. Accuracy: 49.9715\n",
      "Iteration: 145500. Loss: 0.7872561812400818. Accuracy: 46.905\n",
      "Iteration: 146000. Loss: 0.7287011742591858. Accuracy: 48.6165\n",
      "Iteration: 146500. Loss: 0.7925201654434204. Accuracy: 49.9715\n",
      "Iteration: 147000. Loss: 0.8369357585906982. Accuracy: 49.9715\n",
      "Iteration: 147500. Loss: 0.7828415036201477. Accuracy: 49.9715\n",
      "Iteration: 148000. Loss: 0.8396921753883362. Accuracy: 49.9715\n",
      "Iteration: 148500. Loss: 0.726646900177002. Accuracy: 46.905\n",
      "Iteration: 149000. Loss: 0.9453551769256592. Accuracy: 49.9715\n",
      "Iteration: 149500. Loss: 0.9340428709983826. Accuracy: 49.9715\n",
      "Iteration: 150000. Loss: 0.8766767382621765. Accuracy: 47.785\n",
      "Iteration: 150500. Loss: 1.062929630279541. Accuracy: 49.9715\n",
      "Iteration: 151000. Loss: 0.78777676820755. Accuracy: 46.905\n",
      "Iteration: 151500. Loss: 0.7822723388671875. Accuracy: 49.9715\n",
      "Iteration: 152000. Loss: 0.7174103260040283. Accuracy: 49.9715\n",
      "Iteration: 152500. Loss: 0.8362779021263123. Accuracy: 49.9715\n",
      "Iteration: 153000. Loss: 0.9559977054595947. Accuracy: 46.905\n",
      "Iteration: 153500. Loss: 0.7999813556671143. Accuracy: 49.9715\n",
      "Iteration: 154000. Loss: 0.9113651514053345. Accuracy: 49.9715\n",
      "Iteration: 154500. Loss: 0.7272654175758362. Accuracy: 49.9715\n",
      "Iteration: 155000. Loss: 0.7231755256652832. Accuracy: 49.9715\n",
      "Iteration: 155500. Loss: 0.7818761467933655. Accuracy: 49.9715\n",
      "Iteration: 156000. Loss: 0.8956836462020874. Accuracy: 46.905\n",
      "Iteration: 156500. Loss: 0.7862080931663513. Accuracy: 49.9715\n",
      "Iteration: 157000. Loss: 0.7182359099388123. Accuracy: 49.9715\n",
      "Iteration: 157500. Loss: 0.7388771176338196. Accuracy: 49.9715\n",
      "Iteration: 158000. Loss: 0.7180020213127136. Accuracy: 49.9715\n",
      "Iteration: 158500. Loss: 0.7919002771377563. Accuracy: 49.9715\n",
      "Iteration: 159000. Loss: 0.8723288178443909. Accuracy: 49.9715\n",
      "Iteration: 159500. Loss: 0.7224461436271667. Accuracy: 49.9715\n",
      "Iteration: 160000. Loss: 0.9579437971115112. Accuracy: 49.9715\n"
     ]
    }
   ],
   "source": [
    "input_dim = 7\n",
    "hidden_dim = 100\n",
    "output_dim = len(set(y_train))\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "learning_rate = 0.2\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = torch.tensor(inputs).requires_grad_()\n",
    "        inputs = inputs.to(device)\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        #images = images.view(-1, 28*28).requires_grad_()\n",
    "        labels = labels.to(device)\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = torch.tensor(inputs).requires_grad_()\n",
    "                inputs = inputs.to(device)\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                #images = images.view(-1, 28*28).requires_grad_()\n",
    "                labels = labels.to(device)\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * float(correct) / float(total)\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 200000 points : 7473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "#Number of mislabeled points out of a total 75 points : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_long, labels_data, test_size=0.2)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary class acc 1 - 7674/200000 = 0.96163\n",
    "# mult class acc 1- 7473/200000 = 0.962635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Algorithm\n",
    "def naive_bayes(train, test):\n",
    "    summarize = summarize_by_class(train)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict(summarize, row)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "def summarize_by_class(dataset):\n",
    "    separated = separate_by_class(dataset)\n",
    "    summaries = dict()\n",
    "    for class_value, rows in separated.items():\n",
    "        summaries[class_value] = summarize_dataset(rows)\n",
    "    return summaries\n",
    "\n",
    "def separate_by_class(dataset):\n",
    "    separated = dict()\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        class_value = vector[-1]\n",
    "        if (class_value not in separated):\n",
    "            separated[class_value] = list()\n",
    "        separated[class_value].append(vector)\n",
    "    return separated\n",
    "\n",
    "\n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "    probabilities = dict()\n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "        for i in range(len(class_summaries)):\n",
    "            mean, stdev, _ = class_summaries[i]\n",
    "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "    return probabilities\n",
    "\n",
    "# Predict the class for a given row\n",
    "def predict(summaries, row):\n",
    "    probabilities = calculate_class_probabilities(summaries, row)\n",
    "    best_label, best_prob = None, -1\n",
    "    for class_value, probability in probabilities.items():\n",
    "        if best_label is None or probability > best_prob:\n",
    "            best_prob = probability\n",
    "            best_label = class_value\n",
    "    return best_label\n",
    "\n",
    "def summarize_dataset(dataset):\n",
    "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "    del(summaries[-1])\n",
    "    return summaries\n",
    "\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "    return sqrt(variance)\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.concatenate([X_train, np.array([y_train]).reshape(X_train.shape[0],1)],axis=1)\n",
    "v = np.concatenate([X_test, np.array([y_test]).reshape(X_test.shape[0],1)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = naive_bayes(t, v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.1875"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96163"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-0.03837\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "- Multilayer perceptron approach\n",
    "    - clean up data\n",
    "    - normalize\n",
    "    - build model\n",
    "- SVM approach\n",
    "- naive bayes approach\n",
    "    - build naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
