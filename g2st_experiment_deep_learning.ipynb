{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying machine learning to number theory problem\n",
    "\n",
    "Sato-tate group classification problem:\n",
    "\n",
    "Given normalized Frobenius traces, is it possible to \n",
    "- determine whether Sato-Tate group  is USp(4)?\n",
    "- Extended problem: Classifying specific type of Sato-Tate group in the LMF (6 classes)\n",
    "\n",
    "#### Some applicable machine learning techniques\n",
    "- Multi-layer perceptron\n",
    "- Naive Bayes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./g2st.txt') as f:\n",
    "    data = [l for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0:[-11648,0,-36324,0,-28647,0,-2376]:4314388037048:[0.000000,0.000000,0.000000,0.000000,0.742781,1.436842,-1.315192,2.498780,0.000000,2.917300,-1.098885,-0.520756,1.536443,-1.466033,0.474713,0.000000,1.350105,-1.317171,-0.847998,0.000000,-0.796030,-0.394132,-1.546778,1.149392,0.752577,-1.064828,1.397926,-0.341743,2.035653,1.638464,0.325515,1.276939,2.193129,0.619059,1.824686,-0.597948,0.594635,0.000000,-2.591337,-0.284988,0.850657,1.652228,1.071439,-0.265489,-0.528655,0.524097,1.034954,-1.545976,2.019822,-1.996105,1.233253,-1.219422,-1.214913,0.000000,0.238620,-0.475551,-3.037872,-0.913168,-0.226819,-2.034840,-0.898650,0.659580,-2.832620,0.000000,-0.642345,-2.554782,-0.422224,2.714378,1.656897,0.616399,1.430733,1.216848,-1.204525,0.998752,0.395575,1.172477,0.389896,-1.541386,0.576683,-1.145458,0.380091,0.566315,-2.058233,-0.931493,0.371792,0.000000,-0.365529,0.000000,1.805175,0.537194,-1.426809,2.482156,-2.453405,-1.399262,2.235655,0.000000,-0.508456,1.348639,-0.670755,1.339158]\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocessing and Spliting data\n",
    "def split_data_train_test(data, long_input=True, normalized_data=True, test_size=0.2):\n",
    "    datapoints = [point.split(':') for point in data]\n",
    "    labels_data = [int(point[0]) for point in datapoints]\n",
    "    if long_input:\n",
    "        input_data = [json.loads(point[3]) for point in datapoints]\n",
    "    else:\n",
    "        input_data = [json.loads(point[1]) for point in datapoints]\n",
    "    input_data_pd = pd.DataFrame(input_data)\n",
    "    if not long_input:\n",
    "        input_data_pd[6] = input_data_pd[6].fillna(0)\n",
    "\n",
    "    if normalized_data:\n",
    "        input_data_pd = preprocessing.normalize(input_data_pd)\n",
    "        input_data_pd = pd.DataFrame(input_data_pd, columns=input_data_pd.columns)\n",
    "    \n",
    "    X = input_data_pd.to_numpy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels_data, test_size=0.2)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "class PolynomialDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.input_data = X\n",
    "        self.labels = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data_train_test(data, normalized_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "train_dataset = PolynomialDataset(X_train, y_train)\n",
    "test_dataset = PolynomialDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 50\n",
    "n_iters = 20000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "        \n",
    "        \n",
    "        # Define batch norm\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Define proportion or neurons to dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "hidden_dim = 200\n",
    "output_dim = 2\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")#(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.5968632698059082. Accuracy: 57.4275\n",
      "Iteration: 1000. Loss: 0.5118539333343506. Accuracy: 83.948\n",
      "Iteration: 1500. Loss: 0.41208088397979736. Accuracy: 90.553\n",
      "Iteration: 2000. Loss: 0.24629293382167816. Accuracy: 94.74\n",
      "Iteration: 2500. Loss: 0.1650465726852417. Accuracy: 96.563\n",
      "Iteration: 3000. Loss: 0.13303349912166595. Accuracy: 97.0475\n",
      "Iteration: 3500. Loss: 0.10587829351425171. Accuracy: 97.439\n",
      "Iteration: 4000. Loss: 0.05692749843001366. Accuracy: 97.5775\n",
      "Iteration: 4500. Loss: 0.04783552512526512. Accuracy: 97.684\n",
      "Iteration: 5000. Loss: 0.11418955773115158. Accuracy: 97.616\n",
      "Iteration: 5500. Loss: 0.21382589638233185. Accuracy: 97.8205\n",
      "Iteration: 6000. Loss: 0.05707945302128792. Accuracy: 97.94\n",
      "Iteration: 6500. Loss: 0.1571904718875885. Accuracy: 97.866\n",
      "Iteration: 7000. Loss: 0.06765960901975632. Accuracy: 98.021\n",
      "Iteration: 7500. Loss: 0.03050486370921135. Accuracy: 98.041\n",
      "Iteration: 8000. Loss: 0.0990460216999054. Accuracy: 98.1035\n",
      "Iteration: 8500. Loss: 0.10834471881389618. Accuracy: 98.144\n",
      "Iteration: 9000. Loss: 0.03089049644768238. Accuracy: 98.127\n",
      "Iteration: 9500. Loss: 0.019055593758821487. Accuracy: 98.149\n",
      "Iteration: 10000. Loss: 0.05120445787906647. Accuracy: 98.2115\n",
      "Iteration: 10500. Loss: 0.2808935046195984. Accuracy: 98.2135\n",
      "Iteration: 11000. Loss: 0.03158790245652199. Accuracy: 98.2325\n",
      "Iteration: 11500. Loss: 0.03091762214899063. Accuracy: 98.2495\n",
      "Iteration: 12000. Loss: 0.03542129695415497. Accuracy: 98.2745\n",
      "Iteration: 12500. Loss: 0.022038159891963005. Accuracy: 98.284\n",
      "Iteration: 13000. Loss: 0.1275506466627121. Accuracy: 98.281\n",
      "Iteration: 13500. Loss: 0.24680133163928986. Accuracy: 98.2555\n",
      "Iteration: 14000. Loss: 0.05142531916499138. Accuracy: 98.3175\n",
      "Iteration: 14500. Loss: 0.015552950091660023. Accuracy: 98.343\n",
      "Iteration: 15000. Loss: 0.016998624429106712. Accuracy: 98.322\n",
      "Iteration: 15500. Loss: 0.03582633659243584. Accuracy: 98.3625\n",
      "Iteration: 16000. Loss: 0.01900186575949192. Accuracy: 98.3705\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = torch.tensor(inputs).requires_grad_()\n",
    "        inputs = inputs.to(device)\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        #images = images.view(-1, 28*28).requires_grad_()\n",
    "        labels = labels.to(device)\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = torch.tensor(inputs).requires_grad_()\n",
    "                inputs = inputs.to(device)\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                #images = images.view(-1, 28*28).requires_grad_()\n",
    "                labels = labels.to(device)\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * float(correct) / float(total)\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
